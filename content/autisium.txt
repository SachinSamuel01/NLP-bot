
Robot-Assisted Autism Spectrum Disorder Diagnostic Based
on Artiﬁcial Reasoning
Andr´
es A. Ram´
ırez-Duque
· Anselmo Frizera-Neto · Teodiano Freire Bastos
Received:  April  / Accepted:  December 
© Springer Nature B.V. 
Abstract
Autism spectrum disorder (ASD) is a neurodevelopmental disorder that affects people from birth, whose symptoms are
found in the early developmental period. The ASD diagnosis is usually performed through several sessions of behavioral
observation, exhaustive screening, and manual coding behavior. The early detection of ASD signs in naturalistic behavioral
observation may be improved through Child-Robot Interaction (CRI) and technological-based tools for automated behavior
assessment. Robot-assisted tools using CRI theories have been of interest in intervention for children with Autism Spectrum
Disorder (CwASD), elucidating faster and more significant gains from the diagnosis and therapeutic intervention when
compared to classical methods. Additionally, using computer vision to analyze child’s behaviors and automated video coding
to summarize the responses would help clinicians to reduce the delay of ASD diagnosis. In this article, a CRI to enhance
the traditional tools for ASD diagnosis is proposed. The system relies on computer vision and an unstructured and scalable
network of RGBD sensors built upon Robot Operating System (ROS) and machine learning algorithms for automated face
analysis. Also, a proof of concept is presented, with participation of three typically developing (TD) children and three
children in risk of suffering from ASD.
Keywords Child-Robot interaction · Autism spectrum disorder · Convolutional neural network · Robot reasoning model ·
Statistical shape modeling
 Introduction
Research in Child-Robot Interaction (CRI) aims to provide
the necessary conditions for the interaction between a
child and a robotic device taking into account some
fundamental features, such as child’s neurophysical and
physical condition, and the child’s mental health .
That is how Robot-Assisted Therapies (RAT) using CRI
theories have been of interest as an intervention for
CwASD, elucidating faster and more significant gains from
the therapeutic intervention when compared to traditional
therapies .
ASD is a neurodevelopmental disorder that affects people
from birth, and its symptoms are found in the early
 Andr´
es A. Ram´
ırez-Duque


Universidade Federal do Espir´
ıto Santo., Av. Fernando Ferrari,
 , Vitoria, Brazil
developmental period. Individuals suffering from ASD
exhibit persistent deficits in social communication, social
interaction and repetitive patterns of behavior, interests, or
activities . Some of the ASD signs may be observed
before the age of  months, although a reliable diagnosis
can only be performed at  months of age, according to ,
or  months according to .
The use of computer vision to analyze the child’s
behaviors, and automated video coding to summarize the
interventions, can help the clinicians to reduce the delay
of ASD diagnosis, providing the CwASD with access
to early therapeutic interventions. In addition, CRI-based
intervention can transform traditional diagnosis methods
through a robotic device to systematically elicit child’s
behaviors that exhibit ASD signs .
Some of the first systems developed to assist ASD
therapists and make diagnosis based on robotic devices
have primarily been open loop and remotely operated sys-
tems. However, these approaches are unable to perform
autonomous feedback to enhance the interaction .
Journal of Intelligent & Robotic Systems  :–
/ Published online: 

March
Content courtesy of Springer Nature, terms of use apply. Rights reserved.
Nevertheless, different systems are able to modify the
behavior of the robot according to environmental interac-
tions and the child’s response, using a closed-loop and
artificial cognition approaches . These systems have
been hypothesized to offer technological mechanisms for
supporting more flexible and potentially more naturalistic
interaction . In fact, literature reports that automatic
robot’s social behaviors modulation according to specifics
scenarios has a strong effect on child’s social behavior
. However, despite the increase of positive evidence,
this technology has rarely been applied to specific ASD
diagnosis.
This work aims to present a robot-assisted framework
using an artificial reasoning module to assist clinicians with
the ASD diagnostic process. The framework is composed of
a responsive robotic platform, a flexible and scalable vision
sensor network, and an automated face analysis algorithm
based on machine learning models. In this research we take
advantage of some neural models available as open sources
projects to build a completely new pipeline algorithm for
global recognition and tracking of child’s face among many
faces present in a typical unstructured clinical intervention,
in order to estimate the child’s visual focus of attention
along the time. The proposed system can be used in different
behavioral analysis scenarios typical of an ASD diagnostic
process. In order to illustrate the feasibility of the proposed
system, in this paper an experimental trial to assess joint-
attention behavior is presented employing an in-clinic setup
(unstructured environment).
The main contributions of this paper are: (i) the
development of a new artificial reasoning module upon
a flexible and scalable ROS-based vision system using
state-of-the-art machine learning neural models; (ii) the
proposal and implementation of a supervised CRI (child-
robot interaction) based on an open source social robotic
platform to enhance the traditional tools for ASD diagnosis
using an in-clinic setup protocol. For the best of our
knowledge, there are no open source projects available for
face analysis based on a multi-camera approach using ROS
with the characteristics described in our research.
 Related Work
Recent researches have shown the acceptance and efficiency
of
technologies used
as
auxiliary tools for
therapy
and teaching of individuals with ASD . Such
technologies may also be useful for people surrounding
ASD individuals (therapists, caregivers, family members).
For example, the use of artificial vision systems to measure
and analyze the child’s behavior can lead to alternative
screening and monitoring tools that help the clinicians to
get feedback from the effectiveness of the intervention .
Additionally, social robots have great potential for aid in
the diagnosis and therapy of children with ASD .
A higher degree of control, prediction and simplicity may
be achieved in interactions with robots, impacting directly
on frustration and reducing the anxiety of these individuals
.
Respect to the use of computer vision techniques,
previous studies already analyzed child’s behaviors, such
as visual attention, eye gaze, eye contact, smile events, and
visual exploration using cameras and eye trackers 
and RGBd cameras . These studies have shown
the potential of vision systems in improving the behavioral
coding in ASD therapies. However, these studies did not
implement techniques of CRI to enhance the intervention.
On the other hand, studies about how CwASD respond
to a robot mediator compared to human mediator have
been reported, such as intervention scenarios with imitation
games , telling stories  and free play tasks
. These works used features, such as proxemics,
body gestures, visual contact and eye gaze as behavioral
descriptors, whereas the behavior analysis was estimated
using manual video coding.
Researchers of Vanderbilt University published a series
of research showing an experimental protocol to assess joint
attention (JA) tasks defined as the capacity for coordinated
orientation of two people toward an object or event .
The protocol consisted of directing the attention of the
child towards objects located in the room through adaptive
prompts . Bekele et al. inferred the participant’s eye
gaze by the head pose, which was calculated in real-time by
an IR camera array . In their last works, Zheng et al. and
Warren et al. used a commercial eye tracker to estimated the
children’s eye gaze around the robot and manual behavioral
coding for global evaluation . However, eye tracker
devices require pre-calibration and may limit the movement
of the individual. The results of these works showed that the
robot attracted children’s attention and that CwASD reached
all JA task. Nevertheless, developing JA tasks is more
difficult with a robot than with humans . Anzalone et al.
developed a CRI scenario using the NAO robot to perform
JA tasks, in which the authors used an RGBD camera
to estimate only body and head movements. The results
showed that JA performance of children with ASD was
similar to the performance of TD children when interacting
with the human mediator, however, with a robot mediator,
the children with ASD presented a lower performance than
the TD children, i.e, the children with ASD needed more
social cues to finalize the task . Chevalier et al. analyzed
in their study, some features, such as proprioceptive and
visual integration in CwASD, using an RGBD sensor
to record the interventions sessions and manual behavior
coding to analyzed the participants’ performance . In
none of the previous works, a closed-loop subsystem was
J Intell Robot Syst  :–

Content courtesy of Springer Nature, terms of use apply. Rights reserved.
implemented to provide some level of artificial cognition to
enable automated robot behavior.
In contrast with the aforementioned researches, other
works implemented automated face analysis and artificial
cognition through robot-mediator and computer vision,
which analyzed child’s engagement , emotions
recognition capability  and child’s intentions
. In these works, two different strategies were
implemented, where the most common is based on mono-
camera approach using an external RGB or RGBd sensor
 or using on-board RGB cameras mounted
on the robotic-platform . Other strategies are
based on a highly structured environment composed of
an external camera plus an on-board camera  or a
network of vision sensors attached to a small table .
These strategies based on multi-camera methods improve
the system’s performance, but remain constrained in relation
to desired features, such as flexibility, scalability, and
modularity. Thus, despite the potential that these techniques
have shown, achieving automated child’s behavior analysis
in a naturalistic way into unstructured clinical-setups with
robots that interact accordingly remains a challenge in CRI.
 System Architecture Overview
The ROS system used in this work is a flexible and
scalable open framework for writing modular robot-
centered systems. Similar to a computing operating system,
ROS manages the interface between robot hardware and
software modules and provides common device drivers, data
structures and tool-based packages, such as visualization
and debugging tools. In addition, ROS uses an interface
definition language (IDL) to describe the messages sent
between process or nodes, this feature facilitates the multi-
language (C++, Python and Lisp) development .
The overall system developed here was built using a
node graph architecture, taking advantages of the principal
ROS design criteria. As with ROS, our system consists of
a number of nodes to local video processing together a
robot’s behavior estimation, distributed around a number
of different hosts and connected at runtime in a peer-to-
peer topology. The inter-node connection is implemented
as a hand-shaking and occurs in XML-RPC protocol
along with a web-socket communication for robot’s web-
based node . The node structure
is flexible, scalable and can be dynamically modified,
i.e., each node can be started and left running along an
experimental session or resumed and connected to each
other at runtime. In addition, from a general perspective,
any robotic platform with web-socket communication can
be integrated. The developed system is composed of two
interconnected modules as shown in Fig. : an artificial
reasoning module and a CRI-channel module. The module
architectures are detailed in the following subsections.
. Architecture of Reasoning Module
In this module, a distributed architecture for local video
processing is implemented. The data of each RGBD sensor
in the multi-camera system are processed for two nodes,
in which the first is a driver level node and the second
is a processing node. The driver node transforms the
streaming data of the RGBD sensor into the ROS messages
format. The driver addresses the data through a specialized
transport provided by plugings to publishes images in a
compressed representations while the receptor node only
sees sensor msgs/Image messages. The data processing
node executes the face analysis algorithm. This node uses
a image transport subscriber and a ROS packages called
CvBridge to turn the data into a image format supported for
the typical computer vision algorithms. Later, the same node
publishes the head pose and eye gaze direction by means of
a ROS navigation message defined as nav msgs/Odometry.
An additional node hosted in the most powerful
workstation carries out a data fusion of all navigation
messages that were generated in the local processing
stage. In addition to the fusion, this node computes the
visual focus of attention (VFOA) and publishes this as a
std msgs/Header, in which the time stamp and the target
name of the VFOA estimation are registered.
. Architecture of CRI-Channel
The system proposed here has two bidirectional communi-
cation channels, a robot-device, and a web-based applica-
tion to interact with both the child and the therapist. The
robot device can interact with the CwASD executing differ-
ent physical actions, such as facial expression, upper limb
poses, and verbal communication. Thus, according to the
child’s performance, the reasoning module can modify the
robot’s behavior through automatic gaze shifting, chang-
ing the facial expression and providing sound rewards. The
client-side application was developed to allow the therapist
to control and register all step of the intervention proto-
col. This interface was also used to supervise and control
the robot’s behavior and to offer feedback to the therapist
about the child’s performance along the intervention. This
App has two channels of communication for interacting
with the reasoning module. The first connection uses a web-
socket protocol and a RosBridge suite package to support
the interpretation of ROS messages, as well as, JSON-based
commands in ROS. The second one uses a ROS module
Tools for using the Kinect One  in ROS, 
com/code-iai/iai kinect.
J Intell Robot Syst  :–

Content courtesy of Springer Nature, terms of use apply. Rights reserved.
Fig.  Node graph architecture
of the proposed ROS-based
system. The system is composed
of two interconnected modules,
an artificial reasoning module
and a CRI-channel module. The
ONO web server has two way of
bidirectional communication: a
websocket and a standard ROS
Subscriber
developed in the server-side application to directly run a
ROS node and communicate with standard ROS publishers
and subscribers.
 The Robotic Platform ONO
The CRI is implemented through the open source platform
for social robotics (OPSORO), which is a promising
and straightforward system developed for face to face
communication composed of a low-cost modular robot
called ONO  and web-based applications
. Some of the most important requirements and
characteristics that make ONO interesting for this CRI
strategy are explained in the following sections.
. Appearance and Identity
The robot is covered in foam and also fabric to have a more
inviting and huggable appearance to the children. The robot
has an oversized head to make its facial expressions more
prominent and to highlight the importance for communica-
tion and emotional interaction. As a consequence of its size
and pose, children can interact with the robot at eye height
when the robot is placed on a table.
The robot ONO has not a predefined identity, as the
only element previously conceived is the name. Unlike other
robots that have well-defined identities, such as Probo , in this work the ONO’s identity is built with
the participation of the child through a co-creation process.
For this reason, a neutral appearance is initially used. In the
Open Source Platform for Social Robotics (OPSORO) 
opsoro.com.
intervention, the therapist can provide the child with clothes
and accessories to define the identity of ONO.
. Mechanics Platform
As the initial design of ONO is composed only of the
actuated face, in this work it was needed to provide the ONO
with some body language. For this purpose, motorized arms
were designed and implemented.
The new design of ONO has a fully face and two
arms actuated, giving a total of  Degrees of Freedom
(DOF). The ONO is able to perform facial expressions and
nonverbal cues, such as waving, shake hands and pointing
towards objects, moving its arms , eyes ( DOF
x ), eyelids , eyebrows , and mouth
( DOF). The robot has also a sound module that allows
explicit positive feedback as well as reinforcement learning
through playing words, conversations and other sounds.
. Social Expressiveness
In order to improve social interaction with a child, the ONO
is able to exhibit different facial expressions. The ONO’s
expressiveness is based on the Facial Action Coding System
(FACS) developed in . Each DOF that composes the
ONO’s face is linked with a set of Action Units (AU) defined
by the FACT, and each facial expression is determined for
specific AU values. The facial expressions are represented as
a D vector f e = (v, a) in the emotion circumplex model
defined by valence and arousal . In this context, the basic
facial expressions are specified on a unit circle, where the
neutral expression corresponds to the origin of the space
f e = . The relation between the DOF position and
the AU values is resolved through a lookup table algorithm
using a predefined configuration file .
J Intell Robot Syst  :–

Content courtesy of Springer Nature, terms of use apply. Rights reserved.
Fig.  ONO robot, developed
through the open source
platform for social robotics
(OPSORO)
. Adaptability and Reproducibility
The application of the Do-It-Yourself (DIY) concept is
the principal feature of ONO’s design, which facilitates
its dissemination and use in research areas other than
engineering as health care. These characteristics allow ONO
building for any person without specialized engineering
knowledge. Additionally, it is possible to replicate ONO
without the need for high-end components or manufacturing
machines . The electronic system is based on a
Raspberry Pi single-board computer combined with a
custom OPSORO module with circuitry to control up to
 servos, drive speakers and touch sensors. Any sensor
or actuator compatible with the embedded communication
protocols (UART, IC, SPI) implemented on the Raspberry
Pi can be used by this platform.
. Control and Autonomy
With the information delivered for the automated reasoning
module, it was possible to automate the ONO’s behavior
and, then, the robot can infer and interpret the children’s
intentions to react most accurately to the action performed
by them, thus enabling a more efficient and dynamic
interaction with ONO. In this work, the automated ONO’s
behavior is partially implemented, i.e., the framework can
modify some physical actions of ONO using the feedback
information about the child’s behavior. The actions suitable
to be modified are gaze shift toward the child in specifics
events, changing from neutral to positive facial expression
when the child looks toward the target, and providing
sound rewards. Also, an Aliveness Behavior Module (ABM)
is implemented to improve the CRI, which consist of
blinking the robot’s eyes and changing its arms among
some predefined poses. Also, the robot can be manually
operated through a remote controller hosted in the client-
side application.
 Reasoning Module: Machine Learning
Methods for Child’s Face Analysis
The automated child’s face analysis consists of monitoring
nonverbal cues, such as head and body movements, head
pose, eye gaze, visual contact and visual focus of attention.
In this work, a pipeline algorithm is implemented using
machine learning neural models for face analysis. The
chosen methods were developed using state-of-art trained
neural models, available by Dlib  and OpenFace
. Some modification such as, turn the neural model an
attribute of the ROS node class and evaluate this in each
topic callback, were needed to run the neural models into a
common ROS node.
The algorithm proposed for child’s face analysis involves
face detection, recognition, segmentation and tracking,
landmarks detection and tracking, head pose, eye gaze and
visual focus of attention (VFOA) estimation. In addition, the
architecture proposed here also implement new methods for
asynchronous matching and fusion of all local data, visual
focus of attention estimation based on Hidden Markov
Model (HMM) and direct connection with the CRI-channel
to influence the robot’s behaviors. A scheme of the pipeline
algorithm is shown in Fig. .
Dlib C++ Library 
A Open Source Facial Behavior Analysis 
TadasBaltrusaitis/OpenFace.
J Intell Robot Syst  :–

Content courtesy of Springer Nature, terms of use apply. Rights reserved.
Fig.  Pipeline algorithm of the
automated child’s face analysis
. Child’s Face Detection and Recognition
The in-clinic setup requires differentiate the child’s face
from other faces detected and found in the scene. For this
reason, a face recognition process was also implemented in
this work. First, the face detection is executed to initialize
the face recognition process and, subsequently, initialize
the landmarks detection. In this work, both detection and
recognition are implemented using deep learning models,
which are described in this section.
In the detection process, a Convolutional Neural Network
(CNN) based face detector with a Max-Margin Object
Detection (MMOD) as loss layer is used . The CNN
consist first of a block composed of three downsampling
layers, which apply convolution with a x filter size and
× stride to reduce the size of the image up to eight
times its original size and generate a feature map with 
dimensions. Later, the result are processed for one more
block composed of four convolutional layers to get the final
output of the network. The three first layers of the last
block have × filter size and x stride, but, the last layer
has only  channel and a × filter size. The values in
the last channel are large when the network thinks it has
found a face at a particular location. All convolutional block
above are implemented with two additional layers among
convolutional layers, pointwise linear transformation, and
Rectified Linear Units (RELU) to apply the non-saturating
activation function f (x) = max(, x). The training dataset
used to create the model is composed of  faces and is
available at Dlib’s homepage.
The face recognition algorithm used in this work is
inspired on the deep residual model from . The
 face detection dataset---.tar.
gz.
residual network (ResNet) model developed by He et. al
reformulates the convolutional layers to learn a residual
functions F(x) := H(x) −x with reference to the layer
inputs x, instead of learning unreferenced functions. In the
practical implementation, the previous formulation means
inserting shortcut connections, which turn the network into
its counterpart residual version . The CNN model then
transforms each face detected to a D vector space in
which images from the same person will be close to each
other, but faces from different people will be far apart.
Finally, the faces are classified as child’s face, caregiver’s
face and therapist’s face.
Both detection and recognition CNN model were
implemented and trained from  and released in Dlib
. Face Analysis, Landmarks, Head Pose and Eye
Gaze
This work uses the technique for landmarks detection, head
pose and eye gaze estimation developed by Baltruˇ
saitis et
al., named Conditional Local Neural Fields (CLNF) .
This technique is an extension of the Constrained Local
Model (CLM) algorithm using specialized local detectors
or patch experts. CNLF model consists of a statistical
shape model, which its learned from data examples and is
parametrized for m components of linear deformation to
control the possible shape variations of the non-rigid objects
. Approaches based on CLM  and CLNF 
model the object appearance in a local fashion, i.e, each
feature point has its own appearance model to describe the
amount of misalignment.
CLNF-based landmark detection consists of three main
parts: the shape model, the local detectors or patch experts,
and the fitting algorithm, which are detailed below.
J Intell Robot Syst  :–

Content courtesy of Springer Nature, terms of use apply. Rights reserved.
.. Shape Model
The CLNF technique uses a linear model to describe non-
rigid deformations called Point Distribution Model (PDM).
The PDM is used to estimate the likelihood of the shapes
being in a specific class, given a set of feature points .
This is important for model fitting and shape recognition.
The shape of a face that has n landmark points can be
described as:
X = ,

and the class that describes a valid instance of a face using
PDM can be represented as:
X = ¯
X + q,

where
¯
X is the mean shape of the face,  described
the principal deformation modes of the shape, and q
represent the non-rigid deformation parameters. Both ¯
X
and  are learned automatically from labeled data using
Principal Component Analysis (PCA). The probability
density distribution of the instances into the shape class is
expressed as a zero mean Gaussian with Covariance matrix
 = () evaluated at q:
p(q) = N(q; ; ) =

√(π)m ||exp

−
(qT −q)


Once the model is defined, it is necessary to place the D
PDM in an image space. The following equation is used
to transform between D space to image space using weak
perspective projection :
xi = s · RD · ( ¯
Xi + iq) + t,

where ¯
Xi
= [¯
xi, ¯
yi, ¯
zi]T is the mean value of the ith
landmark. The instance of the face in an image is, therefore,
controlled using the parameter vector p = [s, w, t, q],
where q represents the local non-rigid deformation, s is a
scaling term, w is the rotation term that controls the  × 
matrix RD, and t is the translation term.
The global parameters are used to estimate the head
pose in reference to the camera space using orthographic
camera projection and solving the Perspective-n-Point
(PnP) problem respect to the detected landmarks. The PDM
used in  was trained on two public datasets .
This result in a model with  non-rigid (Principal modes)
and  rigid shape parameters.
.. Patch Experts
The patch experts scheme is the main novelty implemented
in the CLNF model. The new Local Neural Field (LNF)
patch expert takes advantage of the non linear relationship
between pixel values and the patch response maps. The LNF
captures two kinds of spatial characteristics between pixels,
such as similarity and sparsity .
LNF patch expert can be interpreted as a three layer
perceptron with a sigmoid activation function followed by
a weighted sum of the hidden layers. It is also similar to
the first layer of a Convolutional Neural Network .
The new LNF patch expert is able to learn from multiple
illuminations and retain accuracy. This becomes important
when creating landmark detectors and trackers that are
expected to work in unseen environments and on unseen
people.
The learning and inference process is developed using
a gradient-based optimization method to help in finding
locally optimal model parameters faster and more accu-
rately.
In the CLNF model implemented in ,  set in total
of LNF patch experts were trained for seven views and
four scales. The framework uses patch experts specifically
trained to recognize the eyelids, iris and the pupil, in order
to estimate the eye gaze .
.. Fitting Algorithm
For each new image or video frame, the fitting algorithm
of CLNF-based landmark detection process attempts to
find the value of the local and global deformable model
parameters p that minimizes the following function :
E(p) = R(p)
n

i=
Di(xi; I),

where R is a weight to penalize unlikely shapes, which
depends on the shape model, and D represents the
misalignment of the ith landmark in the image I, which
is function of both the parameters p and the patch experts.
Under the probabilistic point of view, the solution of  is
equivalent to maximize the a posteriori probability (MAP)
of the deformable model parameters p:
p

p | {li = }n
i= , I

∝p((p))
n

i=
p (li =  | xi, I) ,

where, li ∈{, −} is a discrete random variable indicating
whether the ith landmark is aligned or misaligned, p(p)
is the prior probability of the deformable parameters p,
and p (li =  | xi, I) is the probability of a landmark being
aligned at a particular pixel location xi, which is quantified
from the response maps created by patch. Therefore, the
last term in  represents the joint probability of the patch
expert response maps.
The MAP problem is solved using a optimization strategy
designed specifically for CLNF fitting called non-uniform
J Intell Robot Syst  :–

Content courtesy of Springer Nature, terms of use apply. Rights reserved.
regularized landmark mean shift (NU-RLMS) , which
uses two step process. The first step evaluates each of
the patch experts around the current landmark using a
Gaussian Kernel Density Estimator (KDE). The second step
iteratively updates the model parameters to maximize .
The NU-RLMS uses expectation maximization algo-
rithm, where the E-step involves evaluating the posterior
probability over the candidates, and the M-step finds the
parameter updated through the mean shift vector v. The
mean shift vector points in the direction where the feature
point should go, but the motion is restricted by the statisti-
cal shape model and the R(p). This interpretation leads to
the new update function:
argmin
p

∥Jp −v∥
W + r ∥p + p∥
˜
−
	
,

where r is a regularization term, J is the Jacobian, which
describe how the landmarks location are changing based
on the infinitesimal changes of the parameters p, ˜
− =
diag([; ; ; ; ; ; λ−
 ; ... ; λ−
m ]), and W allows for
weighting of mean-shift vectors. Non-linear least squares
leads to the following update rule:
p = −

J T WJ + r− 
r−p −J T Wv

.

To construct W, the performance of patch experts on training
data is used.
. Data Fusion
The fusion of the local results for the head pose estimation
is done applying a consensus over the rotation algorithm
. This algorithm consists of calculating the weighted
average pose between each camera estimation and its
immediate sensors’ estimation neighbors using the axis-
angle representation. The local pose is penalized by two
weights: the alignment confidence of landmarks detection
procedure and the Mahalanobis distances between the head
pose and a neutral pose.
. Field of View (FoV) and Visual focus of Attention
(VFOA)
The VFOA estimation model is implemented as a dynamic
Bayesian network through a Hidden Markov Model
(HMM). The model assumes a specific set of child’s
attention attractors or targets F. The estimation process
decodes the sequence of child’s head poses Ht
=
(H yaw
t
, H pitch
t
) ∈R in terms of VFOA states Ft ∈F
at time t . The probability distribution of the head
poses in reference to a given VFOA target is represented
by a Gaussian distribution, whereas the transitions among
these targets are represented by the transition matrix A. The
HMM equations can then be written as follows:
P (Ht | Ft = f, μh
t ) = N(Ht| μh
t (f ), H(f ))

p(Ft = f | Ft− = ˆ
f ) = Af ˆ
f .

The Gaussian covariances is defined manually to reflect
target sizes and head pose estimation variability. Moreover,
the Gaussian means corresponding to each specific target
μh
t
is calculated through a gaze model that sets this
parameter as a fixed linear combination of the target
direction and the head reference direction :
μh
t (f ) = α ⋆μt(f ) + ( −α) ⋆Rt,

where ⋆denotes the component wise product  = ,
α =  are adjustable constants
that describe the fraction of the gaze shift that corresponds
to the child’s head rotation, μt ∈K is the directions
of the given K targets, and Rt
∈
R represents the
reference direction, which is the average head pose over
a time window W R. The above assumption describes the
body orientation behavior of any child who tends to orient
himself/herself towards the set of gaze targets to make more
comfortable to rotate his/her head towards different targets
.
Rt =

W R
t

i=t−W R
Hi.

Finally, for the estimation of the VFOA sequence a classic
Viterbi algorithm of HMM is implemented .
 Case Study
For the case study, the vision system is composed of
three Kinect V sensors. Each sensor is connected to a
workstation equipped with a processor of Intel Core i
family and a GeForce GTX GPU board (two workstation
with GTX board, and one workstation with GTX
board). All workstation are connected through a local area
network synchronized using the NTP protocol. The sensors
were intrinsically and extrinsically calibrated through a
conventional calibration process using a standard black-
white chessboard.
. In-clinic Setup
A multidisciplinary team of psychologists, doctors and
engineers developed a case study using a psychology room
equipped with a unidirectional mirror to perform behavioral
Network Time Protocol Homepage, 
Tools for using the Kinect One  in ROS, 
com/code-iai/iai kinect.
J Intell Robot Syst  :–

Content courtesy of Springer Nature, terms of use apply. Rights reserved.
Fig.  Representation of the
interventions room of in-clinic
setup
observation appropriately. The room was prepared with a
table and three chairs: one for the child, another for the
caregiver and a third one for the therapist. The robot was
placed on the table, and the following toys, a helicopter,
a truck and a train, were attached to room’s walls. The
RGBD sensors were located close to the walls, and no
additional camera was placed on the robot or the table, so
as not to attract the child’s attention. A representation of the
interventions room of in-clinic setup is shown in Fig. .
. Intervention Protocol
In this work, a technology-based system was used as a
tool in various stages of the ASD diagnostic process.
The framework can be implemented to extract different
behavioral features to be assessed, e.g., eye contact,
stereotyped movements of the head, concentration and
excessive interest in objects or events. However, for the
scope of this research, a specific clinical setup intervention
to assess Joint Attention (JA) behaviors is presented. The
intervention aims to evaluate the capacity of JA; which can
be divided into three classes: initiation of joint attention
(IJA), responding to joint attention bids (RJA), and initiation
of request behavior (IRB) . The therapist guides the
intervention all the time and leverages the robot device as
an alternative channel of communication with the child,
for the above, both the specialist and the robot remained
in the room during the intervention. The children were
accompanied throughout the session by a caregiver who
was oriented not to help the child in the execution of the
Fig.  The child’s nonverbal
cues elicited by the CRI, to look
towards the therapist, towards
the robot, point and self
occlusion
J Intell Robot Syst  :–

Content courtesy of Springer Nature, terms of use apply. Rights reserved.
Fig.  Performance of the
child’s face analysis pipeline for
the case study. Face detection
and recognition, landmarks
detection, head pose and eye
gaze estimation were executed
tasks. The exercise developed aimed to direct the attention
of the child towards objects located in the room through
stimuli, such as, look at, point and speak. The stimuli were
generated first only by the therapist and later just by the
robot.
. Subjects
Three children without confirmed ASD diagnosis, but with
evidence of risk factors, and three typically developing
(TD) children as the control group participated in the
experiments. All volunteers participated with their parent’s
consent, which were five boys ( ASD,  TD) and
one TD girl, between  months to  months. Each
volunteer participated in one single session. The goal was to
analyze the based-line of the child’s behavior and establish
differences in the behavioral reaction between TD and ASD
children for stimuli generated through CRI and leverage the
novelty effect raised by the robot mediator.
 Results and Discussion
The child’s nonverbal cues elicited by the CRI can be
observed in Fig. . Some examples of children’s behavior
tagged to perform the behavioral coding are shown in the
six pictures. The tagged behaviors were: to look towards
an object, towards the robot, and towards the therapist, to
point and, to respond to a prompt of both mediators and self
occlusion. Typical occlusion problem, as occlusion by hair,
hands and the robot were detected.
The performance of video processing in the proof of
concept session is reported in Fig. . In the case study
sessions, the child’s face detection and recognition, the
Fig.  Evolution over time of
the child’s head/neck rotation
(yaw rotation) for a TD group
J Intell Robot Syst  :–

Content courtesy of Springer Nature, terms of use apply. Rights reserved.
Fig.  Evolution over time of the child’s head/neck rotation (yaw rotation) for a TD volunteer and VFOA estimation results
landmarks detection, head pose and eye gaze estimation for
different viewpoints are shown in Fig. . The recognition
process was able to detect all faces in the session
successfully in most cases.
The child’s head pose was captured throughout the
session and analyzed automatically to estimate the evolution
over time of child’s head and the VFOA. Along the
session, the child’s neck right/left rotation movement was
predominant (Yaw axis), while the neck flexion/extension
(Pitch axis) and neck R/L lateral flexion movements (Roll
axis) remained approximately constant. The Yaw rotation of
the TD children group is reported in Fig. . The vertical light
blue stripe indicates the intervention period with therapist-
mediator, and the vertical light green stripe represents
the period with robot-mediator. The continuous blue line
represents the raw data recorded, and the continuous red line
describes the average data trend. From the observation of the
three plot, the TD children started the intervention looking
towards the robot, evidently, the robot was a naturalistic
attention attractor. Subsequently, when the therapist begins
the protocol explaining the tasks, the children attention
shifts towards the therapist. The children remained this
behavior until that the therapist introduced the robot-
mediator. In this transition, the children’s behaviors, such as,
RJA and IJA toward the therapist were observed. Once the
therapist changed the mediation with the robot, the children
turned his/her attention to the robot and the objects in the
room.
A more detailed analysis of one of the TD volunteers is
shown in Fig. . The plot (A) shows the overall intervention
session; the plot (B) and plot (C) are a zoom of the period
with therapist and robot mediator, respectively. The colors
convention in the three plots of Fig.  describes the results
generated by the automated estimation of VFOA. From
these scenarios, some essential aspects already emerge. In
the therapist-mediator interval, the child responded to JA
task using only one repetition for all prompt level. The
child’s behavior of RJA was according to the protocol,
i.e., the child looked towards the therapist to wait for
instructions, rapidly the child searched in the target, and
next looked again toward the therapist (Color sequence:
light blue - yellow - light blue - orange - light blue - red).
This behavior was the same for all prompts. In contrast,
with the robot-mediator, the child did not look toward
the robot among indications at consecutive targets (Color
sequence: light green - yellow - orange - red - orange -
yellow). The above happened because, in the protocol, both
mediators executed the instructions in the same order, and
J Intell Robot Syst  :–

Content courtesy of Springer Nature, terms of use apply. Rights reserved.
Fig.  Evolution over time of the child’s head/neck rotation (yaw rotation) for a ASD group
the child memorized the commands and the object’s position
until the robot mediator interval. This fact did not affect
the intervention’s aim, as the robot mediator succeeded to
elicit the child’s behaviors of RJA and IJA. In addition,
as highlighted in the plot (A) in Fig. , when the session
finalized and the robot mediator said goodbye, again, RJA
and IJA behaviors were perceived. The pictures (a-d) show
these events: first the child said goodbye towards the robot,
then, he looked the therapist to confirm that the session
ended and looked again towards the robot, finally the child
took the robot’s hand.
From the analysis of the three TD volunteers, the same
reported behaviors were perceived. However, the analysis
of the children in the ASD group showed different behavior
patterns concerning comfort, visual contact and novelty
stimulus effect during the sessions. The evolution over time
of the child’s head/neck rotation (yaw rotation) for an ASD
group is shown in Fig. . On the one hand, the three children
in the ASD group maintained more visual contact with
the robot compared to the therapist and exhibited more
interest in the robot platform compared to the TD children.
However, the performance of the children in the activities
of JA did not improve significantly when the robot executed
the prompt. On the other hand, the clinicians manifested that
in all cases the first visual contact toward them occurred
in the instant that the robot entered the scene and started
interacting, i.e., the ONO mediation elicited behaviors of
IJA towards the therapist. In addition, the CwASD exhibited
less discomfort regarding the session, from the first moment
when the robot initiated mediation in the room and, in some
cases, when showed appearance of verbal and non-verbal
pro-social behaviors. These facts did not arise with the TD
children, because the first visual contact with the therapist
occurred when they entered the room. Additionally, TD
children showed the ability to divide the attention between
the robot and the therapist from the beginning to the end
of the intervention, exhibiting comfort in every moment.
The behavior modulation of CwASD is observed in Fig. .
Before the period with robot-mediator the children exhibited
discomfort (unstable movements of their head), and after
of this period, the head movement tended to be more
stable.
The novelty of a robot-mediator at diagnostic session
can be analyzed as an additional stimulus of the CRI.
Accordingly, in this case study the children of the ASD
group showed more behavior modification (attention and
comfort) produced by the robot interaction at the beginning
of the CRI, remaining until the end of the session. On the
other hand, the children of the TD group responded to the
novelty effect of the robot mediator from the time the child
entered the room and saw the robot, until the beginning of
the therapist presentation. For the above, despite the novelty
J Intell Robot Syst  :–

Content courtesy of Springer Nature, terms of use apply. Rights reserved.
of the stimuli effect, these did not seem to affect the social
interaction between the TD children and the therapist, and
in contrast, these stimuli seemed to enhance the CwASD
social interaction with the therapist along the intervention.
These results are impressive, since they show the
potential of CRI intervention to systematically elicit
differences between the pattern of behavior on TD and ASD
children. We identified RJA and IJA toward the therapist at
the beginning of the intervention, at the transition between
therapist to robot mediator, and at the end for all TD
children. In contrast, we only identified IJA towards the
therapist in the transition between mediators, for ASD
children. This fact shows a clear difference of behavior
pattern between CwASD and TD children, which can be
analyzed using a JA task protocol. In fact, these pattern
differences can be used as evidence to improve the ASD
diagnosis.
 Conclusions
This work presented a Robot-Assisted tool to assist and
enhance the traditional practice of ASD diagnosis. The
designed framework combines a vision system with the
automated analysis of nonverbal cues in addition to a
robotic platform; both developed upon open source projects.
This research contributes to the state-of-the-art with an
innovative flexible and scalable architecture capable to
automatically register events of joint attention and patterns
of visual contact before and after of a robot-based mediation
as well as the pattern of behavior related to comfort or
discomfort along the ASD intervention.
In addition, an artificial vision pipeline based on a multi-
camera approach was proposed. The vision system performs
face detection, recognition and tracking, landmark detection
and tracking, head pose, gaze and estimation of visual focus
of attention was proposed, with its performance considered
suitable for use into conventional ASD intervention. At
least one camera captured the child’s face in each sample
frame. Furthermore, the feedback information about the
child’s performance was successfully used to modulate the
supervised behavior of ONO, improving the performance of
the CRI and the visual attention of the children. Regarding
the VFOA estimation, the algorithm was able to estimate
the target into the FoV in different situations recurrently.
Also, the robot was able to react according to the estimation.
However, the algorithm only failed when occlusion by the
child’s hands is generated. On the other hand, the occlusion
by the therapist and the robot was compensated using the
multi-camera approach. The child’s face recognition system
showed to be imperative to analyze the child’s behavior in
the clinical setup implemented in this work, which required
the caregiver’s attention in the room.
Despite the limited number of children of this study,
preliminary results of this case study showed the feasibility
of identifying and quantify differences in the patterns of
behavior of TD children and CwASD elicited by the CRI
intervention. Through the proof of concept, it is evidenced
here the system ability to improve the traditional tools used
in ASD diagnosis. As future works, it is recommended a
study to replicate the protocol proposed in this paper with
ten CwASD and ten TD children. Another suggestion is to
quantify other kinds of behaviors in addition to that assessed
in this paper, such as verbal utterance patterns, physical
and emotional engagement, object or event preferences and
gather more evidence to improve the assistance to therapists
in ASD diagnosis processes.
Acknowledgements This work was supported by the Google Latin
America Research Awards (LARA) program. The first author scholar-
ship was supported in part by the Coordenac
¸˜
ao de Aperfeic
¸oamento de
Pessoal de N´
ıvel Superior - Brasil (CAPES) - Finance Code .
Disclosure statement No potential conflict of interest was reported by
the authors.
References
. Belpaeme, T., Baxter, P.E., de Greeff, J., Kennedy, J., Read, R.,
Looije, R., Neerincx, M., Baroni, I., Zelati, M.C.: Child-Robot
interaction: perspectives and challenges. In: th International
Conference, ICSR , pp. –. Springer International
Publishing, Bristol 
. Diehl, J.J., Schmitt, L.M., Villano, M., Crowell, C.R.: The clinical
use of robots for individuals with autism spectrum disorders: A
critical review. Res. Autism Spectr. Disord. 
. Scassellati, B., Admoni, H., Maja, M.: Robots for use in autism
research. Annu. Rev. Biomed. Eng. 
. Pennisi, P., Tonacci, A., Tartarisco, G., Billeci, L., Ruta, L.,
Gangemi, S., Pioggia, G.: Autism and social robotics: A
systematic review 
. American Psychiatric Association: DSM- diagnostic classifica-
tion. In: Diagnostic and Statistical Manual of Mental Disorders.
American Psychiatric Association,  
. Eggebrecht, A.T., Elison, J.T., Feczko, E., Todorov, A., Wolff, J.J.,
Kandala, S., Adams, C.M., Snyder, A.Z., Lewis, J.D., Estes, A.M.,
Zwaigenbaum, L., Botteron, K.N., McKinstry, R.C., Constantino,
J.N., Evans, A., Hazlett, H.C., Dager, S., Paterson, S.J., Schultz,
R.T., Styner, M.A., Gerig, G., Das, S., Kostopoulos, P., Schlaggar,
B.L., Petersen, S.E., Piven, J., Pruett, J.R.: Joint attention and brain
functional connectivity in infants and toddlers. Cerebral Cortex

. Steiner, A.M., Goldsmith, T.R., Snow, A.V., Chawarska, K.:
Disorders in infants and toddlers. J. Autism Dev. Disord. ,
– 
. Belpaeme, T., Baxter, P.E., Read, R., Wood, R., Cuay´
ahuitl, H.,
Kiefer, B., Racioppa, S., Kruijff-Korbayov´
a, I., Athanasopoulos,
G., Enescu, V., Looije, R., Neerincx, M., Demiris, Y., Ros-
Espinoza, R., Beck, A., Canamero, L., Hiolle, A., Lewis, M.,
Baroni, I., Nalin, M., Cosi, P., Paci, G., Tesser, F., Sommavilla, G.,
Humbert, R.: Multimodal child-robot interaction: building social
bonds. Journal of Human-Robot Interaction 
J Intell Robot Syst  :–

Content courtesy of Springer Nature, terms of use apply. Rights reserved.
. Vanderborght, B., Simut, R., Saldien, J., Pop, C., Rusu, A.S.,
Pintea, S., Lefeber, D., David, D.O.: Using the social robot probo
as a social story telling agent for children with ASD. Interact. Stud.

. Warren, Z.E., Zheng, Z., Swanson, A.R., Bekele, E., Zhang,
L., Crittendon, J.A., Weitlauf, A.F., Sarkar, N.: Can robotic
interaction improve joint attention skills? J. Autism Dev. Disord.

. Wood, L.J., Dautenhahn, K., Lehmann, H., Robins, B., Rainer,
A., Syrdal, D.S.: Robot-mediated interviews: Do robots pos-
sess advantages over human interviewers when talking to chil-
dren with special needs? Lecture Notes in Computer Sci-
ence (including subseries Lecture Notes in Artificial Intelli-
gence and Lecture Notes in Bioinformatics)  LNAI, –

. Feil-Seifer, D., Mataric, M.J.: bIA A control architecture for
autonomous robot-assisted behavior intervention for children
with Autism Spectrum Disorders. In: ROMAN  The th
IEEE International Symposium on Robot and Human Interactive
Communication, pp. – 
. Leo, M., Del Coco, M., Carcagn´
ı, P., Distante, C., Bernava,
M., Pioggia, G., Palestra, G.: Automatic emotion recognition in
Robot-Children interaction for ASD treatment. In: Proceedings
of the IEEE International Conference on Computer Vision, -
Febru
. Esteban, P.G., Baxter, P.E., Belpaeme, T., Billing, E., Cai, H.,
Cao, H.-L., Coeckelbergh, M., Costescu, C., David, D., De Beir,
A., Fang, Y., Ju, Z., Kennedy, J., Liu, H., Mazel, A., Pandey,
A., Richardson, K., Senft, E., Thill, S., Van De Perre, G.,
Vanderborght, B., Vernon, D., Hui, Y., Ziemke, T.: How to build
a supervised autonomous system for Robot-Enhanced therapy
for children with autism spectrum disorder. Paladyn Journal of
Behavioral Robotics 
. Pour, A.G., Taheri, A., Alemi, M., Ali, M.: Human–Robot
facial expression reciprocal interaction platform: case studies
on children with autism. Int. J. Soc. Robot. , –

. Feng, Y., Jia, Q., Wei, W.: A control architecture of Robot-
Assisted intervention for children with autism spectrum disorders.
J. Robot. ,  
. Bekele, E., Crittendon, J.A., Swanson, A., Sarkar, N., Warren,
Z.E.: Pilot clinical application of an adaptive robotic system for
young children with autism. Autism: The International Journal of
Research and Practice 
. Huijnen, C.A.G.J., Lexis, M.A.S., Jansens, R., de Witte, L.P.:
Mapping robots to therapy and educational objectives for children
with autism spectrum disorder. J. Autism Dev. Disord. ,
– 
. Aresti-Bartolome, N., Begonya, G.-Z.: Technologies as support
tools for persons with autistic spectrum disorder: s systematic
review. Int. J. Environ. Res. Public Health , –

. Boucenna, S., Narzisi, A., Tilmont, E., Muratori, F., Pioggia, G.,
Cohen, D., Mohamed, C.: Interactive technologies for autistic
children: a review. Cogn. Comput. 
. Grynszpan, O., Patrice, L., Weiss, T., Perez-Diaz, F., Gal, E.:
Innovative technology-based interventions for autism spectrum
disorders: a meta-analysis. Autism 
. Rehg, J.M., Rozga, A., Abowd, G.D., Goodwin, M.S.: Behavioral
imaging and autism. IEEE Pervasive Comput. , –, 

. Cabibihan, J.J., Javed, H., Ang, M., Aljunied, S.M.: Why robots?
a survey on the roles and benefits of social robots in the
therapy of children with autism. Int. J. Soc. Robot. , –

. Sartorato, F., Przybylowski, L., Sarko, D.K.: Improving therapeu-
tic outcomes in autism spectrum disorders: enhancing social com-
munication and sensory processing through the use of interactive
robots. J. Psychiatr. Res. , – 
. Chong, E., Chanda, K., Ye, Z., Southerland, A., Ruiz, N., Jones,
R.M., Rozga, A., Rehg, J.M.: Detecting gaze towards eyes in
natural social interactions and its use in child assessment. Proc.
ACM Interact. Mob. Wearable Ubiquitous Technol. , :–
: 
. Ness, S.L., Manyakov, N.V., Bangerter, A., Lewin, D., Jagannatha,
S., Boice, M., Skalkin, A., Dawson, G., Janvier, Y.M., Goodwin,
M.S., Hendren, R., Leventhal, B., Shic, F., Cioccia, W., Gahan,
P.: JAKE® Multimodal data capture system: Insights from an
observational study of autism spectrum disorder. Frontiers in
Neuroscience 
. Rehg, J.M., Abowd, G.D., Rozga, A., Romero, M., Clements,
M.A., Sclaroff, S., Essa, I., Ousley, O.Y., Li, Y., Kim, C., Rao,
H., Kim, J.C., Lo Presti, L., Zhang, J., Lantsman, D., Bidwell,
J., Ye, Z.: Decoding children’s social behavior. In:  IEEE
Conference on Computer Vision and Pattern Recognition, pp.
– 
. Adamo, F., Palestra, G., Crifaci, G., Pennisi, P., Pioggia, G.,
Ruta, L., Leo, M., Distante, C., Cazzato, D.: Non-intrusive
and calibration free visual exploration analysis in children
with autism spectrum disorder. In: Computational Vision and
Medical Image Processing V - Proceedings of th Eccomas
Thematic Conference on Computational Vision and Medical
Image Processing, VipIMAGE , pp .– 
. Michaud, F., Salter, T., Duquette, A., Mercier, H., Lauria, M.,
Larouche, H., Larose, F.: Assistive technologies and Child-Robot
interaction. American Association for Artificial Intelligence ii,
– 
. Duquette, A., Michaud, F., Mercier, H.: Exploring the use of
a mobile robot as an imitation agent with children with low-
functioning autism. Auton. Robot. 
. Simut, R.E., Vanderfaeillie, J., Peca, A., Van de Perre, G., Bram,
V.: Children with autism spectrum disorders make a fruit salad
with probo, the social robot: an interaction study. J. Autism Dev.
Disord. 
. Bekele, E., Lahiri, U., Swanson, A.R., Crittendon, J.A., Warren,
Z.E., Nilanjan, S.: A step towards developing adaptive robot-
mediated intervention architecture (ARIA) for children with
autism. IEEE Trans. Neural Syst. Rehabil. Eng. , –

. Zheng, Z., Zhang, L., Bekele, E., Swanson, A., Crittendon, J.A.,
Warren, Z.E., Sarkar, N.: Impact of robot-mediated interaction
system on joint attention skills for children with autism. In: IEEE
International Conference on Rehabilitation Robotics 
. Anzalone, S.M., Tilmont, E., Boucenna, S., Xavier, J., Jouen,
A.L., Bodeau, N., Maharatna, K., Chetouani, M., Cohen, D.: How
children with autism spectrum disorder behave and explore the
-dimensional (spatial D + time) environment during a joint
attention induction task with a robot. Res. Autism Spectr. Disord.

. Chevalier, P., Martin, J.C., Isableu, B., Bazile, C., Iacob,
D.O., Adriana, T.: Joint attention using human-robot interaction:
impact of sensory preferences of children with autism. In: th
IEEE International Symposium on Robot and Human Interactive
Communication, RO-MAN , pp. – 
. Lemaignan, S., Garcia, F., Jacq, A., Dillenbourg, P.: From real-
time attention assessment to “with-me-ness” in human-robot
interaction. In: ACM/IEEE International Conference on Human-
Robot Interaction, -April, pp. – 
. Del Coco, M., Leo, M., Carcagni, P., Fama, F., Spadaro, L.,
Ruta, L., Pioggia, G., Distante, C.: Study of mechanisms of
J Intell Robot Syst  :–

Content courtesy of Springer Nature, terms of use apply. Rights reserved.
social interaction stimulation in autism spectrum disorder by
assisted humanoid robot. IEEE Transactions on Cognitive and
Developmental Systems 
. Palestra, G., Varni, G., Chetouani, M., Esposito, F.: A multimodal
and multilevel system for robotics treatment of autism in children.
In: Proceedings of the International Workshop on Social Learning
and Multimodal Interaction for Designing Artificial Agents - DAA
’, pp. –. ACM Press, New York 
. Quigley, M., Gerkey, B., Conley, K., Faust, J., Foote, T., Leibs,
J., Berger, E., Wheeler, R., Ng, A.: ROS : an open-source robot
operating system. In: ICRA workshop on open source software,
number ., pp.  
. Vandevelde, C., Saldien, J., Ciocci, C., Vanderborght, B.: The
use of social robot ono in robot assisted therapy. In: International
Conference on Social Robotics, Proceedings, m 
. Dautenhahn, K.: A paradigm shift in artificial intelligence: why
social intelligence matters in the design and development of robots
with human-like intelligence.  Years of Artificial Intelligence,
pp. – 
. Ekman, P., Friesen, W.: Facial Action Coding System. Consulting
Psychologists Press 
. King, D.E.: Dlib-ml: a machine learning toolkit. J. Mach. Learn.
Res. , – 
. Baltruˇ
saitis, T., Robinson, P., Morency, L.-P.: OpenFace: an open
source facial behavior analysis toolkit. IEEE Winter Conference
on Applications of Computer Vision 
. He, K., Zhang, X., Ren, S., Jian, S.: Deep residual learning for
image recognition. In:  IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. –. IEEE, 
. Baltruˇ
saitis, T., Robinson, P., Morency, L.P.: Constrained local
neural fields for robust facial landmark detection in the wild. In:
Proceedings of the IEEE International Conference on Computer
Vision, pp. – 
. Cristinacce, D., Cootes, T.F.: Feature detection and tracking with
constrained local models. In: Proceedings of the British Machine
Vision Conference , pp. – 
. Saragih, J.M., Lucey, S., Cohn, J.F.: Deformable model fitting
by regularized landmark mean-shift. Int. J. Comput. Vis. ,
– 
. Baltruˇ
saitis, T., Robinson, P., Morency, L.P.: D constrained local
model for rigid and non-rigid facial tracking. In: Proceedings of
the IEEE Computer Society Conference on Computer Vision and
Pattern Recognition, pp. – 
. Belhumeur, P.N., Jacobs, D.W., Kriegman, D.J., Neeraj, K.:
Localizing parts of faces using a consensus of exemplars. IEEE
Trans. Pattern Anal. Mach. Intell. 
. Le, V., Brandt, J., Lin, Z., Bourdev, L., Huang, T.S.: Interactive
Facial Feature Localization, pp. –. Springer, Berlin 
. Jorstad, A., Dementhon, D., Jeng Wang, I., Burlina, P.: Distributed
consensus on camera pose. IEEE Trans. Image Process. ,
– 
. Ba, S.O., Odobez, J.-M.: Multi-Person visual focus of attention
from head pose and meeting contextual cues. IEEE Trans. Pattern
Anal. Mach. Intell. 
. Sheikhi, S., Jean-Marc, O.: Combining dynamic head pose-
gaze mapping with the robot conversational state for attention
recognition in human-robot interactions. Pattern Recogn. Lett. ,
– 
Publisher’s Note Springer Nature remains neutral with regard to
jurisdictional claims in published maps and institutional affiliations.
Andr´
es A. Ram´
ırez-Duque received his bachelor’s degree in Mecha-
tronics Engineering from the Universidad Nacional de Colombia,
Bogot´
a, Colombia, in , and his Industrial Automation Master
degree from the Universidad Nacional de Colombia, Bogot´
a, Colom-
bia, in . He is currently working toward a Ph.D. degree in the
Assistive Technology Center, Federal University of Esp´
ırito Santo,
Vit´
oria, Brazil. He won a Google Latin America Research Award .
His current research interests include Child-Robot interaction, cloud
parallel computing, high performance computing, smart environments
and serious games applied to Children with development impairments.
Anselmo Frizera-Neto received his bachelor’s degree in Electrical
Engineering  from the Federal University of Esp´
ırito Santo
 at the
University of Alcal´
a, Spain. From  to  he was a researcher of
the Bioengineering Group of the Consejo Superior de Investigaciones
Cient´
ıficas (Spain) where he carried out research related to his
doctoral thesis. He is currently a permanent professor and adjunct
coordinator of the Graduate Program in Electrical Engineering at
UFES. He has authored or co-authored more than  papers in
scientific journals, books and conferences in the fields of electrical
and biomedical engineering. He has conducted or co-directed master’s
and doctoral theses in research institutions from Brazil, Argentina,
Italy and Portugal. His research is aimed at rehabilitation robotics, the
development of advanced strategies of human-robot interaction and the
conception of sensors and measurement technologies with applications
in different fields of electrical and biomedical engineering. Along with
Andr´
es Ram´
ırez-Duque, he won a Google Latin America Research
Award .
Teodiano Freire Bastos received his B.Sc. degree in Electrical
Engineering from Universidade Federal do Esp´
ırito Santo (Vit´
oria,
Brazil) in , his Specialist degree in Automation degree from
Instituto de Autom´
atica Industrial (Madrid, Spain) in , and
his Ph.D. degree in Physical Science (Electricity and Electronics)
from Universidad Complutense de Madrid (Spain) in . He made
two postdocs, one at the University of Alcal´
a  and
another at RMIT University . He is currently a
full professor at Universidade Federal do Esp´
ırito Santo (Vit´
oria,
Brazil), teaching and doing research at the Postgraduate Program of
Electrical Enginneering, Postgraduate Program of Biotechnology and
RENORBIO Ph.D. Program. His current research interests are signal
processing, rehabilitation robotics and assistive technology for people
with disabilities
J Intell Robot Syst  :–

Content courtesy of Springer Nature, terms of use apply. Rights reserved.
.
.
.
.
.
.
Terms and Conditions
 
Springer Nature journal content, brought to you courtesy of Springer Nature Customer Service Center GmbH (“Springer Nature”).
 
Springer Nature supports a reasonable amount of sharing of  research papers by authors, subscribers and authorised users (“Users”), for small-
scale personal, non-commercial use provided that all copyright, trade and service marks and other proprietary notices are maintained. By
accessing, sharing, receiving or otherwise using the Springer Nature journal content you agree to these terms of use (“Terms”). For these
purposes, Springer Nature considers academic use (by researchers and students) to be non-commercial.
 
These Terms are supplementary and will apply in addition to any applicable website terms and conditions, a relevant site licence or a personal
subscription. These Terms will prevail over any conflict or ambiguity with regards to the relevant terms, a site licence or a personal subscription
(to the extent of the conflict or ambiguity only). For Creative Commons-licensed articles, the terms of the Creative Commons license used will
apply.
 
We collect and use personal data to provide access to the Springer Nature journal content. We may also use these personal data internally within
ResearchGate and Springer Nature and as agreed share it, in an anonymised way, for purposes of tracking, analysis and reporting. We will not
otherwise disclose your personal data outside the ResearchGate or the Springer Nature group of companies unless we have your permission as
detailed in the Privacy Policy.
 
While Users may use the Springer Nature journal content for small scale, personal non-commercial use, it is important to note that Users may
not:
 
 
use such content for the purpose of providing other users with access on a regular or large scale basis or as a means to circumvent access
control;
use such content where to do so would be considered a criminal or statutory offence in any jurisdiction, or gives rise to civil liability, or is
otherwise unlawful;
falsely or misleadingly imply or suggest endorsement, approval , sponsorship, or association unless explicitly agreed to by Springer Nature in
writing;
use bots or other automated methods to access the content or redirect messages
override any security feature or exclusionary protocol; or
share the content in order to create substitute for Springer Nature products or services or a systematic database of Springer Nature journal
content.
 
In line with the restriction against commercial use, Springer Nature does not permit the creation of a product or service that creates revenue,
royalties, rent or income from our content or its inclusion as part of a paid for service or for other commercial gain. Springer Nature journal
content cannot be used for inter-library loans and librarians may not upload Springer Nature journal content on a large scale into their, or any
other, institutional repository.
 
These terms of use are reviewed regularly and may be amended at any time. Springer Nature is not obligated to publish any information or
content on this website and may remove it or features or functionality at our sole discretion, at any time with or without notice. Springer Nature
may revoke this licence to you at any time and remove access to any copies of the Springer Nature journal content which have been saved.
 
To the fullest extent permitted by law, Springer Nature makes no warranties, representations or guarantees to Users, either express or implied
with respect to the Springer nature journal content and all parties disclaim and waive any implied warranties or warranties imposed by law,
including merchantability or fitness for any particular purpose.
 
Please note that these rights do not automatically extend to content, data or other material published by Springer Nature that may be licensed
from third parties.
 
If you would like to use or distribute our Springer Nature journal content to a wider audience or on a regular basis or in any other manner not
expressly permitted by these Terms, please contact Springer Nature at
 
 

 
Ouss et al. Translational Psychiatry   : 

Translational Psychiatry
A R T I C L E
O p e n A c c e s s
Behavior and interaction imaging at  months of
age predict autism/intellectual disability in high-risk
infants with West syndrome
Lisa Ouss, Giuseppe Palestra
, Catherine Saint-Georges, Marluce Leitgel Gille, Mohamed Afshar,
Hugues Pellerin, Kevin Bailly, Mohamed Chetouani, Laurence Robel, Bernard Golse, Rima Nabbout,
Isabelle Desguerre, Mariana Guergova-Kuras and David Cohen

Abstract
Automated behavior analysis are promising tools to overcome current assessment limitations in psychiatry. At
 months of age, we recorded  infants with West syndrome (WS) and  typically developing (TD) controls during a
standardized mother–infant interaction. We computed infant hand movements (HM), speech turn taking of both
partners (vocalization, pause, silences, overlap) and motherese. Then, we assessed whether multimodal social signals
and interactional synchrony at  months could predict outcomes (autism spectrum disorder (ASD) and intellectual
disability (ID)) of infants with WS at  years. At follow-up,  infants developed ASD/ID (WS+). The best machine
learning reached .% accuracy classifying WS vs. TD and .% accuracy classifying WS+ vs. WS−. The  best
features to distinguish WS+ and WS−included a combination of infant vocalizations and HM features combined with
synchrony vocalization features. These data indicate that behavioral and interaction imaging was able to predict ASD/
ID in high-risk children with WS.
Introduction
Behavior and interaction imaging is a promising domain
of affective computing to explore psychiatric conditions–.
Regarding
child
psychiatry,
many
researchers
have
attempted to identify reliable indicators of neurodevelop-
mental disorders (NDD) in high-risk populations (e.g., sib-
lings of children with autism) during the ﬁrst year of life to
recommend early interventions. However, social signals
and any alterations of them are very difﬁcult to identify at
such a young age. In addition, exploring the quality and
dynamics of early interactions is a complex endeavor. It
usually requires (i) the perception and integration of mul-
timodal social signals and (ii) an understanding of how two
interactive partners synchronize and proceed in turn
taking.
Affective computing offers the possibility to simulta-
neously analyze the interaction of several partners while
considering the multimodal nature and dynamics of social
signals and behaviors. To date, few seminal studies have
attempted
to
apply
social
signal
processing
to
mother–infant interactions with or without a speciﬁc
condition, and these studies have focused on speech turns
, motherese, head movements, hand
movements,
movement
kinematics,
and
facial
expressions.
Here, we focused on West syndrome (WS), a rare epi-
leptic encephalopathy with early onset (before age  year)
and a high risk of NDD outcomes, including one-third of
WS children showing later autism spectrum disorder
(ASD) and/or intellectual disability (ID). We recruited 
infants with WS and  typically developing (TD) controls
to participate in a standardized early mother–infant
© The Author(s) 
OpenAccessThisarticleislicensedunderaCreativeCommonsAttribution.InternationalLicense,whichpermitsuse,sharing,adaptation,distributionandreproduction
in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if
changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If
material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain
permission directly from the copyright holder. To view a copy of this license, visit 
Correspondence: Lisa Ouss () or David Cohen
()
Service de Psychiatrie de l’Enfant, AP-HP, Hôpital Necker,  rue de Sèvres,
 Paris, France
Institut des Systèmes Intelligents et de Robotique, CNRS, UMR , Sorbonne
Université,  Place Jussieu,  Paris Cedex, France
Full list of author information is available at the end of the article
():,;
():,;
():,;
():,;
interaction protocol and followed infants with WS to
assess outcomes at  years of age. We aim to explore
whether multimodal social signals and interpersonal
synchrony of infant–mother interactions at  months
could predict outcomes.
Materials and methods
Design, participants, and clinical measures
We performed a prospective follow-up study of infants
with WS. The Institutional Review Board (Comité de
Protection des Personnes from the Groupe-Hospitalier
Necker Enfants Malades) approved the study, and both
parents gave written informed consent after they received
verbal and written information on the study. They were
asked to participate to a follow-up study to assess out-
come of WS taking into account development, early
interaction, genetics and response to pharmacological
treatment. The study was conducted from November
 to March  in the Neuro-pediatrics Department
Center for Rare Epilepsia of Necker Enfants-Malades
Hospital, Paris. Of the  patients screened during the
study period, we enrolled all but two cases  with
WS. Seven patients dropped out before the age of 
leading to a sample of  patients with detailed follow-up
data. Typical developing infants  were recruited
from Maternal and Infant Prevention institutions, in
pediatric consultations, or by proxy.
To assess neurodevelopmental outcomes, we focused on
ID and ASD. ID was assessed through the Brunet-Lézine
Developmental Examination, performed for all children at
the age of  years. The Brunet-Lézine Developmental
Examination estimates a developmental quotient (DQ)
based upon normative data available for -year-old
French toddlers. The diagnosis of autism was based
upon several measurements and an expert assessment
that was blind to other variables: (i) At  years of age, all
parents completed the Autism Diagnostic Interview-
Revised (ADI-R) to assess autism signs by dimensions
and developmental delay. (ii) At  and  years of age, all
patients were assessed with the Children’s Autism Rating
Scale (CARS). (iii) An expert clinician (LR) who was
blind to child history assessed autism and ID from -min
videotapes of child/mother play at  years of age. Finally,
diagnoses of ASD and/or ID at age  were based upon a
consensus approach using direct assessment of the child
by a clinician with expertise in autism (LO) as well as by
clinical information from the CARS, ADI-R, and DQ.
Video recordings
Infant–mother interactions were assessed between 
and  months of age during a play session . Two
synchronized cameras (face and proﬁle; Fig. SA) recor-
ded the movements in two dimensions while the infant
was sitting in a baby chair. Audio interactions were also
Around  months
 
e
m
o
r
d
n
y
s
 
t
s
e
W
n
e
r
l
d
i
h
c

l
a
c
i
p
y
T
g
n
i
p
o
l
e
v
e
d
controls 
Lab recording of 
infant-mother
interacon
Free interacon
Interacon
with the giraﬀe
Mother singing
 sequences
of interacon
WS with
ID/ASD 
WS without
ID/ASD 
Typical developing
controls 
At  years
Machine 
learning
classiﬁcaon
West 
syndrome
vs. TD
WS with
ID/ASD
vs. WS 
without
ID/ASD 
Audio features extracon
Infant: typical vocalizaon, 
atypical vocalizaon, pause
Mother: vocalizaon, 
motherese, pause
Synchrony: overlap, silence, 
infant synchrony rao
Video features extracon
Infant’s hand movement: 
velocity, acceleraon, 
curvature, spaal, pause
Fig.  Pipeline of our machine learning approach to classify WS vs. TD.
Ouss et al. Translational Psychiatry   : 
Page  of 
recorded. The standardized situation encompassed three
sequences of  min:  free play after instruct-
ing the mother to interact “as usual” without any toy;
 free play using the help of a toy (Sophie the
giraffe);  mother singing to her baby. Due to
the position of the baby chair on the ﬂoor and the
mother’s seated position, the mother was positioned
slightly higher in all of the recordings. The mother’s
indicated position was on the left of the child as shown on
the picture, but exceptions were sometimes observed
during the recordings. For infant hand movement (HM)
features,  min was extracted from each -min video and
all recordings, according to two criteria: the child’s hands
should be visible for at least part of the sequence (e.g., the
mother is not leaning on the child), and the minute
represented the greatest amount of interaction between
the mother and the child. For audio and speech turn-
taking
computing,
we
only
used
the
-min
audio
recording of sequence .
Vision computing (Fig. SB, vision computing panel)
To process infant hand movements (HM), we used the
methods developed in Ouss et al.. Here, we summarize
the successive steps to calculate HM features. In step 
(hand trajectory extraction and data processing), the two-
dimensional coordinates of the hand were extracted from
each of the video recordings by tracking a wristband on
the right hand (yellow in Fig. SA, video-audio recording
panel). The tracking framework comprised three steps:
prediction, observation, and estimation as proposed in
ref. . As the hand motion was highly nonlinear, we
developed an approach using a bootstrap-based particle
ﬁlter with a ﬁrst-order model to address abrupt changes in
direction and speed. To address hand occlusion, we
implemented
an
approach
combining
tracking
with
detection by adding a boolean variable to the state vector
associated with each particle.
Each extracted trajectory consisted of  pairs of x
and y coordinates ( frames per second, generating 
pairs of coordinates in the  s; see Fig. S left panel,
vision computing). The frames where the hand was not
visible were clearly indicated in each trajectory as missing
coordinates for these time points. To account for differ-
ences in the camera zoom parameters, the trajectories
obtained were normalized using a ﬁxed reference system
present in the settings of each video recording. The nor-
malization was performed on all trajectories, and % of
the normalization factors ranged between . and .
with a few outlier trajectories that required greater cor-
rection. Forty-one percent of the trajectories required <%
correction. Although the recordings between the two
cameras were synchronized and in principle allowed D
reconstruction of the trajectory, the accumulation of
missing data prevented such reconstruction. However, D
motion capture with appropriately deﬁned movement
descriptors can be powerful for detecting clinically rele-
vant changes, thereby justifying the independent ana-
lysis of the D-trajectory videos (see Fig. SB, vision
computing, d panel on the left).
In step , the descriptors of the HM were calculated
from the planar trajectories (Fig. SB, table shown in the
vision
computing
panel).
Descriptors
covered
those
already reported in the literature as important in char-
acterizing infants’ HM.  To describe the space
explored by the hand, we calculated the maximum dis-
tance observed on the two axes (xRange, yRange) and the
standard deviation of the X and Y coordinates observed
during the  s (xSd, ySd). We also calculated the max-
imum distance between any two points of the trajectory
using
the
FarthestPair
java
library
(
princeton.edu/code/) (Fig. SB, vision computing panel,
red line in the third panel from the left).  To evaluate
HM dynamics, we calculated the velocity and accelera-
tion.  Also related to HM dynamics, we calculated HM
pauses deﬁned as part of the trajectory in which the
velocity was lower than a speciﬁc threshold for a mini-
mum duration of  s.  Finally, the curvature of the
trajectories was calculated using a standard deﬁnition of
the curvature (κ) of plane curves in Cartesian coordinates
as γ(t) = (x(t), y(t)). The curvature calculated at each point
of the trajectory is presented in the right panel of Fig. SB
(video computing), where the ﬁrst . s of the trajectory
are plotted and the associated calculated curvatures at
each point (and respective time, indicated on the axis) are
presented as columns.
Audio computing (Fig. SC, audio computing)
We extracted two types of audio social signals from the
audio channel of the mother–infant interaction: speech
turn taking (STT) and motherese. For STT extraction, we
followed the methods developed by Weisman et al. and
Bourvis et al. (Fig. S, audio computing). First, we used
ELAN to segment the infants’ and mothers’ speech turns
and annotate the dialog acts. Mothers’ audio interactions
were categorized as mother vocalization (meaningful
vocalizations, laugh, singing, animal sounds) or other
noise (clap hands, snap ﬁngers or snap the tongue, mouth
noise, etc.). Similarly, infants’ audio production was
deﬁned as infant vocalization (babbling vocalizations,
laugh, and cry) or atypical vocalization (other noise such
as “rale”). The infant’s and mother’s utterances were
labeled by two annotators (blind to group status). Cohen’s
kappa between the two annotators was calculated for each
dyad, each task and each item of the grid. For all items, the
kappa values were between . and .
From the annotation, we extracted all the speech turns
of the infant and the mother. A speech turn is a con-
tinuous stream of speech with < ms of silence. We
Ouss et al. Translational Psychiatry   : 
Page  of 
obtained a list of triples: speaker label (infant or mother),
start time, and duration of speech turn. From these triples,
we also deduced the start time and duration of the time
segments when the mother or the infant were not
speaking
(pauses).
Therefore,
we
extracted
Mother
Vocalizations; Mother Other Noise; Infant Vocalizations;
Infant Atypical Vocalizations; Mother Pauses; Infant
Pauses. We also extracted three dyadic features: 
Silence deﬁned as sequences of time during which neither
participant was speaking for more than  ms; 
Overlap Ratio deﬁned as the duration of vocalization
overlaps between mothers and infants divided by the
duration of the total interaction. This ratio measures the
proportion of interactional time in which both partici-
pants were simultaneously vocalizing;  Infant Syn-
chrony Ratio deﬁned as the number of infants’ responses
to their mother’s vocalization within a time limit of  s
divided by the number of mother vocalizations during the
time paradigm. The -s window was based on the avail-
able literature on synchrony.
From the mother vocalizations, we also computed
affective speech analysis, as previous work has shown that
motherese may shape parent-infant interactions. The
segments of mother vocalizations were analyzed using a
computerized classiﬁer for categorization as “motherese”
or “non-motherese/other speech” initially developed to
analyze home movies. The system exploits the fusion of
two classiﬁers, namely, segmental and suprasegmental.
Consequently, the utterances are characterized by both
segmental (Mel frequency cepstrum coefﬁcients) and
suprasegmental/prosodics (e.g., statistics with regard to
fundamental frequency, energy, and duration) features.
The detector used the GMM (Gaussian mixture model)
classiﬁer for both segmental and suprasegmental features
(M, number of Gaussians for the GMM Classiﬁer: M = 
and , respectively, and λ = weighting coefﬁcient used in
the equation fusion: λ = .). For the purpose of the
current study, we explored the performance of our
motherese classiﬁer in French mothers. We analyzed
 sequences from French mothers ( motherese vs.
 other speech) that were blindly validated by two
psycholinguists. We calculated the Intraclass correlation
(ICC) between the two raters (the expert and the algo-
rithm) and found a good and very signiﬁcant ICC (ICC =
. , p < .). This level of predic-
tion made it suitable for further analysis of the entire
data set.
Based on this automatic detection of motherese, we
created two subclasses for mother vocalizations: mother-
ese vs. non-motherese. Two variables were derived:
Motherese Ratio (duration of motherese vocalization/
duration of interaction) and Non-motherese Ratio (dura-
tion of non-motherese vocalization/duration of interac-
tion). We also derived two synchrony ratios: Synchrony
Motherese Ratio and Synchrony Non-motherese Ratio,
which reﬂect the ratio of time during which the infant
vocalizes in response to his/her mother motherese and
other speech (non-motherese).
Prediction of the outcome using machine learning
The pipeline of our approach is shown in Fig. . First, a
data quality analysis was performed to ensure the validity
of the data. As expected, all data were available for audio
analysis. However, a substantial proportion of the data
were discarded due to video recording or vision com-
puting issues. We ﬁnally kept  video recordings for the
WS and  videos for the TD groups. Second, given the
number of features ( infant HM for each camera and
each sequence;  STT) compared with the data set (
WS and  TD), we reduced our data using principal
component analysis (PCA). Third, we tested several
algorithms to classify WS vs. TD based on the whole data
set available for both vision and audio computing features
. The best algorithm was deci-
sion stump. All results presented here are based on the
classiﬁcation with a decision stump algorithm. We also
analyzed WS with ID/ASD (WS+) vs. WS without ID/
ASD (WS−). For each classiﬁcation, we also extracted a
confusion matrix and explored which individual features
contributed the most to a given classiﬁcation using
Pearson correlations.
Results
Table S summarizes the demographic and clinical
characteristics of children with WS. At follow-up, 
infants out of  children with WS developed ASD/ID
(WS+). Eight children had ASD and ID, whereas  had
only ID. As expected, all variables related to ASD and ID
were signiﬁcantly different in WS+ compared with WS−.
Figure a summarizes the best classiﬁcation models
using the decision stump algorithm (leave one out). As
shown, multimodal classiﬁcation outperformed unimodal
classiﬁcation to distinguish WS and TD. Therefore, we
only used the multimodal approach to classify WS+ vs.
WS−. The best model reached .% accuracy classify-
ing WS vs. TD and .% accuracy classifying WS+ vs.
WS−based on multimodal features extracted during early
interactions. Interestingly, the confusion matrices (Fig.
b) show that when classifying WS vs. TD, all errors came
from TD being misclassiﬁed as WS ; when clas-
sifying WS+ vs. WS−, most errors came from WS+ being
misclassiﬁed as WS−.
Table  lists the best features for each multimodal
classiﬁcation based on the Pearson correlation values. The
best features to distinguish WS and TD included four
infant HM features,  mother audio feature. In contrast,
the best features to distinguish WS+ and WS−included a
combination
of
infant
vocalization
features
,
Ouss et al. Translational Psychiatry   : 
Page  of 










Mumodal
Video
Audio
West vs. TD 
West with ID/ASD vs. 
West with out ID/ASD 

Machine learning classiﬁcaon
Decision stump (leave one out)
a
Confusion
matrices
b
Fig.  Machine learning classiﬁcation of WS vs. TD and WS+ vs. WS−based on uni- and multimodal features extracted during early infant–mother
interaction.
Table 
Best features for classiﬁcation (based on signiﬁcant Pearson’s correlation between feature and class).
Feature characteristics
Pearson r
p-value
West vs. Typical developing
Ratio of all maternal audio intervention during free interaction
Audio, mother
.
Total number of infant HM pauses (side view camera) during free interaction
Video, infant
.
Total number of infant HM pauses (side view camera) when the mother is singing
Video, infant
.
Vertical amplitude of the giraffe (front view camera)
Video, infant
−.
.
Movement acceleration max (side view camera) during free interaction
Video, infant
.
West with ASD/ID vs. West without ASD/ID
Total number of all infant vocalization during free interaction
Audio, infant
−.
<.
Synchrony ratio (infant response to mother)
Audio, synchrony
−.
<.
Ratio of all infant vocalization during free interaction
Audio, infant
−.
.
Motherese synchrony ratio (infant response to motherese)
Audio, synchrony
−.
.
Non-motherese synchrony ratio (infant response to non-motherese)
Audio, synchrony
−.
.
HM acceleration SD (front view camera) during the giraffe interaction
Video, infant
−.
.
HM acceleration max (side view camera) during the giraffe interaction
Video, infant
−.
.
HM velocity SD (front view camera) during the giraffe interaction
Video, infant
−.
.
Curvature max (side view camera) during the giraffe interaction
Video, infant
−.
.
Relative time spent motionless (pause) (front view camera) during free interaction
Video, infant
.
HM hand movement, ASD autism spectrum disorder, ID intellectual disability, SD standard deviation.
Ouss et al. Translational Psychiatry   : 
Page  of 
synchrony vocalization features  and infant HM
features , the last of which showed lower correla-
tion scores.
Discussion
To the best of our knowledge, this is the ﬁrst study to
apply
multimodal
social
signal
processing
to
mother–infant interactions in the context of WS. Com-
bining
speech
turns
and
infant
HM
during
an
infant–mother interaction at  months signiﬁcantly pre-
dicted the development of ASD or severe to moderate ID
at  years of age in the high-risk children with WS.
Confusion matrices showed that the classiﬁcation errors
were not random, enhancing the interest of the compu-
tational method proposed here. In addition, the best
contributing features for the performed classiﬁcations
differed when classifying WS vs. TD and WS+ vs. WS−.
Infant HMs were the most signiﬁcant features to distin-
guish WS versus TD, probably reﬂecting the motor
impact due to acute WS encephalopathy. For classifying
WS+ vs. WS−, the contribution of infant audio features
and synchrony features became much more relevant
combined with several HM features.
We believe that the importance of synchrony and
reciprocity during early interactions is in line with
recent studies that have investigated the risk of ASD or
NDD during the ﬁrst year of life from home movies
, from prospective follow-up of high-risk
infants such as siblings  or infants with
WS , and from prospective studies assessing
tools to screen risk for autism . In the ﬁeld of
ASD, synchrony, reciprocity, parental sensitivity, and
emotional engagement are now proposed as targets of
early interventions, which could prevent early inter-
active vicious circles. Parents of at-risk infants try to
compensate for the lack of interactivity of their child by
modifying their stimulation and therefore sometimes
reinforcing
the
dysfunctional
interactions.
Early
identiﬁcation of these interactive targets is especially
useful among babies with neurological comorbidities
because
delays
in
developmental
milestones
and
impairments in early social interactions are not sufﬁ-
cient to predict ASD.
Similarly, we believe that the importance of HM in
distinguishing WS vs. TD on one hand, and WS+ vs.
WS−on the other hand, is also in line with the studies
that investigated the importance of non-social behaviors
for investigating the risk of ASD or NDD during the ﬁrst
year of life. For example, studying home movies, Purpura
et al. found more bilateral HM and ﬁnger movements in
infants who will later develop ASD. Similarly, several
prospective follow-up studies of high-risk siblings– or
retrospective studies on home movies reported spe-
ciﬁc motor atypical repertoire in infants with ASD.
In ASD, early social signals have previously been
assessed with automatized and computational procedures,
focusing on eye tracking at early stages–, vocal pro-
ductions, analysis of acoustics of ﬁrst utterances or cry
episodes, but none was done in an interactive setting.
Our study proposed a paradigm shift from the assessment
of infant behavior to dyadic assessment of interactions, as
previously achieved in retrospective approaches using
home movies. The aim is not to implement studies of
social signal processing in routine clinical work but rather
to decompose clinical intuitions and signs and validate the
most relevant cues of these clinical features. From clinical
work, back to clinics, social signal processing is a rigorous
step to help clinicians better identify and assess early
targets of interventions.
Given the exploratory nature of both our approach and
method, our results should be interpreted with caution
taking into account strengths (prospective follow-up,
automatized multimodal social signal processing, and
ecological
standardized
assessment)
and
limitations.
These limitations include  the overall sample size
knowing that WS is a rare disease;  the high rate of
missing data during video recording due to the ecological
conditions of the infant–mother interaction (mothers
interposing between the camera and the infant); the ﬁnal
sample size of WS+  that limited the power of
machine learning methods.
We conclude that the method proposed here combining
multimodal automatized assessment of social signal pro-
cessing during early interaction with infants at risk for
NDD is a promising tool to decipher clinical features that
remain difﬁcult to identify and assess. In the context of
WS, we showed that such a method we proposed to label
‘behavioral and interaction imaging’ was able to sig-
niﬁcantly predict the development of ASD or ID at  years
of age in high-risk children who had WS and were
assessed at  months of age.
Acknowledgements
The authors thank all of the patients and families who participated in this
study. The study was funded by the EADS foundation (PILE), by the Agence
Nationale de la Recherche  and the Groupement de
Recherche en Psychiatrie . It was partially performed in the Labex
SMART , which is supported by French state funds and
managed by the ANR in the Investissements d’Avenir program under reference
ANR--IDEX--. The sponsors had no involvement in the study design,
data analysis, or interpretation of the results.
Author details
Service de Psychiatrie de l’Enfant, AP-HP, Hôpital Necker,  rue de Sèvres,
 Paris, France. Institut des Systèmes Intelligents et de Robotique, CNRS,
UMR , Sorbonne Université,  Place Jussieu,  Paris Cedex, France.
Département de Psychiatrie de l’Enfant et de l’Adolescent, AP-HP, Hôpital
Pitié-Salpêtrière, -, Boulevard de l’Hôpital,  Paris, Cedex , France.
Ariana Pharmaceuticals, Research Department, Paris, France. Service de
Neuropédiatrie, AP-HP, Hôpital Necker, , Rue de Vaugirard,  Paris,
France
Ouss et al. Translational Psychiatry   : 
Page  of 
Conﬂict of interest
The authors declare that they have no conﬂict of interest.
Publisher’s note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional afﬁliations.
Supplementary Information accompanies this paper at (
./s---).
Received:  December  Revised:  January  Accepted:  January

References
.
Spodenkiewicz, M. et al. Distinguish self- and hetero-perceived stress through
behavioral imaging and physiological features. Prog. Neuropsychopharmacol.
Biol. Psychiatry , – .
.
Leclere, C. et al. Interaction and behaviour imaging: a novel method to
measure mother-infant interaction using video D reconstruction. Transl.
Psychiatry , e .
.
Messinger, D. S., Mahoor, M. H., Chow, S. M. & Cohn, J. F. Automated mea-
surement of facial expression in infant-mother interaction: a pilot study.
Infancy , – .
.
Wan, M. W. et al. Parent-infant interaction in infant siblings at risk of autism. Res
Dev. Disabil. , – .
.
Rogers, S. J. et al. Autism treatment in the ﬁrst year of life: a pilot study of
infant start, a parent-implemented intervention for symptomatic infants. J.
Autism Dev. Disord. , – .
.
Zwaigenbaum, L., Bryson, S. & Garon, N. Early identiﬁcation of autism spectrum
disorders. Behav. Brain Res , – .
.
Feldman, R. Parent-infant synchrony and the construction of shared timing;
physiological precursors, developmental outcomes, and risk conditions. J.
Child Psychol. Psychiatry , – .
.
Delaherche, E. et al. Interpersonal synchrony: a survey of evaluation methods
across disciplines. IEEE Trans. Affect Comput , – .
.
Vinciarelli, A., Pantic, M. & Bourlard, H. Social signal processing: survey of an
emerging domain. Image Vis. Comput , – .
.
Jaffe, J., Beebe, B., Feldstein, S., Crown, C. L. & Jasnow, M. D. Rhythms of
dialogue in infancy: coordinated timing in development. Monogr. Soc. Res
Child Dev. , – .
.
Cohen, D. et al. Do parentese prosody and fathers’ involvement in interacting
facilitate social interaction in infants who later develop autism? PLoS ONE ,
e .
.
Hammal, Z., Cohn, J. F. & Messinger, D. S. Head movement dynamics during
play and perturbed mother-infant interaction. IEEE Trans. Affect Comput. ,
– .
.
Ouss, L. et al. Developmental trajectories of hand movements in typical infants
and those at risk of developmental disorders: an observational study of
kinematics during the ﬁrst year of life. Front Psychol. ,  .
.
Ouss, L. et al. Taking into account infant’s engagement and emotion during
early interactions may help to determine the risk of autism or intellectual
disability in infants with West syndrome. Eur. Child Adolesc. Psychiatry ,
– .
.
Josse, D. Le manuel BLR-C, “Brunet-Lézine Révisé: Echelle de Developpement
Psychomoteur de la Premiere Enfance .
.
Lord, C., Rutter, M. & Le Couteur, A. Autism diagnostic interview-revised: a
revised version of a diagnostic interview for caregivers of individuals with
possible pervasive developmental disorders. J. Autism Dev. Disord. , –
.
.
Schopler, E., Reichler, R. J., DeVellis, R. F. & Daly, K. Toward objective classiﬁ-
cation of childhood autism: Childhood Autism Rating Scale (CARS). J. Autism
Dev. Disord. , – .
.
Czyz, J., Ristic, B. & Macq, B. A color-based particle ﬁlter for joint detection and
tracking of multiple objects. in Proceedings  IEEE International
Conference on Acoustics, Speech, and Signal Processing (IEEE, Philadelphia, PA,
).
.
Hue, C. Méthodes Séquentielles de Monte Carlo pour le Filtrage non Linéaire
Multi-Objets dans un Environnement Bruité. Applications au Pistage Multi-Cibles
et à la Trajectographie d’Entités dans des Séquences d’Images D. PhD Thesis,
Université de Rennes I, Rennes, France .
.
Isard, M. & Blake, A. Condensation—conditional density propagation for visual
tracking. Int J. Comput Vis. , – .
.
Marcroft, C., Khan, A., Embleton, N. D., Trenell, M. & Plotz, T. Movement
recognition technology as a method of assessing spontaneous general
movements in high risk infants. Front Neurol. ,  .
.
Weisman, O. et al. Dynamics of non-verbal vocalizations and hormones during
father-infant interaction. IEEE Trans. Affect Comput , – .
.
Bourvis, N. et al. Pre-linguistic infants employ complex communicative loops
to engage mothers in social exchanges and repair interaction ruptures. R. Soc.
Open Sci. ,  .
.
Saint-Georges, C. et al. Do parents recognize autistic deviant behavior long
before diagnosis? Taking into account interaction using computational
methods. PLoS ONE , e .
.
Saint-Georges, C. et al. Motherese in interaction: at the cross-road of emotion
and cognition? .
.
Mahdhaoui, A. et al. Computerized home video detection for motherese may
help to study impaired interaction between infants who become autistic and
their parents. Int J. Methods Psychiatr. Res. , e–e .
.
Iba, W. & Langley, P. Induction of one-level decision trees. in Machine Learning:
Proceedings of the Ninth International Workshop (eds Sleeman, D. & Edwards, P.)
– .
.
Wan, M. W. et al. Quality of interaction between at-risk infants and caregiver at
– months is associated with -year autism outcome. J. Child Psychol.
Psychiatry , – .
.
Olliac, B. et al. Infant and dyadic assessment in early community-based
screening for autism spectrum disorder with the PREAUT grid. PLoS ONE ,
e .
.
Green, J. et al. Parent-mediated intervention versus no intervention for infants
at high risk of autism: a parallel, single-blind, randomised trial. Lancet Psychiatry
, – .
.
Purpura, G. et al. Bilateral patterns of repetitive movements in - to -
month-old infants with autism spectrum disorders. Front Psychol. , e
.
.
Loh, A. et al. Stereotyped motor behaviors associated with autism in high-risk
infants: a pilot videotape analysis of a sibling sample. J. Autism Dev. Disord. ,
– .
.
Morgan, L., Wetherby, A. M. & Barber, A. Repetitive and stereotyped move-
ments in children with autism spectrum disorders late in the second year of
life. J. Child Psychol. Psychiatry , – .
.
Elison, J. T. et al. Repetitive behavior in -month-olds later classiﬁed with
autism spectrum disorder. J. Am. Acad. Child Adolesc. Psychiatry , –
.
.
Wolff, J. J. et al. Longitudinal patterns of repetitive behavior in toddlers with
autism. J. Child Psychol. Psychiatry , – .
.
Phagava, H. et al. General movements in infants with autism spectrum dis-
orders. Georgian Med. N. , – .
.
Libertus, K., Sheperd, K. A., Ross, S. W. & Landa, R. J. Limited ﬁne motor and
grasping skills in -month-old infants at high risk for autism. Child Dev. ,
– .
.
Bedford, R. et al. Precursors to social and communication difﬁculties in infants
at-risk for autism: gaze following and attentional engagement. J. Autism Dev.
Disord. , – .
.
Elsabbagh, M. et al. What you see is what you get: contextual modulation of
face scanning in typical and atypical development. Soc. Cogn. Affect Neurosci.
, – .
.
Jones, W. & Klin, A. Attention to eyes is present but in decline in --month-
old infants later diagnosed with autism. Nature , – .
.
Paul, R., Fuerst, Y., Ramsay, G., Chawarska, K. & Klin, A. Out of the mouths of
babes: vocal production in infant siblings of children with ASD. J. Child Psychol.
Psychiatry , – .
.
Sheinkopf, S. J., Iverson, J. M., Rinaldi, M. L. & Lester, B. M. Atypical cry acoustics
in -month-old infants at risk for autism spectrum disorder. Autism Res. ,
– .
Ouss et al. Translational Psychiatry   : 
Page  of 
Research and Applications
Machine learning approach for early detection of autism
by combining questionnaire and home video screening
Halim Abbas, Ford Garberson, Eric Glover, and Dennis P Wall
Cognoa Inc., Palo Alto, CA, USA , , Department of Pediatrics, Stanford
University, Stanford, CA, USA, Department of Biomedical Data Science, Stanford University, Stanford, CA, USA
Correspondence to: Cognoa Inc., Palo Alto, CA, USA; 
Received  September ; Revised  March ; Editorial Decision  March ; Accepted  April 
ABSTRACT
Background: Existing screening tools for early detection of autism are expensive, cumbersome, time- intensive,
and sometimes fall short in predictive value. In this work, we sought to apply Machine Learning (ML) to gold
standard clinical data obtained across thousands of children at-risk for autism spectrum disorder to create a
low-cost, quick, and easy to apply autism screening tool.
Methods: Two algorithms are trained to identify autism, one based on short, structured parent-reported ques-
tionnaires and the other on tagging key behaviors from short, semi-structured home videos of children. A com-
bination algorithm is then used to combine the results into a single assessment of higher accuracy. To over-
come the scarcity, sparsity, and imbalance of training data, we apply novel feature selection, feature
engineering, and feature encoding techniques. We allow for inconclusive determination where appropriate in
order to boost screening accuracy when conclusive. The performance is then validated in a controlled clinical
study.
Results: A multi-center clinical study of n ¼  children is performed to ascertain the performance of these
algorithms and their combination. We demonstrate a signiﬁcant accuracy improvement over standard screen-
ing tools in measurements of AUC, sensitivity, and speciﬁcity.
Conclusion: These ﬁndings suggest that a mobile, machine learning process is a reliable method for detection
of autism outside of clinical settings. A variety of confounding factors in the clinical analysis are discussed along
with the solutions engineered into the algorithms. Final results are statistically limited and will beneﬁt from fu-
ture clinical studies to extend the sample size.
Key words: supervised machine learning, autism spectrum disorder, diagnostic techniques and procedures, mobile applications
INTRODUCTION
Diagnosis within the first few years of life dramatically improves the
outlook of children with autism, as it allows for treatment while the
child’s brain is still rapidly developing. Unfortunately, autism is
typically not diagnosed earlier than age  in the United States, with
approximately % of cases remaining undiagnosed at age . This
delay in diagnosis is driven primarily by a lack of effective screening
tools and a shortage of specialists to evaluate at-risk children. The
use of higher accuracy screening tools to prioritize children to be
seen by specialists is therefore essential.
Most autism screeners in use today are based on questions for
the parent or the medical practitioner, that produce results by com-
paring summed answer scores to predetermined thresholds. Notable
examples are the Modified Checklist for Autism in Toddlers, Re-
vised (M-CHAT), a checklist-based screening tool for autism that is
intended to be administered during developmental screenings for
children between the ages of  and  months, and the Child Be-
havior Checklist (CBCL). Both are parent-completed screening
tools. For both instruments, responses to each question are summed
with each question given equal weighting, and if the total is above a
pre-determined threshold the child is considered to be at high risk of
V
C The Author(s) . Published by Oxford University Press on behalf of the American Medical Informatics Association.
All rights reserved. For permissions, please email: 

Journal of the American Medical Informatics Association, , , –
doi: ./jamia/ocy
Advance Access Publication Date:  May 
Research and Applications
autism. In the case of CBCL there are multiple scales based upon dif-
ferent sets of questions corresponding to different conditions. The
“Autism Spectrum Problems” scale of CBCL is used when comparing
its performance to the performances of our algorithms in this paper.
In this paper, we present two new machine learning screeners that
are reliable, cost-effective, short enough to be completed in minutes,
and achieve higher accuracy than existing screeners on the same age
span as existing screeners. One is based on a short questionnaire about
the child, which is answered by the parent. The other is based on iden-
tification of specific behaviors by trained analysts after watching two
or three short videos of the child within their natural environment
that are captured by parents using a mobile device.
The parent questionnaire screener keys on behavioral patterns
similar to those probed by a standard autism diagnostic instrument,
the Autism Diagnostic Interview – Revised (ADI-R). This clinical
tool consists of an interview of the parent with  multi-part ques-
tions with multiple choice and numeric responses which are deliv-
ered by a trained professional in a clinical setting. While this
instrument is considered a gold-standard, and gives consistent
results across examiners, the cost and time to administer it can be
prohibitive in a primary care setting. In this paper, we present our
approach to using clinical ADI-R instrument data to create a
screener based on a short questionnaire presented directly to parents
without supervision.
The video screener keys on behavioral patterns similar to those
probed in another diagnostic tool, the Autism Diagnostic Observa-
tion Schedule (ADOS). ADOS is widely considered a gold standard
and is one of the most common behavioral instruments used to aid
in the diagnosis of autism. It consists of an interactive and struc-
tured examination of the child by trained clinicians in a tightly con-
trolled setting. ADOS is a multi-modular diagnostic instrument,
with different modules for subjects at different levels of cognitive de-
velopment. In this paper, we present our approach to mining ADOS
clinical records, with a focus on younger developmental age, to cre-
ate a video-based screener that relies on an analyst evaluating short
videos of children filmed by their parents at home.
The use of behavioral patterns commonly probed in ADI-R and
ADOS scoresheets as inputs to train autism screening classifiers was
introduced, studied, and clinically validated in previous work.–
There are several new aspects in this paper. First, the algorithms de-
tailed in the present study have been designed to be more accurate
and more robust against confounding biases between training and
application data. Next, this paper focuses considerable attention on
the impact of confounding factors on machine learning algorithms
in this context. Examples of these confounding biases will be dis-
cussed below and highlighted in Table . Labeled data usually origi-
nates from tightly controlled clinical environments and is, hence,
clean but sparse, unbalanced, and of a different context to the data
available when applying the screening techniques in a less formal en-
vironment. This paper also presents a combination between the
algorithms for a more powerful single screener. Lastly, this paper
generalizes the algorithms to be non-binary, sometimes resulting in
an “inconclusive” determination when presented with data from
more challenging cases. This allows higher screening accuracy for
those children who do receive a conclusive screening, while still pre-
senting a clinically actionable inconclusive outcome in the more
challenging cases.
These classifiers of this paper were applied to screen children in
a clinical study using the Cognoa App. To date, Cognoa has been
used by over   parents in the US and internationally. The ma-
jority of Cognoa users are parents of young children between  and
 months. The clinical study consisted of  at-risk children who
had undergone full clinical examination and received a clinical diag-
nosis at a center specialized in neurodevelopmental disorders.
METHODS
It is not feasible to amass large training sets of children who have
been evaluated by the mobile screeners and who also have received a
professional medical diagnosis. Our approach is to start with histor-
ical medical instrument records of previously diagnosed subjects,
and use those as training data for screeners that will rely on informa-
tion acquired outside the clinical setting. Expected performance deg-
radation from applying the algorithms into a less controlled setting
would result in inaccurate screeners if conventional machine learn-
ing methods were used. Much of this paper outlines the details of
creative machine learning methods designed to overcome this chal-
lenge and create reliable screeners in this setting.
Training data were compiled from multiple repositories of
ADOS and ADI-R score-sheets of children between  and 
months of age including Boston Autism Consortium, Autism Ge-
netic Resource Exchange, Autism Treatment Network, Simons Sim-
plex
Collection,
and
Vanderbilt
Medical
Center.
Since
such
repositories are highly imbalanced with very few non-autistic
patients, the controls across the datasets were supplemented with
balancing data obtained by conducting ADI-R interviews by a
trained clinician on a random sample of children deemed at low risk
for autism from Cognoa’s user base. For both algorithms a smaller
set of optimal features was selected using methods that will be dis-
cussed below. Details about the final selected features are given in
the Supplementary Material.
The clinical validation sample consists of  children who pre-
sented to one of three autism centers in the United States between 
and  months of age. All participants were referred through the
clinics’ typical referral program process, and only those with
English-speaking parents were considered for the study. The three
clinical centers were approved on a multisite IRB (project number
). Every child received an ADOS as well as standard screen-
ers like M-CHAT and CBCL as appropriate, and a diagnosis was ul-
timately ascertained by a licensed health care provider. For  of
those children, the parents also used their mobile devices to com-
plete the short parental questionnaire and submit the short videos
required for the screeners discussed in this paper. The sample break-
down by age group and diagnosis for both the training and clinical
validation datasets is shown in Table .
Approach
We trained two independent ML classifiers and combined their out-
puts into a single screening assessment. The parent questionnaire
classifier was trained using data from historical item-level ADI-R
score-sheets with labels corresponding to established clinical diagno-
ses. The video classifier was trained using ADOS instrument score-
sheets and diagnostic labels. In each case, progressive sampling was
used to verify sufficient training volume as detailed in the Supple-
mentary Materials. Multiple machine learning algorithms were eval-
uated including ensemble techniques on the training data. A number
of algorithms performed well. Random Forests were chosen because
of robustness against overfitting.
ADI-R and ADOS instruments are designed to be administered by
trained professionals in highly standardized clinical settings and typi-
cally take hours. In contrast, our screening methods are deliberately
Journal of the American Medical Informatics Association, , Vol. , No. 

designed to be administered at home by parents without expert super-
vision, and to take only minutes to complete. This change of environ-
ment causes significant data degradation and biases resulting in an
expected loss of screening accuracy. For each classifier, we present
mindful adjustments to ML methodology to mitigate these issues.
These biases and efforts to mitigate them are discussed below.
Differences Between Training and Application
Environments
The screeners are trained on historical patient records that corre-
spond to controlled, lengthy clinical examinations, but applied via
web or mobile app aimed at unsupervised parents at home. Table 
details the various mechanisms by which confounding biases may
consequently creep into the application data. Note that inaccuracies
introduced by such biases cannot be probed by cross- validation or
similar analysis of the training data alone.
Hyperparameter Optimization
For each parental questionnaire and video model that will be dis-
cussed below, model hyperparameters were tuned with a boot-
strapped grid search. In all cases, class labels were used to stratify
the folds, and (age, label) pairs were used to weight-balance the sam-
ples. More details can be found in the Supplementary Materials.
Parent Questionnaire
Multiple model variants representing incremental improvements
over a generic ML classification approach are discussed below.
Generic ML Baseline Variant
A random forest was trained over the ADI-R instrument data. Each
of the instrument’s  data columns was treated as a categorical
variable and one-hot encoded. The subject’s age and gender were in-
cluded as features as well. Of the resulting set of features, the top 
were selected using feature-importance ranking in the decision
forest.
Robust Feature Selection Variant
Due to the small size and sparsity of the training dataset, generic fea-
ture selection was not robust, and the selected features (along with
the performance of the resulting model) fluctuated from run to run
due to the stochastic nature of the learner’s underlying bagging ap-
proach. Many ADI-R questions are highly correlated, leading to
multiple competing sets of feature selection choices that were seem-
ingly equally powerful during training, but which had different per-
formance characteristics when the underlying sampling bias was
exposed via full bootstrapped cross-validation. This resulted in a
wide performance range of the variant of the Generic ML baseline
method as shown in Table .
Table . Dataset Breakdown by Age Group and Condition Type for Each of the Sources of Training Data and for the Clinical Validation
Sample. The Negative Class Label Includes Normally Developing (i.e. neurotypical) Children as Well as Children with Developmental Delays
and Conditions other than Autism
Number of samples
Age
Condition
Classification type Questionnaire
Video
Clinical validation
(years)
training
training
< 
Autism
þ



< 
Other condition




< 
Neurotypical




 
Autism
þ



 
Other condition




 
Neurotypical




Table . Differences Between Training and Application Environments. These Differences are Expected to Cause Bias that Cannot be Cap-
tured by Cross-validation Studies
Aspect
Training Setting
Application Setting
Source
ADI-R and ADOS instrument administered
by trained professionals during clinical
eval-uations
Short parent questionnaires displayed on smartphone, and behavior tagging by
ana-lysts after observing two or three -minute home videos uploaded by
parents
Proctor
Highly trained medical professionals
Parents answering the questionnaires are un-trained, and the analysts evaluating
the home videos are only minimally trained. As a result, their answers may not
be as consistent, objective, or reliable
Setting
Clinic setting with highly standardized and
semi-structured interactions
At home. Not possible to recreate the structured clinical environment, resulting in
an undesired variability of the output signals. Subjects might also behave differ-
ently at the clinic than at home, further amplifying the bias
Duration
The ADI-R can take up to  hours to com-
plete; The ADOS can take up to 
minutes of direct observation by trained
professionals
Under  minutes to complete the parent questionnaire, and a few minutes of
home video. As a result, some symptoms and behavioral patterns might be pre-
sent but not observed. Also causes big uncertainty about the severity and fre-
quency of observed symptoms
Questionnaires
Sophisticated language involving psycholog-
ical concepts, terms, and subtleties unfa-
miliar to nonexperts
Simpliﬁed questions and answer choices result in less nuanced, noisier inputs

Journal of the American Medical Informatics Association, , Vol. , No. 
Robust feature selection overcame that limitation using a two-
step approach. First, a -count bootstrapped feature selection
was run, with a weight balanced % random sample selected in
each iteration. The top  features were selected each time, and a
rank-invariant tally was kept for the number of times each feature
made it to a top- list. Next, the top  features in the tally were
kept as candidates and all other features were discarded. A final
feature-selection run was used to pick the best subset of these can-
didate features. This approach was found to be more robust to sta-
tistical fluctuations, usually selecting the same set of features when
run multiple times. A minimal subset of maximally performant fea-
tures was chosen and locked for clinical validation, totaling  fea-
tures for the young children and  features for the old. Details
about these selected features are available in the Supplementary
Material.
Age Silo Variant
This variant built upon the improvements of the robust feature selec-
tion method, by exploiting of the dichotomy between pre-phrasal
and fully-phrasal language capability in at-risk children. Language
development is significant in this domain as it is known to affect the
nature in which autism presents, and consequently the kinds of be-
havioral clues to look for in order to screen for it.
This variant achieved better performance by training separate
classifiers for children in the younger and older age groups of
Table . The age dichotomy of <,  was chosen to serve as the
best proxy for language ability. Feature selection, model parameter-
tuning, and cross-validation were run independently for each age
group classifier. Before siloing by age group, the classifier was lim-
ited to selecting features that work well across children of both de-
velopmental stages. Siloing enabled the classifiers to specialize on
features that are most developmentally appropriate within each age
group.
Severity-level Feature Encoding Variant
Building upon the method including age siloing above, this variant
achieved better performance by replacing one-hot feature encoding
with a more context-appropriate technique. One-hot encoding does
not distinguish between values that correspond to increasing levels of
severity of a behavioral symptom, and values that do not convey a clear
concept of severity. This is especially troublesome since a typical ADI-
R instrument question includes answer choices from both types of val-
ues. For example, ADI-R question , which focuses on the child’s ten-
dency to confuse and mix up pronouns, allows for answer codes , ,
, , , , and . Among those choices,  through  denote increasing
degrees of severity in pronominal confusion, while  denotes any other
type of pronominal confusion not covered in - regardless of severity.
Codes  and  denote the non-applicability of the question (for exam-
ple, to a child still incapable of phrasal speech) or the lack of an answer
(for example, if the question was skipped) respectively. When coding
the answers to such questions, generic one-hot encoding would allow
for non-symptomatic answer codes to be selected as screening features
based on phantom correlations present in the dataset.
Severity-level encoding converts all answer codes that do not con-
vey a relevant semantic concept to a common value, thereby reducing
the chance of useless feature selection, and reducing the number of fea-
tures to choose from. In addition, severity-level encoding condenses the
signal according to increasing ranges of severity. For example, the
encoding of ADI-R question  would map its responses to new fea-
tures with s in the following cases (all other new features would be
zero): ( !“¼,” ! “,” ! , ! , !
“¼ ,” , !None). This more closely resembles the way medical prac-
titioners interpret such answer choices, and helps alleviate the problem
of sparsity over each of the one-hot encoded features in the dataset.
Aggregate Features Variant
Building upon the method including severity level encoding above,
this variant achieved better performance by incorporating aggregate
Table . Performance of Increasingly Effective Classiﬁer Variants Based on the Training Data for the Parent Questionnaire. Results in the
Top Table are Based on Cross-validated Training Performance. Results in the BottomTable (which are only available for variants using the
optimally selected features) are Based on Actual Clinical Results
AUC
Sensitivity
Specificity
All ages
<  years
>¼  years
All ages
<  years
>¼  years
All ages
<  years
>¼  years
Training scenario
Generic ML baseline
.
Robust feature selection
variant
.
Age silo variant
.
Severity-level feature
encoding variant
.
Aggregate features variant
.
With inconclusive
allowance 
.
Application scenario
Age silo variant
.
Severity-level feature
encoding variant
.
Aggregate features variant
.
With inconclusive
allowance 
.
Journal of the American Medical Informatics Association, , Vol. , No. 

features such as the minimum, maximum, and average severity level,
as well as number of answer choices by severity level across the
questions corresponding to the  selected features. These new fea-
tures were especially helpful due to the sparse, shallow, and wide na-
ture of the training set, whereupon any semantically meaningful
condensation of the signal can be useful to the trained classifier.
Inconclusive Results Variant
Children with more complex symptom presentation are known to
pose challenges to developmental screening. These children often
screen as false positives or false negatives, resulting in an overall
degradation of screening accuracy that is observed by all standard
methods and has become acceptable in the industry. Given that our
low-cost instruments do not rely on sophisticated observations to
differentiate complex symptom cases, our approach was to avoid
assessing them altogether, and to try instead to spot and label them
as “inconclusive.”
Building upon the method including feature engineering, two
methods to implement this strategy were devised. The first was to
train a binary classifier with a continuous output score, then replace
the cutoff threshold with a cutoff range, with values within the cut-
off range considered inconclusive. A grid search was used to deter-
mine the optimal cutoff range representing a tradeoff between
inconclusive determination rate and accuracy over conclusive sub-
jects. The second approach was to train and cross-validate a simple
binary classifier, label the correctly and incorrectly predicted sam-
ples as conclusive or inconclusive respectively, and then build a sec-
ond classifier to predict whether a subject would be incorrectly
classified by the first classifier. At runtime, the second classifier was
used to spot and label inconclusives. The conclusives were sent for
classification by a third, binary classifier trained over the conclusive
samples only. Both methods for labeling inconclusive results yielded
similar performance. Therefore, the simpler method of using a
threshold range in the machine learning output was used to report
inconclusive results for this paper.
The inconclusive rate is a configurable model parameter that
controls the tradeoff between coverage and accuracy. Throughout
this paper, the inconclusive rate for this variant was set to %.
Video
The second of our two-method approach to autism screening is an
ML classifier that uses input answers about the presence and severity
of target behaviors among subjects. This information was provided
by an analyst upon viewing two or three -minute home videos of
children in semi-structured settings that are taken by parents on
their mobile phones. The classifier was trained on item-level data
from two of the ADOS modules (module : preverbal, module :
phrased speech) and corresponding clinical diagnosis.
Two decision forest ML classifiers were trained corresponding to
each ADOS module. For each classifier,  questions were selected
using the same robust feature selection method, and the same allow-
ance for inconclusive outcomes was made as for the parental ques-
tionnaire classifier. Each model was independently parameter-tuned
with a bootstrapped grid search. Class labels were used to stratify
the cross-validation folds, and (age, label) pairs were used to weight-
balance the samples.
Problems related to the change of environment from training to
application are especially significant in the case of video screening
because ADOS involves a  minute direct observation of the child
by experts, whereas our screening was based on unsupervised short
home videos. Specifically, we expect the likelihood of inconclusive
or unobserved behaviors and symptoms to be much higher in the ap-
plication than in the training data, and the assessed level of severity
or frequency of observed symptoms to be less reliable in the applica-
tion than in the training data. The following improvements were
designed to help overcome these limitations.
Presence of Behavior Encoding
To minimize potential bias from a video analyst misreading the se-
verity of a symptom in a short cell phone video, this encoding
scheme improves feature reliability at the expense of feature infor-
mation content by collapsing all severity gradations of a question
into one binary value representing the presence vs absence of the be-
havior or symptom in question. Importantly, a value of  denotes
the presence of behavior, regardless of whether the behavior is indic-
ative of autism or of normalcy. This rule ensures that a value of 
corresponds to a reliable observation, whereas a  does not necessar-
ily indicate the absence of a symptom but possibly the failure to ob-
serve the symptom within the short window of observation.
Missing Value Injection to Balance the Nonpresence of Features for
the Video Screener Training Data
While collapsing severity gradations into a single category over-
comes noisy severity assessment, it does not help with the problem
of a symptom not present or unnoticeable in a short home video.
For this reason, it is important that the learning algorithm treat a
value of  as semantically meaningful, and a value of  as inconse-
quential. To this end, we augmented the training set with duplicate
samples that had some feature values flipped from  to . The injec-
tion of s was randomly performed with probabilities such that the
sample-weighted ratio of positive to negative samples for which the
value of any particular feature is  is about %. Such ratios ensure
that the trees in a random forest will be much less likely to draw
conclusions from the absence of a feature.
Combination
It is desirable to combine the questionnaire and video screeners to
achieve higher accuracy. However, the needed overlapping training
set was not available. Instead, the clinical validation dataset itself
was used to train the combination model.
The numerical responses of each of the parent questionnaire and
video classifiers were combined using L-regularized logistic regres-
sion, which has the advantage of reducing the concern of overfitting,
particularly given the logistic model has only three free parameters.
Bootstrapping and cross -validation studies showed that any overfit-
ting that may be present from this procedure is not detectable within
statistical limitations. Since each of the individual methods was
siloed by age, separate combination algorithms were trained per age
group silo. For each combination algorithm, optimal inconclusive
output criteria were chosen using the logistic regression response,
using the same techniques as for the parental questionnaire and
video classifiers. The performance characteristics of the overall
screening process compared to standard alternative screeners are
shown below.
RESULTS
Parent Questionnaire Performance on Training Data
Bootstrapped cross-validation performance metrics for the optimally
parameter-tuned version of each of the variants of the parental

Journal of the American Medical Informatics Association, , Vol. , No. 
questionnaire are reported in the top of Table . The results for
baseline variant are reported as a range rather than a single value,
because the unreliability of generic feature selection leads to differ-
ent sets of features selected from run to run, with varying perfor-
mance results.
Parents of children included in the clinical study answered short,
age-appropriate questions chosen using the robust feature selection
method discussed above. The clinical performance metrics for each of
the classification variants that build upon that feature selection scheme
are shown in the bottom of Table . The difference in performance be-
tween the training and validation datasets is driven by the differences
that are emphasized in Table . See below and the results of Table 
for a discussion of the statistical significance of these results.
ROC curves in Figure  show how our parent questionnaire clas-
sification approach outperforms some of the established screening
tools like MCHAT and CBCL on the clinical sample. Since clinical
centers are usually interested in screening tools with a high sensitiv-
ity, we have drawn shaded regions between % and % sensitiv-
ity to aid the eye.
Combination Screening Performance on Clinical Data
ROC curves in Figure  show how combining the questionnaire and
video classifiers into a single assessment further boosted perfor-
mance on the clinical study sample. When up to % of the most
challenging cases are allowed to be determined, inconclusive the per-
formance on the remaining cases is shown in Figure . Note that the
ROC curves in these figures for M-CHAT contain only younger chil-
dren (mostly under four years of age) due to the fact that this instru-
ment is not intended for older children. A same-sample comparison
between M-CHAT and the ML screeners can be seen in the age
binned figures .
Results for Young Children
Young children are of particular interest given the desire to identify
autism as early as possible. Results restricted to only children less
than four years old are shown in Figures  and .
Statistical Significance
For the training data, sample sizes are large enough that statistical
limitations are minimal. However, results reported for the clinical
data have significant statistical limitations. In this section we com-
pare the performance of the screening algorithms on the clinical
data that we have discussed in this paper:  the questionnaire-
based
algorithm
of,

M-CHAT,

CBCL,

the
questionnaire-based algorithm of this paper, and  the combined
questionnaire plus video algorithm of this paper. Direct comparisons
in performance between many of these algorithms are reported
along with statistical significances in Table .
DISCUSSION
We have introduced a novel machine learning algorithm based on a
parental questionnaire and another based on short home videos
recorded by parents and scored by a minimally trained analyst. We
have discussed pitfalls such as data sparsity, and the mixed ordinal
and categorical nature of the questions in our training data. We
have also identified several important confounding factors that arise
from differences between the training and application settings of the
algorithms. We have shown novel feature encoding, feature selec-
tion, and feature aggregation techniques to address these challenges,
and have quantified their benefits. We have shown the benefits of
allowing some subjects with lower certainty output from the algo-
rithms to be classified as inconclusive. We have also shown the bene-
fits of combining the results of the two algorithms into a single
determination.
By specializing the machine learning models on a dichotomy of
age groups, we found that the screener for younger children capital-
ized on non-verbal behavioral features such as eye contact, gestures,
and facial expressions, while the screener for older children focused
more on verbal communication and interactions with other children.
For more details please refer to the Supplementary Material.
The methods and resulting improvements shown in this paper
are expected to translate well into other clinical science applications
Table . Performance Comparisons Between Various Algorithms on Clinical Data
Base model
Model from this paper
AUC improvement
Mean recall improvement
 publication
Questionnaire
., 
M-CHAT
Questionnaire
., 
CBCL
Questionnaire
., 
 publication
Questionnaire & video
., 
M-CHAT
Questionnaire & video
., 
CBCL
Questionnaire & video
., 
 publication
Questionnaire þ inconclusive
., 
M-CHAT
Questionnaire þ inconclusive
., 
., 
CBCL
Questionnaire þ inconclusive
., 
 publication
Questionnaire & video þ inconclusive
., 
M-CHAT
Questionnaire & video þ inconclusive
., 
CBCL
Questionnaire & video þ inconclusive
., 
Questionnaire
Questionnaire & video
., 
Questionnaire
Questionnaire þ inconclusive
., 
., 
Questionnaire
Questionnaire & video þ inconclusive
., 
Q. and video
Questionnaire & video þ inconclusive
., 
Each row evaluates the improvement of one of the algorithms from this paper over a “Base model” algorithm for the AUC metric, and for the average between
the autism and the non-autism recalls at a response threshold point that achieves approximately % sensitivity. Negative values would represent a worsening of
performance for a given algorithm compared to the base model. Both average values of the improvements and  conﬁdence intervals are reported. Algo-
rithms that are labeled “inconclusive” allow up to % of the most difﬁcult samples to be discarded from the metric evaluation. Note that the M-CHAT instru-
ment is intended for use on younger children. Therefore, older children were excluded when preforming comparisons to M-CHAT in this table.
Journal of the American Medical Informatics Association, , Vol. , No. 

Figure . ROC curves on the clinical sample for various questionnaire based autism screening techniques, ordered from the least to most sophisticated. Note that
unlike Figures  through  and ,  children are included in this sample (six children did not have videos available).
Figure . ROC curves on the clinical sample for the questionnaire and the video based algorithms, separately and in combination. The established screening tools
MCHAT and CBCL are included as baselines.
Figure . ROC curves on the clinical sample for the questionnaire and the video based algorithms, separately and in combination. Inconclusive determination is
allowed for up to % of the cases. The established screening tools MCHAT and CBCL are included as baselines.
Figure . ROC curves on the clinical results for children under four years of age, for the questionnaire and the video based algorithms, as well as the combination.
Comparisons with the established (nonmachine learning) screening tools MCHAT and CBCL are also shown.

Journal of the American Medical Informatics Association, , Vol. , No. 
including screening for cognitive conditions such as dementia for the
elderly and physical conditions such as concussions in adults. Fur-
ther, we expect that these methods would apply well to any other
survey based domain in which the application context is different
from the training context.
Significant further improvements may be possible. Initial studies
have identified probable improvements to the machine learning
methodology as well as improved methods for handling the biases
between the training data and application settings. A new clinical
trial with larger sample sizes is underway that will make it possible
to validate new improvements resulting from these studies as well as
to improve confidence in the high performance of our algorithms.
CONCLUSION
Machine learning can play a very important role in improving the ef-
fectiveness of behavioral health screeners. We have achieved a sig-
nificant improvement over established screening tools for autism in
children as demonstrated in a multi-center clinical trial. We have
also shown some important pitfalls when applying machine learning
in this domain, and quantified the benefit of applying proper solu-
tions to address them.
FUNDING
This research received no specific grant from any funding agency in the pub-
lic, commercial or not-for-profit sectors.
COMPETING INTERESTS
All authors are affiliated with Cognoa Inc. in an employment and/or
advisory capacity.
CONTRIBUTORS
All listed authors contributed to the study design as well as the draft-
ing and revisions of the paper. All authors approve of the final ver-
sion of the paper to be published and agree to be accountable for all
aspects of the work.
SUPPLEMENTARY MATERIAL
Supplementary material is available at Journal of the American
Medical Informatics Association online.
REFERENCES
.
Durkin MS, Maenner MJ, Meaney FJ. Socioeconomic inequality in the
prevalence of autism spectrum disorder: evidence from a U.S. cross-
sectional study. PLoS One ;  : e.
.
Christensen DL, Baio J, Braun KV, et al. Prevalence and characteristics of
autism spectrum disorder among children aged  years—autism and devel-
opmental disabilities monitoring network,  sites, United States, .
MMWR Surveill Summ ;  : –.
.
Zwaigenbaum L, Bryson S, Lord C, et al. Clinical assessment and
management of toddlers with suspected autism spectrum disorder:
insights from studies of high-risk infants. Pediatrics ;  :
–.
.
BernierMao RA, Yen J. Diagnosing autism spectrum disorders in primary
care. Practitioner ;  : –.
.
Achenbach TM, Rescorla LA. Manual for the ASEBA School-Age Forms
& Proﬁles. Burlington, VT: University of Vermont, Research Center for
Children, Youth, & Families. .
.
Lord C, Rutter M, Le Couteur A. Autism diagnostic interview-revised: a
revised version of a diagnostic interview for caregivers of individuals with
possible pervasive developmental disorders. J Autism Dev Disord ;
 : –.
.
Lord C, Rutter M, Goode S, et al. Autism diagnostic observation schedule:
a standardized observation of communicative and social behavior. J Au-
tism Dev Disord ;  : –.
.
Lord C, Petkova E, Hus V, et al. A multisite study of the clinical diagnosis
of different autism spectrum disorders. Arch Gen Psychiatry ;  :
–.
.
Wall DP, Dally R, Luyster R, et al. Use of artiﬁcial intelligence to shorten
the behavioral diagnosis of autism. PLoS One ;  : e.
. Duda M, Kosmicki JA, Wall DP, et al. Testing the accuracy of an
observation-based classiﬁer for rapid detection of autism risk. Transl Psy-
chiatry ;  : e.
. DudaDaniels MJ, Wall DP. Clinical evaluation of a novel and mobile au-
tism risk assessment. J Autism Dev Disord ;  : –.
. Fusaro VA, Daniels J, Duda M, et al. The potential of accelerating early
detection of autism through content analysis of youtube videos. PLoS One
; ; : e.
. Cognoa, Inc. Palo Alto: CA. 
Figure . ROC curves on the clinical results for children under four years of age, for the questionnaire and the video based algorithms, as well as the combination,
restricted to the children who were not determined to have an inconclusive outcome (tuned to have at most % allowed to be inconclusive). Comparisons with
the established (nonmachine learning) screening tools MCHAT and CBCL are also shown.
Journal of the American Medical Informatics Association, , Vol. , No. 


Scientific Reports |          :  | 

Multi-modular AI Approach to 
Streamline Autism Diagnosis in 
Young Children
Halim Abbas   
, Ford Garberson   
, Stuart Liu-Mayo   
, Eric Glover* & Dennis P. Wall   

Autism has become a pressing healthcare challenge. The instruments used to aid diagnosis are time 
and labor expensive and require trained clinicians to administer, leading to long wait times for at-risk 
children. We present a multi-modular, machine learning-based assessment of autism comprising three 
complementary modules for a unified outcome of diagnostic-grade reliability: A -minute, parent-
report questionnaire delivered via a mobile app, a list of key behaviors identified from -minute, semi-
structured home videos of children, and a -minute questionnaire presented to the clinician at the 
time of clinical assessment. We demonstrate the assessment reliability in a blinded, multi-site clinical 
study on children - months of age  in the United States. It outperforms baseline screeners 
administered to children by .  in AUC and .  in 
specificity when operating at % sensitivity. Compared to the baseline screeners evaluated on children 
less than  months of age, our assessment outperforms the most accurate by . (% CI: . to . 
at %) in AUC and .  in specificity when operating at % sensitivity.
Idiopathic forms of Autism Spectrum Disorder (ASD) have no known biological cause and may correspond to 
multiple conditions with similar symptoms. The incidence of ASD has increased in recent years, and it impacts  
in  children according to the latest studies. ASD is diagnosed from clinical observations according to standard 
criteria relating to the child’s social and behavioral symptoms. Autism is said to be on a spectrum due to the 
varied severities of symptoms, ranging from relatively mild social impairment to debilitating intellectual disabil-
ities, inabilities to change routines and severe sensory reactions. Approximately –% of autistic children are 
non-verbal and have severe symptoms.
Notably, diagnosis within the first few years of life dramatically improves the outlook of children with autism, 
as it allows for treatment during a key window of developmental plasticity. Unfortunately, the latest studies 
show that although % of parents of children with autism reported developmental concerns about their chil-
dren by  months of age, the median age of diagnosis in the United States is  months. The complexity of the 
diagnostic procedures and the shortage of trained specialists can result in children with ASD not getting a diag-
nosis early enough to receive behavioral therapies during the time when they are most effective.
Diagnosing autism in the United States generally takes two steps: developmental screening followed by com-
prehensive diagnostic evaluation if screened positive. Screening instruments typically use questionnaires that 
are answered by a parent, teacher or clinician. They are generally easy and inexpensive to administer and can 
be useful to flag some at-risk children, however, they are not always accurate enough to help inform a diagnosis. 
Standard autism screeners can also have a high false positive rate, leading to unnecessary referrals and healthcare 
costs. Comprehensive diagnostic evaluation instruments, on the other hand, are more accurate but require long 
and expensive interactions with highly trained clinicians.
In this paper, we present improvements to two previously published automated autism assessment modules 
underlying the Cognoa software. The first module is based on a brief questionnaire about the child presented 
directly to parents without supervision. The second module is based on lightly trained analysts evaluating short 
videos of children within their natural environment that are captured by parents using a mobile device. We also 
present a new, third module that is intended to be completed in a primary care setting such as a pediatrician’s 
office during a clinic visit. The third module is based upon a questionnaire that is answered by a clinician after 
examining the child and talking to the parent. We demonstrate that these three modules are as fast and easy to 
Cognoa Inc., Palo Alto, CA, USA. Departments of Pediatrics, Biomedical Data Science and Psychiatry and 
Behavioral Sciences, Stanford University, Stanford, CA, USA. *email: 
OPEN

Scientific Reports |          :  | 

/
administer as most of the typical screening instruments, yet their combined assessment accuracy is shown in this 
work to be significantly higher, such that they may be used to aid in diagnosis of autism.
We present our approach to selecting maximally predictive features for each of the modules. Both the parent 
and the clinician questionnaire modules key on behavioral patterns similar to those probed by a standard autism 
diagnostic instrument, the Autism Diagnostic Interview - Revised (ADI-R). ADI-R is administered by a trained 
clinician, and typically gives consistent results across examiners. But its  point questionnaire often spanning 
. hours of the interviewer and parent’s time makes it largely impractical for the primary care setting. The 
video assessment module keys on behavioral patterns similar to those probed in another diagnostic instrument, 
the Autism Diagnostic Observation Schedule (ADOS). ADOS is a multi-modular diagnostic instrument, with 
different modules for subjects at different levels of cognitive development. It is widely considered a gold standard 
and is one of the most common behavioral instruments used to aid in the diagnosis of autism. It consists of an 
interactive and structured examination of the child by trained clinicians in a tightly controlled setting.
For validation, the three modules are applied to assess children in a clinical study using the Cognoa software. 
To-date, Cognoa has been used by over  parents in the US and internationally. The majority of Cognoa 
users are parents of young children between  and  months. The clinical study underlying the validation 
results discussed in the results section consists of a total of  at-risk children who had undergone full clinical 
examination and received a clinical diagnosis at a center specialized in neurodevelopmental disorders. The 
outputs of the assessment modules are compared to those of three screening instruments. The Modified Checklist 
for Autism in Toddlers, Revised (M-CHAT-R) is a parent-completed questionnaire for autism that is intended 
to be administered during developmental screenings for children between the ages of  and  months and is 
commonly used as an autism screening instrument. The Social Responsiveness Scale - Second Edition (SRS) is 
another standard ASD screener that is based upon a questionnaire filled out by an examiner–. The SRS has a 
preschool form intended for children of ages  months to  months, and a school age form intended for chil-
dren of ages  months through  years of age. We use SRS “total score” scale as a baseline autism assessment. 
The Child Behavior Checklist (CBCL) is a parent-completed questionnaire that provides risk assessments in 
many categories. We use the “
Autism Spectrum Problems” scale of CBCL for comparison. In all cases, the answers 
to the questions comprising the screeners are coded, then the codes are summed and the sum compared against 
a threshold to determine whether the child is at risk.
Methods
We base our approach on de-identified historical patient records. We collect medical instrument score sheet data 
pertaining to children tested for suspicion of autism, and process those into training sets for the predictive models 
underlying each of our three autism assessment modules.
Since we apply said predictive models in a significantly different setting than the clinics where the correspond-
ing training data were generated, we expect a consequential performance degradation resulting in unacceptable 
diagnostic accuracy if conventional machine learning methods are used. To counteract that effect, we apply 
custom machine learning techniques as detailed in this section, building upon previous experimental work. The 
new techniques discussed below are empirical post-hoc feature selection, training data noise injection, and an 
overfitting-resilient probabilistic combination of module outcomes.
Data. 
Training data were compiled from multiple repositories of de-identified ADOS and ADI-R score sheets 
of children between  and  months of age including Boston Autism Consortium, Autism Genetic Resource 
Exchange, Autism Treatment Network, Simons Simplex Collection, and Vanderbilt Medical Center. To counteract 
class imbalance, the sample set negative class was supplemented with  low risk children random-sampled from 
Cognoa’s user-base, and ADI-R was administered on those additional controls.
The diagnostic accuracy of our modules was measured using data from a multi-site blinded clinical validation 
study . The study was performed in  and 
 at three tertiary care centers in the United States. Informed consent was obtained from guardians of each 
child, and all relevant regulations and guidelines were followed. Children enrolled in the study were  to  
months of age, of English-speaking households, and were all referred through the typical referral process for sus-
picion of autism. Every child was measured using autism assessment instruments (such as ADOS, M-CHAT-R, 
and/or CBCL) as appropriate for his or her age. Diagnosis was ultimately ascertained by a licensed health care 
provider. Prior to the clinical assessment, parents used the Cognoa mobile app to complete the parent question-
naire and video assessment modules, and starting in , a clinician also completed the Cognoa clinician ques-
tionnaire. The clinicians were blinded to the results of the assessment rendered by Cognoa. More details on the 
steps of the clinical study are shown in Fig. .
The enrollment process in  yielded  validation samples, which were used to validate the parent ques-
tionnaire and video modules. This same clinical enrollment cohort was used as validation dataset in our previous 
publication on the subject. Given the learnings from this dataset, and prior to the extension of the study in , 
several improvements were made to the algorithms including tuning of model thresholds, training combination 
modules, and performing feature selection for the clinician module which was newly introduced in . The 
enrollment process in  yielded  additional validation participants, bringing the total N to  samples 
over the course of the two years.
The sample breakdown by cohort, age group, and diagnosis for all data used for training and validation is 
shown in Table . In both the training and the validation datasets, the majority of the “Not autism” class label is 
composed mostly of children who are diagnosed with an alternate developmental delay (e.g., ADHD or speech 
and language disorder). Since these conditions share many symptoms with autism, this is a particularly challeng-
ing sample for classification . Only seven of the children in the validation sample are neurotypical, suggesting that 
this sample will be harder to perform correct classifications on than in the general population.

Scientific Reports |          :  | 

/
Algorithm methodology. 
In this section we explain important aspects of our machine learning methodol-
ogy that are common to the classifiers underlying each of our three assessment modules.
Training procedure.  Classifier training, feature selection, and optimization were done separately for children 
under four years of age and four years of age and over. The parent questionnaire and clinician questionnaire clas-
sifiers make predictions based off of answers to questions that probe similar concepts to the ADI-R questionnaire. 
They were trained using the answers to questions from historical item-level ADI-R score sheets with labels corre-
sponding to established clinical diagnoses. The video module makes predictions based off of answers to questions 
that probe similar concepts to the ADOS instrument, as recorded by video analysts. It was trained using ADOS 
instrument score sheets and diagnostic labels. Progressive sampling was used to verify sufficient training volume 
as detailed in the supplementary materials. Gradient boosted decision trees were used for all three modules as 
they consistently performed better than other options that were considered such as neural networks, support 
vector machines, and logistic regression. For all models, hyper-parameters were tuned with a bootstrapped grid 
search. In all cases, true class labels (ASD or non-ASD) were used to stratify the folds, and (age, label) pairs were 
used to weight-balance the samples. More details can be found in the supplementary materials.
In all cases, the machine learning models were trained using historical patient records that correspond to con-
trolled clinical examinations, but focused on application in non-clinical settings aimed for brevity, ease-of-use, 
and/or unsupervised parent usage at home. These differences introduce biases which can be significant enough to 
ruin the performance of an algorithm if not properly addressed, and which cannot be probed by cross validation. 
See the supplementary material for further details. New strategies to address these biases are discussed below that 
result in big improvements in accuracy compared to previous work.
Figure .  Detailed steps performed during the clinical study described in this document.
Table .  Dataset breakdown by age group and condition for each of the sources of training data and for the 
clinical validation sample. Machine learning model training was stratified by age group. Clinical validation  
and  samples are used together to evaluate performance of the parent and video modules in this paper, 
while the clinician module was only available for the clinical  dataset.

Scientific Reports |          :  | 

/
Inconclusive outcomes.  Each of the three modules predicts one of three assessment outcomes: Positive, negative, 
and inconclusive. As outlined in Fig. , support for inconclusive determination is incorporated using a process 
that involves three separate machine learning training runs. The first model is trained to make predictions that are 
used to label the samples in the training data that are the most likely to be misclassified. A second model is then 
trained using these labels to predict the likelihood of any new samples being misclassified. Finally, only samples 
that are likely to be classified correctly are used to train a final, binary autism classifier. Only the latter two models 
are used at prediction time: the one to identify and filter out samples that should be labeled “inconclusive”
, and the 
other to make a binary prediction of whether the child is autistic in those which are likely to be correctly labeled. 
More details about how these models are trained are available in the supplementary material.
Parental module. 
Initial feature selection.  The parental questionnaire probes for a minimal set of highly 
relevant child behavioral patterns that are maximally predictive of autism in combination. Care is taken to phrase 
the questions and answers such that the most reliable signal can be input from everyday parents undertaking the 
questionnaire via mobile app without clinical assistance.
To that effect, a custom feature selection method is devised involving robust bootstrap-driven backwards 
subtraction, the details of which are discussed in a previous publication. Out of an initial set of  questions 
under consideration, this produces an optimal set of  novel questions for children less than four years old, and 
 questions for children four and older.
Empirical post-hoc feature selection refinement.  Following the conclusion of the  clinical validation study 
enrollment, we studied differences in the distribution of answers to each question between the training data and 
the validation data that was collected in the  clinical study. While some questions had quite good agree-
ment, others show a strong bias towards higher (or lower) severity answer choices in the clinical data than in 
the training data. Questions for which the mean absolute severity difference was statistically greater than three 
standard errors (averaged over the autism and the non-autism samples) are rejected. This requirement results in 
the exclusion of  out of the  questions in the younger cohort, and  out of  questions in the older cohort, and 
the models are re-trained (with new hyper-parameter tuning) on the reduced feature set. This further refinement 
of the selected features minimizes the significant biases due to differences between the training and application 
environment. See the supplementary material for more details on these differences.
This feature refinement leads to a larger boost in performance compared with than any other improvement. 
The size of the performance improvement is validated on the held-out sample of children collected during , 
where the new models show a statistically equivalent increase in performance compared with the  sample.
Video module. 
The video assessment module consists of a parent upload of  or  mobile videos, each  to 
 minutes in length, of the child during play or meal time at home. The underlying algorithm produces autism 
assessments based upon the responses of at least three minimally-trained analysts who watch the videos and then 
respond to a behavioral questionnaire.
The data available for training the video module’s classification model are taken from ADOS sessions admin-
istered by clinicians in standardized clinical settings. Gradient boosted decision trees are trained keying off of the 
features identified in the analysis of ADOS records. The questionnaires that the video analysts answer are then 
created to probe for similar behavioral features as those observed in the training data. A challenge of this method-
ology is that the module must make predictions in the face of missing features that are not observable in the short 
videos uploaded by the parents. The video analysts are allowed to skip any questions if not answerable based on 
Figure .  An illustration of the methodology for training diagnostic assessment algorithms capable of 
outputting one of three possible outcomes: “positive”
, “negative”
, or “inconclusive”
. The first binary classifier is 
only used to assist in training and never at runtime. It is trained to predict binary “autism” vs “not autism”
, and 
these labels are then compared with the true ASD results to label which samples are incorrectly classified. The 
samples with their “correct” and “incorrect” labels are used to train the classifiers at runtime. A “indeterminate” 
classifier is trained to predict which samples will have their ASD diagnosis misclassified, which serves as a filter 
to identify “inconclusive” cases at runtime, while only the predicted “correct” samples are used to train the final 
binary ASD diagnosis classifier.

Scientific Reports |          :  | 

/
the posted videos. On average, analysts skipped questions % of the time, with big variations among particular 
questions. This effect, combined with the large discrepancy in the observation time window from the original 
clinical examination to the brief home-video version, would result in a big assessment-accuracy degradation 
unless steps are taken to correct for the bias and variance .
We tackle this problem by introducing bias and variance to the training data in a manner designed to make it statis-
tically similar to the video analyst answers on which the assessments will be run. The data from the  clinical study 
is used to develop this methodology, and the performance of the algorithm on the data from the children enrolled in 
 is used to validate the generalizability of the improvements. Most children who participated in the clinical study 
are also administered a full ADOS, which provided paired ADOS and video data that we use to determine what noise 
patterns to add. Using these paired data, we construct a probability map for each question-response set describing the 
ways video analysts are likely to respond for a given “true” ADOS response. We then use the mapping as a stochastic 
transform to build a new training data set that can be thought of as the results of a hypothetical experiment in which 
the technicians watch parent-supplied video of the children in the training data and respond accordingly.
The addition of simulated “setting noise” to the classifier training data leads to a larger boost in performance 
compared with than any other improvement. Additionally, the optimal parameters for the resulting decision 
tree models favor larger tree depth. This is as expected, since the new models are expected to make determina-
tions as to which features are reliable when present, as well as which features to fall back on when the best features 
are missing.
Clinician module. 
We introduce a module to screen for autism using questionnaire responses from a cli-
nician. A pediatrician might answer these questions during a regular checkup. The questions for the clinician 
were selected in a similar manner as used for the Parental Module (see the supplementary material for details). 
Responses from both the parent and the clinician are used in a machine learning module in the same manner as 
described for the parental questionnaire above. Some key behaviors are probed via questions directed at both the 
parent and the clinician, but the clinician questions are more nuanced and allow for more subtle answer choices. 
In cases where the parent and the clinician give contradictory answers to the same question, the clinician’s answer 
overrides that of the parent. The clinician module was introduced to the clinical validation study beginning in 
. Its results are therefore based on a smaller sample size than those of the other modules.
Feature selection.  In order to create a brief clinician questionnaire appropriate for the primary care setting, 
multiple lists of candidate questions are each compiled and ordered using different strategies. The lists are then 
intersected and prioritized, then the top features in the intersection set are shortlisted. The first list of candidate 
questions is prepared by considering those questions from the original medical instruments that had been excluded 
from the parental questionnaire because they were deemed too difficult for a parent to answer reliably. This list is 
ranked by feature importance values as measured and ranked by a dedicated offline machine learning training and 
cross validation run in the same manner as performed for initial parental module feature selection. The second list 
is prepared from the parental questionnaire questions by simulating the effects of parents over or underestimating 
answer severities on children with machine learning responses near a decision threshold. Children in the training 
data for whom the model response was between  above the ASD-vs-non ASD decision threshold had 
their question severities dropped one at a time by one severity value, while children who were between  
below the decision threshold had their question severities raised by one severity category. The questions in this list 
are then ranked based upon the average size of the resulting shift in model responses. The procedure is repeated 
for children in the training data between  above or below the decision threshold. In each case the top 
 questions are selected (with significant overlap). This results to a total of  candidate questions for young chil-
dren and  for older children. The third list is prepared by consulting domain experts for an assessment of the 
likelihood of each candidate question to benefit from a clinician’s input as a complement to the parent’s input. This 
method is conducted separately for each of the two age-silo groups, and results in an overall clinician questionnaire 
of  questions for children  through  month old, and  questions for children  to six years old.
Module combination. 
Due to limitations on available training data, it is not possible to train a single 
combined model that uses the input features from each of the parental, video, and clinician modules. Instead, 
responses from the modules are each considered to be a probability and combined mathematically using the 
equation:
=
Σ
∗
Σ
−
−
−
r
I
R
I
I
(
)
(
)

comb
T
T



Where rcomb is the result of the combination, I is a vector of s, R is a vector of responses for each module to be 
combined, and Σ is the covariance matrix of the response residuals compared to the true diagnosis. The “training” 
of the combination module consists of calculating the values of Σ to use in this equation, which is done using the 
responses of each module on data from the clinical study. For each child, the Σ values in the rcomb equation were 
calculated with that child excluded. This process is similar to leave-one-out cross validation, and ensures that the 
results reported for our combination procedure do not suffer from overfitting.
Since Eq.  produces only a single model response, the determination of “inconclusive” outcomes proceeds 
in a different manner than for the individual assessment modules. Both a lower and an upper threshold are 
applied on the combined response. Children with a response less than both thresholds are considered to be 
non-ASD, children with a response in between the two thresholds are considered to be inconclusive, and children 
with a response greater than both thresholds are considered to have ASD. As in the single model cases, the two 
thresholds can be tuned independently to optimize the sensitivity, specificity, and model coverage.

Scientific Reports |          :  | 

/
Results
Each of the individual Cognoa assessment modules, their combinations, as well as  baselines based on 
commonly-used autism screening instruments (CBCL, M-CHAT-R, and SRS) are evaluated on the data collected 
during a blinded clinical study. When the inconclusive determination feature is turned off and all samples are 
required to be assessed conclusively, the Cognoa assessment modules achieve ROC AUC up to . and sensi-
tivity and specificity up to % and % respectively. Turning on the inconclusive determination feature with an 
allowance of up to % inconclusive outcomes results in an accuracy improvement over the conclusive samples, 
with AUC up to . with sensitivity and specificity up to % and % respectively. This performance is shown 
to be a statistically significant improvement over each of the baselines used for comparison.
ROC curves in Fig.  show how the parent module performs individually, as well as in combination with the 
video and clinician modules at a % inconclusive rate allowance. Figure  shows a similar comparison with all 
the variants consistently restricted to children under four years of age. ROC curves corresponding to the assess-
ment modules with the inconclusive allowance turned off can be found in the supplementary material.
Statistical model performance comparisons between assessment modules and baselines are shown in Table . 
For each comparison, the subset of children for whom both screeners were administered are selected (n in the 
table), and  bootstrapping experiments are run where n children are selected with replacement. The average 
and  confidence interval improvements in AUC and the specificity between the screeners are evaluated 
across all bootstrapping experiments. In the case of specificity the calculation of the improvement is performed 
using thresholds that are set to achieve % sensitivity. 
Table  shows that Cognoa modules show an improvement of at least . in AUC and at least . in spec-
ificity compared with the CBCL and SRS- screeners at % confidence level. Due to the fact that M-CHAT-R 
screener is only evaluated on younger children the statistical uncertainty in the comparison is larger, however, 
it also shows an improvement of at least . in AUC and . in specificity at % confidence level. In these 
comparisons we allow Cognoa assessment modules to decide to hold aside up to % of the hardest cases as 
inconclusive. The same comparisons when we force the classification on all of the hardest cases can be found in 
Table  of the supplementary material.
Figure .  ROC curves on the clinical sample for the parent, video, and clinician modules, separately and in 
combination. Inconclusive determination is allowed for up to % of the cases. The established screening 
tools M-CHAT-R, SRS- and CBCL are compared as baselines. The ROC curve for the M-CHAT-R baseline 
instrument only includes children under four years of age since M-CHAT-R is not applicable for older children.
Figure .  ROC curves on kids  <  years of age in the clinical sample for the parent, video, and clinician 
modules, separately and in combination. Inconclusive determination is allowed for up to % of the cases. The 
established screening tools M-CHAT-R, SRS- and CBCL are compared as baselines.

Scientific Reports |          :  | 

/
Time to completion comparisons. 
A random sample of  Cognoa users was used in order to measure 
time to completion of each of the Cognoa autism assessment modules. The median time to completion of the 
parent module was just under  minutes. The median time to completion of the clinician module was . minutes. 
The median time per video analysts to score a videos was  minutes. More details can be found in the supple-
mentary material. The results indicate that the parent and clinician modules can be completed in as little time as 
most established autism screeners and in some cases much faster, while achieving significantly higher accuracy. 
The time required for a video analyst to score a video is more lengthy, however, the turnaround time is faster 
than for an ADOS administration and can be performed by minimally trained analysts as opposed to certified 
clinical practitioners.
Discussion
We presented a multi-modular assessment consisting of three machine learning modules for the identification of 
autism via mobile App as well as an evaluation of their performance and time-to-completion in a blinded clinical 
study. The assessment modules outperform conventional autism screeners, as shown in Table  and Fig. . The 
accuracy of the combined assessment is similar to that of gold-standard instruments such as ADOS and ADI-R, 
without requiring hours of time from certified clinical practitioners. This suggests the potential for the Cognoa 
assessment to be useful as an autism diagnostic. The high performance of these modules benefits from the use 
of the techniques described in this paper to identify and set aside up to % of the most challenging samples as 
inconclusive. The supplementary material of this paper shows that we outperform conventional autism screeners 
without this technique as well.
Important open questions remain. First, in all cases in this paper, the assessment modules were validated on 
children who had been preselected as having high risk of autism. Children that are pre-selected in this way tend to 
have autism-like characteristics regardless of their true diagnosis, increasing the challenge of distinguishing true 
ASD cases. These modules are expected to perform better on a general population sample of children. Further 
work is needed to verify this hypothesis by conducting clinical studies on children from the general population. 
Second, the clinician module newly presented in this work appears promising, but so far it has only been applied 
in a secondary-care setting. Further testing in primary care clinics is needed to validate accuracy in that setting. 
In addition, two wider avenues of exploration are interesting as further steps. First, while these assessment mod-
ules have been shown to be effective at identifying the presence or absence of autism, our goal is to extend them 
to identify the severity of the condition (if present) as well. Second, the techniques presented in this paper could 
potentially be used to build algorithms for other child behavioral conditions than autism, as well as behavioral 
conditions affecting adults and seniors.
Received:  March ; Accepted:  February ;
Table .  Statistical tests of performance improvements between models in this paper and standard baseline 
screening models. ΔAUC tells us the increase in AUC found in the screeners of this paper across bootstrapping 
experiments. ΔSpecificity tells us the increase in the specificity in the bootstrapping experiments at a threshold 
designed to achieve % sensitivity. Each Δ calculation shows the average value of the improvement along with 
the  confidence interval.

Scientific Reports |          :  | 

/
References
	 .	 Baio, J. et al. Prevalence of autism spectrum disorder among children aged  years— autism and developmental disabilities 
monitoring network,  sites, united states, . MMWR Surveill Summ , –, 
ssa .
	 .	  Association., A. P. & Association., A. P. Diagnostic and statistical manual of mental disorders : DSM- (American Psychiatric 
Association Arlington, VA, ), th ed. edn.
	 .	  Patten, E., Ausderau, K. K., Watson, L. R. & Baranek, G. T. Sensory response patterns in nonverbal children with asd. Autism Res. 
Treat.,  .
	 .	 Dawson, G. & Bernier, R. A quarter century of progress on the early detection and treatment of autism spectrum disorder. Dev. 
Psychopathol. , –,  .
	 .	  Dawson, G. et al. Randomized, controlled trial of an intervention for toddlers with autism: The early start denver model. Pediatrics 
, e–e   .
	 .	  Gordon-Lipkin, E., Foster, J. & Peacock, G. Whittling down the wait time exploring models to minimize the delay from initial 
concern to diagnosis and treatment of autism spectrum disorder. Pediatr. clinics North Am., – 
pcl... . Exported from  on //.
	 .	 Bernier, R., Mao, A. & Yen, J. Diagnosing autism spectrum disorders in primary care. Practitioner , – .
	 .	  Achenbach, T. & Rescorla, L. Manual for the ASEBA preschool forms & profiles. Univ. Vermont, Res. Cent. for Child. Youth & Fam. 
.
	 .	  Norris, M. & Lecavalier, L. Screening accuracy of level  autism spectrum disorder rating scales: A review of selected instruments. 
Autism, , –,  . PMID: ,
	
.	  Charman, T. & Gotham, K. Measurement issues: Screening and diagnostic instruments for autism spectrum disorders - lessons 
from research and practise. Child Adolesc. Mental Heal., –, 
	
.	 Lord, C., Rutter, M. & Le Couteur, A. Autism diagnostic interview-revised: a revised version of a diagnostic interview for caregivers 
of individuals with possible pervasive developmental disorders. J. Autism Dev. Disord. , – .
	
.	 Lord, C. et al. Autism diagnostic observation schedule: a standardized observation of communicative and social behavior. J Autism 
Dev Disord , – .
	
.	  Abbas, H., Garberson, F., Glover, E. & Wall, D. P. Machine learning approach for early detection of autism by combining 
questionnaire and home video screening. J. Am. Med. Informatics Assoc. ocy, ._jamia_ocy//ocy .
	
.	  Cognoa, Inc.  El Camino Real St , Palo Alto, CA  
	
.	  Wall, D. P., Dally, R. L., Luyster, R., Jung, J.-Y. & DeLuca, T. F. Use of artificial intelligence to shorten the behavioral diagnosis of 
autism. PLoS One,  .
	
.	 Lord, C. et al. A multisite study of the clinical diagnosis of different autism spectrum disorders. Arch. Gen. Psychiatry , – 
.
	
.	  Kanne, S., Arnstein Carpenter, L. & Warren, Z. Screening in toddlers and preschoolers at risk for autism spectrum disorder: 
Evaluating a novel mobile-health screening tool. Autism Res. .
	
.	 Moody, E. J. et al. Screening for autism with the srs and scq: Variations across demographic, developmental and behavioral factors 
in preschool children. J. Autism Dev. Disord. , – .
	
.	 Aldridge, F. J., Gibbs, V. M., Schmidhofer, K. & Williams, M. Investigating the Clinical Usefulness of the Social Responsiveness Scale 
(SRS) in a Tertiary Level, Autism Spectrum Disorder Specific Assessment Clinic. Journal of Autism and Developmental Disorders 
.
	
.	 Schanding, G. T., Nowell, K. P. & Goin-Kochel, R. P. Utility of the Social Communication Questionnaire-Current and Social 
Responsiveness Scale as Teacher-Report Screening Tools for Autism Spectrum Disorders. Journal of Autism and Developmental 
Disorders .
	
.	 Jacobs, R. A. Methods for combining experts’ probability assessments. Neural Computation , –, 
neco.... .
	
.	 Falkmer, T., Anderson, K., Falkmer, M. & Horlin, C. Diagnostic procedures in autism spectrum disorders: a systematic literature 
review. Eur. Child & Adolesc. Psychiatry , –,  .
Author contributions
Halim Abbas, Ford Garberson, and Stuart Liu-Mayo were each involved in all aspects of model development, 
optimization, training, and validation of the models for each of the modules, as well as the writing of this 
paper. Eric Glover and Dennis Wall provided advice on the development of the algorithms. Halim Abbas, Ford 
Garberson, Stuart Liu-Mayo and Dennis Wall co-wrote the manuscript.
Competing interests
All authors are affiliated with Cognoa Inc. in an employment and/or advisory capacity.
Additional information
Supplementary information is available for this paper at 
Correspondence and requests for materials should be addressed to E.G.
Reprints and permissions information is available at .
Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and 
institutional affiliations.
Open Access This article is licensed under a Creative Commons Attribution . International 
License, which permits use, sharing, adaptation, distribution and reproduction in any medium or 
format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Cre-
ative Commons license, and indicate if changes were made. The images or other third party material in this 
article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the 
material. If material is not included in the article’s Creative Commons license and your intended use is not per-
mitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the 
copyright holder. To view a copy of this license, visit 
 
© The Author(s) 
RESEARCH ARTICLE
Early screening of autism spectrum disorder
using cry features
Aida KhozaeiID, Hadi MoradiID*, Reshad HosseiniID, Hamidreza Pouretemad,
Bahareh Eskandari
 School of Electrical and Computer Engineering, University of Tehran, Tehran, Iran,  Intelligent Systems
Research Institute, SKKU, Suwon, South Korea,  Department of Psychology, Shahid Beheshti University,
Tehran, Iran
* 
Abstract
The increase in the number of children with autism and the importance of early autism inter-
vention has prompted researchers to perform automatic and early autism screening. Conse-
quently, in the present paper, a cry-based screening approach for children with Autism
Spectrum Disorder (ASD) is introduced which would provide both early and automatic
screening. During the study, we realized that ASD specific features are not necessarily
observable in all children with ASD and in all instances collected from each child. Therefore,
we proposed a new classification approach to be able to determine such features and their
corresponding instances. To test the proposed approach a set of data relating to children
between  to  months which had been recorded using high-quality voice recording
devices and typical smartphones at various locations such as homes and daycares was
studied. Then, after preprocessing, the approach was used to train a classifier, using data
for  boys with ASD and  Typically Developed (TD) boys. The trained classifier was
tested on the data of  boys and  girls with ASD and  TD boys and  TD girls. The sensi-
tivity, specificity, and precision of the proposed approach for boys were .%, %, and
.%, respectively. These measures were .%, %, and .% for girls, respec-
tively. It was shown that the proposed approach outperforms the common classification
methods. Furthermore, it demonstrated better results than the studies which used voice fea-
tures for screening ASD. To pilot the practicality of the proposed approach for early autism
screening, the trained classifier was tested on  participants between  to  months.
These  participants consisted of  boys and  girls and the results were very encourag-
ing for the use of the approach in early ASD screening.
Introduction
Children with Autism Spectrum Disorder (ASD) are defined by their abnormal or impaired
development in social interaction and communication, as well as restricted and repetitive
behaviors, interests, or activities . The rapid growth of ASD in the past  years has inspired
many research efforts toward the diagnosis and rehabilitation of ASD . In the field of
PLOS ONE
PLOS ONE | 
December , 
 / 
a
a
a
a
a
OPEN ACCESS
Citation: Khozaei A, Moradi H, Hosseini R,
Pouretemad H, Eskandari B  Early screening
of autism spectrum disorder using cry features.
PLoS ONE : e. 
./journal.pone.
Editor: Zhishun Wang, Columbia University,
UNITED STATES
Received: November , 
Accepted: October , 
Published: December , 
Peer Review History: PLOS recognizes the
benefits of transparency in the peer review
process; therefore, we enable the publication of
all of the content of peer review and author
responses alongside final, published articles. The
editorial history of this article is available here:

Copyright: ©  Khozaei et al. This is an open
access article distributed under the terms of the
Creative Commons Attribution License, which
permits unrestricted use, distribution, and
reproduction in any medium, provided the original
author and source are credited.
Data Availability Statement: The original and
cleaned voices and their extracted features (the
data set) in this research and the implementation
codes of the proposed method are deposited in the
following repositories: CodeOcean ./CO.
diagnosis, there are several well-established manual methods to diagnose children over 
months . However, the practical average age of diagnosis is over  years due to the lack of
knowledge about ASD and the lack of expertise for diagnosing autism . It is of the utmost
importance to have early diagnosis/screening in order to provide early intervention which is
more effective at the first few years of life than later on . It is shown that early inter-
vention improves the developmental performance in children with ASD . It has also been
reported that early interventions would be cost saving for families and the treatment service
systems . Consequently, there are two main questions: ) can autism be screened earlier
than  months to reduce the typical diagnosis or intervention age and ) is it possible to
employ intelligent methods for the screening of autism to eliminate the widespread need for
experts? It should be mentioned that our goal was to answer these questions with respect to
screening all children who may not have clear symptoms. The screened children should go
through a diagnosis procedure to acquire confirmation and/or be cautiously worked with.
Fortunately, there are studies in the literature showing that the age of diagnosis can be
lower than  months. For example, Thabtah and Peebles  reviewed several questionnaire-
based approaches that may be able to screen ASD above  months of age. However, those
approaches, like Autism Diagnostic Interview-Revised (ADI-R)  and Autism Diagnostic
Observation Schedule (ADOS)  which have been clinically proven to be effective and ade-
quate, are time-consuming instruments  and need trained practitioners to use them. To
reduce the dependency on the human expertise needed in using such questionnaires , sev-
eral studies proposed machine learning methods to classify children with ASD  using
questionnaires. Their goal was to automate the process and/or find an optimum subset of
questions or features. For instance, Abbas et al.  proposed a multi-modular assessment sys-
tem combined of three modules, a parent questionnaire, a clinician questionnaire, and a video
assessment module. Although the authors used machine learning to automate and improve
classification process, the need for human involvement still exists in order to answer questions
or assess videos.
On the other hand, Emerson et. al showed that fMRI  can be used to predict the diagno-
sis of autism at the age of  in high-risk -month-old infants. Denisova and Zhao  used
movement data from rs-fMRI from – month-old infants to predict future atypical develop-
mental trajectories as biological features. Furthermore, Bosl, Tager-Flusberg, and Nelson 
suggested that useful biomarkers can be extracted from EEG signals for early detection of
autism. Blood-based markers  and prenatal immune markers  were also proposed
to diagnose ASD that can be used right after birth. Although these approaches suggest new
directions towards early ASD diagnosis/screening, they are costly, require expertness and dedi-
cated equipment, which would limit their usage. Furthermore, these methods are still in the
early stages of research and require further approval. Finally, approaches which involve meth-
ods such as fMRI or EEG, are difficult to use on children, especially on children with autism
who may have trouble following instructions appropriately , have atypical behaviors ,
or have excessive head movements .
There are studies that used vocalization-based analysis to screen children with autism. For
instance, Brisson et al.  showed differences in voice features between children with ASD
and Typically Developing (TD) children. Several studies, like , used speech-related features
for the screening of children older than . To reach the goal of early ASD screening, vocaliza-
tions of infants under  years of age have been investigated . Santos et al.  used
vocalizations, such as babbling, to screen ASD children at the age of  months. They collected
data from  and  ASD and TD children, respectively. They reported high accuracy of
around % which can be due to the fact that they used k-fold cross-validation without consid-
ering subject-wise hold out in order to have unseen subjects in the test fold . Oller et al.
PLOS ONE
Early screening of autism using cry features
PLOS ONE | 
December , 
 / 
.v Harvard Dataverse (Contains only a rar
file of sounds): ./DVN/LSTBQW
Funding: HM received a small fund for collecting
data and for diagnosing the subjects. Grant
number  Cognitive Sciences and Technology
Council of Iran cogc.ir The funders had no role in
study design, data collection and analysis, decision
to publish, or preparation of the manuscript.
Competing interests: The authors have declared
that no competing interests exist.
 proposed another vocalization-based classification method in which they included age
and excluded crying. They applied the method on  TD children and  children with ASD
between  to  months and reached % accuracy. Pokorny et al.  extracted eGeMAPS
parameter set , which includes  acoustic parameters, in  month old children. This set
consists of statistics calculated for  frequency-related, energy-related, and spectral low-level
descriptors. They reached a % accuracy on a population of  TD children and  children
with ASD.
Esposito, Hiroi, and Scattoni  showed that crying is a promising biomarker for the
screening of ASD children. Sheinkopf et al.  have shown that there
are differences in the cry of children with ASD compared to TD children. To the best of our
knowledge, our own group’s preliminary study  was the only research that has used cry
sounds for the screening of children with ASD. We used a dataset of  children with ASD and
 TD children older than two years. The accuracy of the proposed method is .% using k-
fold cross validation without considering subject-wise hold out, which is a shortcoming of this
study. In other words, it has been overfitted to the available data and may fail to correctly clas-
sify new samples. So, a thorough examination using an unseen test set on cry features is neces-
sary to evaluate the results. It should be noted that the data from our previous study  could
not be used in the study presented in this paper due to the differences in data collection
procedures.
In all the above studies, it was assumed that the specific sound features, distinguishing chil-
dren with ASD from TD children, are common among all the ASD cases. However, this may
not be the case for all the features. For instance, tiptoe walking, which is one of the repetitive
behaviors of children with ASD, appears in approximately % of these children . Conse-
quently, in the current study, we propose a new cry-based approach for screening children
with ASD. Our screening approach makes use of the assumption that all discriminative charac-
teristics of autism may not appear in all ASD children. This assumption is in contrast with the
assumption put forward in the ordinary instance-based machine learning methods, which
assumes that all instances of a class include all discriminative features needed for classification.
In our proposed method, at first, discriminable instances of cries, which exist in subsets of chil-
dren with ASD, are found. Then it uses these instances to select features to distinguish between
these ASD instances from TD instances. It should be mentioned that the final selected features,
in this study, are common among our set of children with ASD between  to  months of
age. These selected features support the experiential knowledge of our experts stating that the
variations in the cries of children with ASD are more than TD children. This approach is dif-
ferent from the other approaches that either used a dataset of children with a specific age [,
] or used age information for classification . The proposed approach has been imple-
mented and tested on  participants. The results show the effectiveness of the approach with
respect to accuracy, sensitivity, and specificity.
Method
Since this study was performed on human subjects, first, it was approved by the ethics commit-
tee at Shahid Beheshti University of Medical Sciences and Health Services. All the parents of
the participants were informed about the study and signed an agreement form before being
included in the study.
Participants
There were  participants aged between  and  months, who were divided into two
groups, i.e.  ASD and  TD with  boys and  girls in each group. Since we expected to
PLOS ONE
Early screening of autism using cry features
PLOS ONE | 
December , 
 / 
have different vocalization characteristics for boys and girls, the training set was assembled of
only boys, including  TD, and  ASD. In other words, we wanted to eliminate the gender
effects on the feature extraction and model training. Unfortunately, due to the lower number
of girls with ASD in the real world, not enough data for girls with ASD could be collected.
Nonetheless, the model was also tested on the girls to see how it would generalize even on
them.
The inclusion criteria of the ASD participants were: a) being very recently diagnosed with
ASD based on DSM- with no other neurodevelopmental, mental, and intellectual disorder, b)
having no other known medical or genetic conditions, or environmental factors, and c) not
having received any treatment or medication, or having received treatment in less than a
month. There were only two girls who did not fall into these criteria since they had been diag-
nosed more than a year before. The participants’ average language development at the time of
participation, which was assessed based on , was equal to children between  to 
months old. The autism diagnosis procedure started with the Gilliam Autism Rating Scale-Sec-
ond Edition  questionnaire  which was answered by the parents. Then the
parents were interviewed, based on DSM-, while the participants were evaluated and
observed by two child clinical psychologists with Ph.D. degrees. In addition, the diagnosis of
ASD was separately confirmed by at least one child psychiatrist in a different setting. It should
be noted that ADOS, which is a very common diagnostic tool is not administered widely in
Iran since there is no official translation of ADOS in Farsi. TD children were selected from
those in an age range similar to the ASD participants from volunteer families from their
homes and health centers. They had no evidence or official diagnosis of any neurological or
psychological disorder at the time of recording their voices. The children with ASD were older
than  months with the mean, standard deviation, and range of ., ., and  months
respectively. The TD children were younger than  months with the mean, standard devia-
tion, and range of about ., ., and  months respectively. It should be mentioned that
the diagnosis of the children under  years was mainly based on experts’ evaluation, not the
GARS score. Furthermore, all TD participants under  years of age had a follow up study when
they passed the age of , to make sure the initial TD assignment was correct or still valid. To do
so, we used a set of expert-selected questions based on  to assess them through interviews
with parents.
Tables  and  show the details of the participants on the training and test sets, respectively.
In each table, the number of voice instances from each participant and the total duration of all
its instances in seconds are shown in columns  and , respectively. The recording device cate-
gory, i.e. a high-quality recorder (HQR) and typical cell phones (CP), is given in the device cat-
egory column. The next two columns include GARS- scores and the language developmental
milestone of the participants with ASD at the time of the recording. In six cases, there were no
GARS score available at the time of study, demonstrated by ND (No Data). The column
labeled as ‘Place’ shows the location of the recording which can be in homes (H), autism cen-
ters , and health centers . There was a total number of 
samples for all children. .% of the samples were from ASD participants and .% were
from TD participants.
Two groups of  TD and  ASD children were selected for training the classifiers such
that two groups were as balanced as possible with respect to age and the recording device.
Thus, each child in the TD group had a corresponding child in the ASD group around the
same age. As a result of this data balancing, we obtained training participants with an age
between  and  months. The mean ages in the training set were . and . months for
ASD and TD participants, respectively. The standard deviations are  and . months with the
range of  and  months for ASD and TD participants, respectively.
PLOS ONE
Early screening of autism using cry features
PLOS ONE | 
December , 
 / 
Although this approach was trained and tested on children older than  months, we tested the
proposed approach on  participants between  to  months to investigate how it works on chil-
dren under  months. These  participants consisted of  boys and  girls with the mean age
of . for both and standard deviations of . and . respectively. All these participants were eval-
uated at a later date at the age of  or older, by the same follow-up procedure, using our expert-
selected questionnaire. At the time of initial voice collection,  of these participants had no evident
or diagnosed disorder. Two of them were referred to our experts due to the positive results of
screening using our method. The diagnosis or concerns about the two mentioned participants, as
well as the participants with any evidence of having abnormality in the developmental milestones
during the follow-up procedure are summarized in Table . The summary of disorders given in the
last column of Table  is based on the parental interviews and our experts’ evaluation. Unfortu-
nately, Child, Child, and Child’s parents did not cooperate in obtaining expert evaluation.
Data collection and preprocessing
As mentioned earlier, the data was recorded using high-quality devices and typical smart-
phones. The high-quality devices were a UX Sony voice recorder and a Sony UXF voice
recorder. To use typical smartphones, a voice-recording and archiving application was devel-
oped and used on various types of smartphones. All voices, through the application or the
high-quality recorders, were recorded in wav format,  bits, and with the sampling rate of
. kHz. The reason for using various devices was to avoid biasing of the approach to a spe-
cific device. Similarly, the place of recording was not restricted to one place in order to make
the results applicable to all places.
The parents and trained voice collectors were asked to record the voices in a quiet environ-
ment. Furthermore, they were asked to keep the recorders or smartphones about  cm from
the participants’ mouth. Despite the proposed two recommendations, there were recorded
Table . The training set data of participants.

PLOS ONE
Early screening of autism using cry features
PLOS ONE | 
December , 
 / 
voices where the recommendations were not followed and did not have the required quality.
Consequently, those recordings were eliminated from the study. Also, all the cry sounds which
were due to pain, had been removed from the study since they were similar between the TD
and ASD groups.
Table . The test set information.

PLOS ONE
Early screening of autism using cry features
PLOS ONE | 
December , 
 / 
After data collection, there was a preprocessing phase in which only pure crying parts of the
recordings, with no other types of vocalization, were selected. To explain more, the parts of cry
sounds which were accompanied by screaming, saying words/other vocalizations, or that
occurred with closed/non-empty mouth were eliminated. All segmentations and eliminations
were done manually using Sound Forge Pro .. From the selected cries, the beginning and
the end, which contained voice rises and fades, were removed in order to just keep the steady
parts of the cries; this prevents having too much variation in the voice which can lead to
unsuitable statistics. Also, the uvular/guttural parts of the cries were removed. The reason for
this was that we believe these parts distort the feature values of the steady parts of a voice. Each
remaining continuous segment of the cries was considered and used as a sample (instance) in
this study. Finally, since the basic voice features were extracted from  milliseconds frames
, to generate statistical features of the basic features, the minimum length of the cry seg-
ments were set to  frames, i.e.  milliseconds. Thus, any cry samples below  millisec-
onds were eliminated from the study. In this study, the final prepared samples were between
 milliseconds to  seconds.
Feature extraction
Previous studies working on voice features for discriminating ASD children used different sets of
features. These methods share several common features like F, i.e. the fundamental frequency
of a voice, and Mel-Frequency Cepstral Coefficients (MFCC), i.e. coefficients which represent
the short-term power spectrum of a sound . F has been one of the most common features
used . However, since age is an important factor affecting F , this feature is useful
when participants have a similar age. On the other hand, MFCC coefficients and several related
statistical values have been reported to be useful features in several studies . Consider-
ing the useful features reported in previous studies and the specifications of the current study,
several features were selected to be used in this work that are explained in the following.
In this study, each instance was divided into  milliseconds frames, to extract basic voice
features. We used several features proposed by Motlagh, Moradi, and Pouretemad  and by
Belalca
´zar-Bolaños et al. . The features used by Motlagh, Moradi, and Pouretemad 
include certain statistics like mean and covariance of the frame-wise basic features, such as
MFCC coefficients, over a voice segment. They also used the mean and variance of frame-wise
temporal derivative  of the basic features. The frame-wise temporal derivative means
Table . The participants with an abnormality in the follow-up.
UNDD, Unspecified Neurodevelopmental Disorder.
a Clinical observation by our expert based on .
bClinical observation by our expert based on .

PLOS ONE
Early screening of autism using cry features
PLOS ONE | 
December , 
 / 
the difference between two consecutive frames, which in a sense is the rate of change of a fea-
ture value in one frame step. We modified the spectral flatness features by including the range
of – Hz beside the – Hz range. This range was added to cover a wider frequency
range than the normal children frequency range, which showed to be necessary in the process
of feature extraction and selection. Each range is divided into  octaves and the spectral flatness
is computed for those octaves.
We removed all uninformative and noisy features of the set which are explained in the fol-
lowing. The mean of frame-wise temporal derivative of the basic features is removed because it
is not a meaningful feature and is equal to taking the difference between the value of the last
and the first frames. There are means of the features related to the energy, such as the audio
power, total loudness, SONE, and the first coefficient of MFCC, that were removed to make
the classifier independent of the loudness/power in children’s voices. Zero crossing rate (ZCR)
was omitted too, due to its dependency on the noise in the environment.
The second set of features used in this study was from Belalca
´zar-Bolaños et al.  because
it has phonation features, like jitter and shimmer. Jitter and shimmer, which have been reported
to be discriminative for ASD, are linked to perceptions of breathiness, hoarseness, and rough-
ness . Other features used from Belalca
´zar-Bolaños et al.  include glottal features related
to vocal quality and the closing velocity of the vocal folds . The mean of logarithmic energy
feature was omitted for the same reason as other energy-related features. A summary of the fea-
tures, added to or removed from the sets by , is presented in Table .
The proposed subset instance classifier
To explain the proposed classifier, it was assumed that there is a target group of participants
that we want to distinguish from the rest of the participants, called the rest. Furthermore, each
participant in the target group may have several instances that may be used to distinguish the
target group from the rest. Fig A shows a situation in which all instances of all participants of
the target group are differentiable using common classifiers that we call Whole Set Instance
(WSI) classifiers. In this figure, the circles represent our target group and the triangles represent
the rest. The color coding is used to differentiate between the instances of each participant in
each group. In contrast to the situation in Fig A, in Fig B the target group cannot easily be dis-
tinguished from the rest. In such a situation, there are instances of two participants in the target
group, i.e. the red and brown circles that are not easily separable from the instances in the rest
. Furthermore, there is a participant with no instances, i.e. the orange circles, easily
Table . The features and statistics which were added or removed to the two feature sets.

PLOS ONE
Early screening of autism using cry features
PLOS ONE | 
December , 
 / 
separable from the rest . An example of Case  is tiptoe walking in children with ASD,
which is common in about % of these children  who do it most of the time. An example
of Case  is children with ASD who do not tiptoe walk. In other words, there are children with
ASD who cannot be distinguished from TD children using the tiptoe walking behavior factor.
Applying any WSI classifier may fail for the data type shown in Fig B. Consequently, we
proposed SubSet Instance (SSI) classifier that first finds differentiable instances and then trains
a classifier on these instances. As an example, the proposed SSI classifier first tries to find the
circles on the left of the line in Fig B, using a clustering method. Then, it uses these circles, as
exclusive instances having a specific feature common in a subset of the target group, to train a
classifier separating a subset of the target group.
The steps of common WSI classifiers are shown in Fig A. The steps of our proposed SSI
classifier are shown in Fig B. In the SSI classification approach, after the feature extraction
and clustering steps, for each cluster, a classifier is trained to separate its exclusive instances
from the instances of the rest of the participants. In the testing phase, any participant with only
one instance classified in the target group (positive instance), is classified as a target group’s
participant. The pseudo-code for the proposed approach is given in Algorithms  and .
Algorithm . Training SSI classifiers
T: set of all target group instances
R: set of all the rest instances
F: set of all classifiers
ρ: threshold for the number of samples in a cluster
s: the number of minimum samples needed in a cluster to be able to
train a classifier for it
Cj: The jth cluster
n: number of clusters
F = ;
: While j |Cj| > ρ; while there is a cluster bigger than a threshold
or n = 
:
n = n + ; increase the number of clusters
:
Cluster the T + R into n clusters Cj,j = ,. . .,n
:
EC = {Cj  T}; the set of clusters of only exclusive instances,
i.e. exclusive clusters
Fig . Two different hypothetical types of two-dimensional data of the target group and the rest. The instances shown by the warm-colored circles and
the cool-colored triangles are for the target group and the rest, respectively. All instances belonging to a participant have the same color. In (a), all the target
group participants’ instances are distinguishable using a classifier. In (b), only some instances of the target group participants are separable from the other
instances by a classifier.

PLOS ONE
Early screening of autism using cry features
PLOS ONE | 
December , 
 / 
:
If EC ¼ ;; check if there is any exclusive cluster
:
For all Cj in EC with |Cj| > s
:
Train a classifier using positive labels c  Cj and negative
labels r  R
:
Add the classifier to F
:
T ¼ T   P
CjEC
Cj; remove the instances of the exclusive clusters from
target group instances
:
n = ; set  to re-start clustering in two groups on the remain-
ing instances
Algorithm . Testing SSI classifiers
F: set of trained classifiers
A: set of subject instances
:
For all instances a of A
:
P = {a  A|f, classifies a as positive instance}
:
If P ¼ ;
:
The participant is from the target group
:
Else
:
The participant is from the rest
In the proposed training algorithm of the SSI approach, the goal is to find clusters contain-
ing the ASD instances only. Then a classifier is trained using the instances of these clusters and
Fig . An overall view of WSI and SSI methods. (a) In WSI method, after feature extraction, a classifier is trained on all instances and
majority pooling (MP) is usually used in the testing phase. In this study Best-chance threshold Pooling (BP), which is a threshold-based
pooling with the threshold giving the best accuracy on the test set, is also used to give the best chance to WSI classifier. (b) In the
proposed SSI classifier, after feature extraction, clustering is applied to find and select exclusive instances containing instances of the
target group participants only. Then classifiers are trained using exclusive instances, and a participant is classified in the target group in
the testing phase if any classifier detects a positive instance for it.

PLOS ONE
Early screening of autism using cry features
PLOS ONE | 
December , 
 / 
added to a list of all trained classifiers . As shown in the loop of
the algorithm, starting at line , the data is clustered starting with two clusters. Then the num-
ber of clusters is increased until a cluster, containing only the target group instances, emerges.
The exclusive instances in such a cluster are removed from the set of all target group’s
instances, and the loop is restarted. Before restarting the loop, if the number of instances in
this cluster is more than a threshold, a new classifier using these instances is trained and this
classifier is added to the set of all trained classifiers. The loop stops when the number of sam-
ples in each cluster is less than a threshold.
For testing the participants, using the trained classifiers, all the instances of each participant
are classified one by one using all the trained classifiers . A subject would
be classified in the target group if at least one of its instances is classified in the target group at
least by one of the classifiers . Otherwise, if there is no instance
classified among the target group, the participant is classified as the rest .
Details of the implementations
The classifiers were implemented in Python using scikit-learn library.
WSI classifiers.
We have tested several common WSI classifiers, but we report only the
result of SVM with RBF kernel and with no feature selection, which gives the best average
accuracy. It should be noted that several feature selection approaches, like L-SVM and back-
ward elimination, were tested but they only reduced the accuracy. We used group -fold cross-
validation for tuning hyper-parameters. Group K-fold means that all instances of each partici-
pant are placed in only one of the folds. This prevents having the same participant’s instances
in the train and validation folds simultaneously. In each fold, there were two ASD and two TD
participants. It should be mentioned that before applying the algorithms, we balanced the
number of instances of the two groups using upsampling.
Two approaches were exploited to combine the decisions on different samples of a partici-
pant in the WSI approach. The first approach was majority pooling which classifies a partici-
pant as ASD, if the number of instances classified as ASD are more than  percent of all
instances. The second approach was threshold-based pooling which is similar to the first
approach except that a threshold other than  is used.
SSI classifiers.
Before applying the algorithm, we balanced the number of instances of the
two groups by upsampling. The threshold for the minimum number of samples, needed in a
cluster, to be able to train a classifier is set to . It should be mentioned that agglomerative
clustering and decision tree are the methods used for clustering and classification parts of
Algorithm , respectively.
Training the SSI classifiers.
After running Algorithm  on our data, two exclusive clus-
ters with enough instances, i.e. at least  instances in our study, were found. Then two classifi-
ers were trained corresponding to each cluster. One of these exclusive clusters had 
instances from  ASD participants . These  instances consisted of  out of 
instances of ASD,  out of  instances of ASD,  out of  instances of ASD, and  out of 
instances of ASD. As explained in the algorithm, for each cluster, a decision tree classifier was
trained using the ASD instances in the cluster versus all TD instances. Interestingly, only one
feature was enough to discriminate instances in the cluster from all TD instances. Among
those features that can discriminate the cluster’s instances, we selected the Variance of Frame-
wise Temporal Derivative (VFTD) of the th MFCC coefficient as the feature which can dis-
criminate more ASD participants from the set of all participants with a simple threshold. The
classifier obtained by setting a threshold based on this feature was the first classifier. This fea-
ture supports our expert’s report regarding the higher variations in the cry sounds of ASD
PLOS ONE
Early screening of autism using cry features
PLOS ONE | 
December , 
 / 
children than TD children. From  ASD children,  of them can be discriminated using this
feature. For each participant, the number of instances found by this classifier is shown in the
nd column of Table .
After excluding the ASD samples from the first classifier, the second classifier was trained
based on the second exclusive cluster. This cluster included all instances of participant ASD.
The only feature used for classifying this cluster was VFTD of the th SONE coefficient. SONE
is a unit of loudness which is a subjective perception of sound pressure . Having higher
VFTD of the th SONE coefficient confirms the experiential knowledge of our experts men-
tioned before. Among all the ASD participants, eight had instances with VFTD of the th
SONE higher than a threshold . The results of classifica-
tion based on these two features are depicted in Fig . As mentioned in the proposed method
section, the participants with at least one instance classified into this cluster would be consid-
ered as a participant with ASD.
Results
In this part, the performance of our proposed SSI classifier against a common WSI classifier is
evaluated on our test set of ASD and TD participants. Each participant has multiple instances
which are cleaned using the criteria explained in the data collection and preprocessing section.
The participants who had at least one accepted instance were used in the training and testing
phases, which are shown in Tables  and .
The output of the SSI approach was two classifiers, each of them works by setting a thresh-
old based on a feature. The number of instances of ASD participants in the training set, cor-
rectly detected by the first and the second classifiers, are shown in the second and third
columns of Table , respectively. On the other hand, the best-resulting classifier for the WSI
approach was Radial Basis Function-Support Vector Machine (RBF-SVM) .
The classification results on the test set for different classifiers are shown in Table . The
portion of each participant’s instances, correctly classified by each classifier, is written as a
percentage under the name of the classifier. The decision made by the WSI and SSI classifi-
ers for each participant is shown by ASD or TD. To classify each subject using the WSI clas-
sifier, the Majority Pooling (MP) and the Best-chance threshold Pooling (BP) approaches
were used. BP is a threshold-based pooling with the threshold giving the best accuracy on
the test set for male participants. For the boys, MP has specificity, sensitivity, and precision
equal to %, .%, and .%, respectively. On the other hand, BP leads to specificity,
sensitivity, and precision equal to .%, .%, and .%, respectively. The threshold
Table . The number of instances of each participant in the training set that are classified as ASD using each
trained SSI classifier.

PLOS ONE
Early screening of autism using cry features
PLOS ONE | 
December , 
 / 
for BP was set to % that means if % of instances of a participant were classified as ASD
instance, the participant was classified as having ASD. The results of the percentage of
instances correctly classified by the two classifiers in the SSI approach are shown as C (the
first SSI classifier) and C (the second SSI classifier) in Table . The aggregated result of the
decisions by C and C makes the final decision of the SSI classifier which is shown in the
decision column, under the SSI classification section. The achieved specificity, sensitivity,
and precision using the proposed method for the boys are %, .%, and .%,
respectively.
To further show the applicability of the proposed approach to girls, we applied the boys’
trained classifiers on the test set of the girls. The results are shown in the last row of Table 
which show that the MP approach has specificity, sensitivity, and precision equal to %,
.%, and .%, respectively. Furthermore, the BP approach gives specificity, sensitivity,
and precision all equal to .%, respectively. The results of the proposed SSI classifier is
% specificity, .% sensitivity, and .% precision.
A two-dimensional scatter plot of the two features, used in C and C classifiers, are shown
in Fig . As can be seen in this figure, the instances of a participant with ASD are scattered in
the area containing instances of both TD and ASD participants. Nevertheless, there are
instances for this participant uniquely distinguishable using the selected two features.
We compared the results of our proposed method with that of the only method available
in the literature which was trained using only cry features  based on our data. The
results  show the superiority of our method, compared to the previously proposed
method.
Investigating the trained classifier on participants under  months
The SSI classifier which was trained using the training set in Table  was also tested on the
data of children younger than  months. From  participants under  months, two boys
 were classified as ASD by the mentioned trained classifier.
These participants were referred to our experts for diagnosis. These two were suspected of
having neurodevelopmental problems. All other boys were classified as TD. However,
among them, Child was diagnosed with ASD at the age of . Also, Child showed
Fig . Two classifiers trained on the two exclusive clusters found during the SSI classifier training phase. (a) The Variance of Frame-wise Temporal Derivative
(VFTD) of the th MFCC coefficient separates  instances of  ASD subjects from all TD instances of the training set. (b) VFTD of the th SONE coefficient
separates  instances of  ASD participants from all TD instances of the training set.

PLOS ONE
Early screening of autism using cry features
PLOS ONE | 
December , 
 / 
symptoms of having ADHD and sensory processing disorder at the age of . Three other
children had symptoms which suggested that they are not TD children. Two of the girls
who were  months old were classified as ASD, using the trained classifier. The other girls
were classified as TD. The results of testing the trained SSI classifier on this data set are
summarized in Table .
The original and cleaned voices and their extracted features (the data set) in this research
and the implementation codes of the proposed method are deposited in the following
repositories:
CodeOcean
./CO..v
Harvard Dataverse (Contains only a rar file of sounds):
./DVN/LSTBQW
Table . The results of classifiers on the instances of each participant in the test set.
Each classifier result on a participant’s instances is reported as a percentage.
Dec., Decision; MP, Majority Pooling; BC, Best-chance threshold Pooling; C, Classifier; C, Classifier; Acc., Accuracy.

PLOS ONE
Early screening of autism using cry features
PLOS ONE | 
December , 
 / 
Discussion and conclusion
In this paper, we presented a novel cry-based screening method to distinguish between chil-
dren with autism and typically developing children. In the proposed method, groups of chil-
dren with autism who have specific features in their cry sounds can be determined. This
method is based on a new classification approach called SubSet Instance (SSI) classifier. An
appealing property of the proposed SSI classifier, in the case of voice-based autism screening,
is its high specificity such that a normal child can be detected with no error. We applied the
proposed method on a group of participants consisting of  boys with ASD between  and
Fig . Instances of several ASD and TD participants scattered in the space of two features given by the proposed SSI method. The
instances of a chosen ASD participant are illustrated in green to show that a participant may have instances in the area common with TD
instances besides those two areas separated by the selected thresholds as ASD. The mentioned ASD participant (with green instances) is
tagged as ASD, due to having at least one instance with the greater value than at least one of the thresholds on the two features.

Table . Comparison of the results on the test set using the two methods; SSI approach and a baseline approach.

PLOS ONE
Early screening of autism using cry features
PLOS ONE | 
December , 
 / 
 months of age and  TD boys between  and  months of age. The two features, found
in this study, were used to train a classifier on  boys with ASD and  TD boys. Then, the
classifier was used to distinguish  boys with ASD from  TD boys, reaching .% accuracy.
Due to the fact that girls are less likely to have autism and consequently, it is harder to collect
enough data from girls than boys, the number of girls with ASD was not sufficient to train a
separate classifier for this gender. It should be noted that we tested the trained system on 
girls with ASD and  TD girls. It was seen that the trained classifier can screen girls with %
lower accuracy than boys of the test set. In other words, it seems that gender differences should
be considered in the training of the system. In testing the data from participants under 
months, one TD girl was classified as ASD which was not the case for any TD children of the
male counterparts. This result also confirms the aforementioned point about the gender effect.
However, in future work, we would try to collect more data on girls to be able to train a system
to accurately screen girls. Furthermore, we would also try to train a single classifier for boys
and girls to determine whether it can be used for both of them.
It should be mentioned that our training and test data were completely separate, to make
the trained model more general. The features found in this study are applicable in the age
range of our participants from  to  months. This is in contrast to other approaches that
either used a dataset of children with a specific age  or used age information for classifi-
cation . Due to the age invariant features found in this study, it can be claimed that there
are markers in the voices of children with ASD that are sustained at least in a range of ages.
The two discriminative features, found in this study, were a coefficient of MFCC and a SONE
coefficient. MFCC and SONE are related to the power spectrum of a speech signal. SONE mea-
sures loudness in specific Bark bands . On the other hand, MFCC, which is the inverse DFT
of log-spectrum in the Mel scale, is related to the timbre of the voice . Therefore, MFCC and
SONE can be interpreted to be related to the timbre and loudness of a tone. Furthermore, based
on the feedback from our experts, there is unpredictability in the crying sound of children with
autism which is not the case for TD children. Consequently, we used the variance of temporal
difference as a feature suitable for screening children with autism. This is due to the fact that if a
signal is constant or changes linearly over time, the variance of temporal difference is zero.
Therefore, the variance of temporal difference can be seen as the amount of ambiguity or unpre-
dictability of a sound. On the other hand, the heightened variability in the two features, found in
this study, for children with ASD is significant due to the reports from other studies 
which shows increased biological signals variability in children with ASD and infants at high
risk for autism in comparison with TD children. These features are statistical features of the cry
instances that hold constant, at least, across an age range studied in this research.
To the best of our knowledge,  were the only studies on screening children
with autism using voice features on children younger than  years of age. Our proposed
method has higher precision than these two, i.e. % more than ,
using only cry features. The use of cry features as suitable biomarkers for autism screening
matches the claims in .
Table . Classification of the participants under  months using our trained SSI classifier.
a Other developmental or mental disorders

PLOS ONE
Early screening of autism using cry features
PLOS ONE | 
December , 
 / 
In the present study only children with ASD and TD children were tested. Other develop-
mental disorders or health issues were not tested to see how children with such disorders
would be classified using the proposed method which can decrease the specificity of %.
However, this approach is proposed to be used as a screening tool and the final diagnosis
should be done under experts’ supervision. So, this approach can be applied as a general
screener of autism spectrum disorder.
The trained classifier was also tested on  participants between  to  months of age. The
classifier screened two boys from the rest, i.e. Child and Child . Child showed evi-
dences of genetic disease and was diagnosed with developmental delay and Child received
UNDD classification by our experts. This suggests that a) the system can be used for children
under  years of age, and b) it may be able to distinguish other neurodevelopmental disorders.
On the other hand, there were  boys, i.e. Child to Child , who had no evidence of
mental or developmental disorders at the time of their recording. At the same time, our approach
did not distinguish them as children with ASD either. However, when they were older than 
years, they showed symptoms of neurodevelopmental disorders. Out of these children, we could
manage to collect new recordings from Child and Child that were classified as children with
ASD using our approach. Unfortunately, Child, Child, and Child did not cooperate and
could not be evaluated by an expert to validate the results of our expert-selected questionnaire.
Furthermore, the parents refused to cooperate send us their children’s recent cry sounds.
The result of studying these  children under the age of  months may suggest that: a)
there could be symptoms in the crying sounds of children with neurodevelopmental disorders
under  months , b) the approach may not be able to screen a participant
with neurodevelopmental disorders under the age of  months due to the possibility that: )
the participant was among those children with neurodevelopmental disorders who do not
have our proposed specific features in their crying sounds, ) the participant’s recorded cry
samples did not include our specific features, and/or ) neurodevelopmental disorders and
their features had not been developed in the child at the time of initial recording. The reason
behind not classifying Child and Child, as children with ASD under the age of , could be
b. or b.. To clearly determine any reason behind this phenomena, a further investigation is
needed.
We believe that this approach can be used to perform early autism screening under 
months of age. Thus, in the future, we need to collect data and test the approach on more data
of children under  months to validate these results with more confidence.
We have to further check the proposed approach and the extracted features on other neuro-
developmental disorders, such as ADHD, to evaluate the capability of the approach to distin-
guish the children with these disorders from TD children.
Furthermore, without comparing the cry sounds of children with ASD to those without
ASD but another disorder, we do not really know if these findings are specific to autism or to
general atypical brain developments. Thus, we should collect cry sounds of children with other
neurodevelopmental disorders and compare voices of children with ASD to voices of children
with other neurodevelopmental disorders to see if these features would be able to separate
them or not.
It has been demonstrated that crying consists of intricate motor activities . On the other
hand, it has been shown that children with ASD have problems in the motor domain and in
coordination of their motor capabilities with other modalities . Consequently, it is possible
that the extracted features in the crying sounds of children with ASD come from this defi-
ciency/problem in the motor domain which requires further investigations.
Finally, automating the preprocessing part is a technical issue that should be addressed if it
is deemed necessary that the cry-based screening be fully automated. This is important since
PLOS ONE
Early screening of autism using cry features
PLOS ONE | 
December , 
 / 
such a screening system can be deployed in systems such as Amazon Alexa  to automati-
cally screen problematic cry sounds.
Acknowledgments
We would like to thank the Center for Treatment of Autism Disorder (CTAD) and its mem-
bers for supporting this study. We would also like to thank all the families who helped this
research by taking the time to collect the cry sounds of their children. The authors would also
like to express their gratitude to Prof. H. Sameti from Sharif University of Technology for his
valuable and constructive feedbacks on the data collection and voice processing.
Author Contributions
Conceptualization: Aida Khozaei, Hadi Moradi, Reshad Hosseini, Hamidreza Pouretemad,
Bahareh Eskandari.
Data curation: Aida Khozaei.
Formal analysis: Aida Khozaei.
Funding acquisition: Hadi Moradi.
Investigation: Hadi Moradi.
Methodology: Aida Khozaei.
Project administration: Hadi Moradi.
Software: Aida Khozaei.
Supervision: Hadi Moradi.
Validation: Aida Khozaei.
Visualization: Aida Khozaei.
Writing – original draft: Aida Khozaei, Hadi Moradi, Reshad Hosseini.
Writing – review & editing: Aida Khozaei, Hadi Moradi, Reshad Hosseini, Hamidreza Poure-
temad, Bahareh Eskandari.
References
.
American Psychiatric Association. Diagnostic and statistical manual of mental disorders (DSM-®).
American Psychiatric Pub; .
.
Chen JL, Sung C, Pi S. Vocational rehabilitation service patterns and outcomes for individuals with
autism of different ages. J Autism Dev Disord. ; :–. 
--y PMID: 
.
Fakhoury M. Autistic spectrum disorders: A review of clinical features, theories and diagnosis. Int J Dev
Neurosci. ; :–.  PMID: 
.
Constantino JN, Charman T. Diagnosis of autism spectrum disorder: reconciling the syndrome, its
diverse origins, and variation in expression. Lancet Neurol. ; :–. 
/S-- PMID: 
.
Calderoni S, Billeci L, Narzisi A, Brambilla P, Retico A, Muratori F. Rehabilitative interventions and brain
plasticity in autism spectrum disorders: focus on MRI-based studies. Front Neurosci. ; :.
 PMID: 
.
Brentani H, Paula CSd, Bordini D, Rolim D, Sato F, Portolese J, et al. Autism spectrum disorders: an
overview on diagnosis and treatment. Braz J Psychiatry. ; :S–S. 
---S PMID: 
PLOS ONE
Early screening of autism using cry features
PLOS ONE | 
December , 
 / 
.
Mandell DS, Novak MM, Zubritsky CD. Factors associated with age of diagnosis among children with
autism spectrum disorders. Pediatrics. ; :–. 
PMID: 
.
Thabtah F, Peebles D. A new machine learning model based on induction of rules for autism detection.
Health Inform J. .  PMID: 
.
Volkmar F, Cook EH, Pomeroy J, Realmuto G, Tanguay P. Practice parameters for the assessment
and treatment of children, adolescents, and adults with autism and other pervasive developmental dis-
orders. J Am Acad Child Adolesc Psychiatry. ; (, Supplement):S–S. 
/S--
.
Campbell M, Schopler E, Cueva JE, Hallin A. Treatment of autistic disorder. J Am Acad Child Adolesc
Psychiatry. ; :–.  PMID: 
.
Zachor DA, Itzchak EB. Treatment approach, autism severity and intervention outcomes in young chil-
dren. Res Autism Spectr Disord. ; :–.
.
Boyd BA, Hume K, McBee MT, Alessandri M, Gutierrez A, Johnson L, et al. Comparative Efficacy of
LEAP, TEACCH and Non-Model-Specific Special Education Programs for Preschoolers with Autism
Spectrum Disorders. J Autism Dev Disord. ; :–. 
- PMID: 
.
Jacobson JW, Mulick JA, Green G. Cost–benefit estimates for early intensive behavioral intervention for
young children with autism—general model and single state case. Behav Interv. ; :–.
.
Jacobson JW, Mulick JA. System and Cost Research Issues in Treatments for People with Autistic Dis-
orders. J Autism Dev Disord. ; :–.  PMID:

.
Thabtah F, Peebles D. Early Autism Screening: A Comprehensive Review. Int J Environ Res Public
Health. ; :.  PMID: 
.
Rutter M, Le Couteur A, Lord C. ADI-R: Autism Diagnostic Interview-Revised. Los Angeles, CA: West-
ern Psychological Services; .
.
Lord C., Risi S., Lambrecht L., Cook E. H. Jr., Leventhal B. L., Lavore Di, et al. The autism diagnostic
observation schedule-generic: a standard measure of social and communication deficits associated
with the spectrum of autism. J Autism Dev Disord. ; , –. PMID: 
.
Levy S, Duda M, Haber N, Wall DP. Sparsifying machine learning models identify stable subsets of pre-
dictive features for behavioral detection of autism. Mol Autism. ; :. 
s--- PMID: 
.
Ku
¨pper C, Stroth S, Wolff N, Hauck F, Kliewer N, Schad-Hansjosten T, et al. Identifying predictive fea-
tures of autism spectrum disorders in a clinical sample of adolescents and adults using machine learn-
ing. Sci Rep. ; :.  PMID: 
.
Abbas H, Garberson F, Liu-Mayo S, Glover E, Wall DP. Multi-modular AI Approach to Streamline
Autism Diagnosis in Young Children. Scientific Reports. ; :. 
s---w PMID: 
.
Emerson RW, Adams C, Nishino T, Hazlett HC, Wolff JJ, Zwaigenbaum L, et al. Functional neuroimag-
ing of high-risk -month-old infants predicts a diagnosis of autism at  months of age. Sci Transl Med.
; :eaag.  PMID: 
.
Denisova K, Zhao G. Inflexible neurobiological signatures precede atypical development in infants at
high risk for autism. Sci Rep. ; :.  PMID:

.
Bosl WJ, Tager-Flusberg H, Nelson CA. EEG analytics for early detection of autism spectrum disorder:
a data-driven approach. Sci Rep. ; :.  PMID:

.
Momeni N, Bergquist J, Brudin L, Behnia F, Sivberg B, Joghataei M, et al. A novel blood-based bio-
marker for detection of autism spectrum disorders. Transl Psychiatry. ; :e. 
/tp.. PMID: 
.
Glatt SJ, Tsuang MT, Winn M, Chandler SD, Collins M, Lopez L, et al. Blood-based gene expression
signatures of infants and toddlers with autism. J Am Acad Child Adolesc Psychiatry. ; :-
.
Croen LA, Braunschweig D, Haapanen L, Yoshida CK, Fireman B, Grether JK, et al. Maternal mid-preg-
nancy autoantibodies to fetal brain protein: the early markers for autism study. Biol Psychiatry. ; 
:–.  PMID: 
.
Greene DJ, Black KJ, Schlaggar BL. Considerations for MRI study design and implementation in pediat-
ric and clinical populations. Dev Cogn Neurosci. ; :–. 
. PMID: 
PLOS ONE
Early screening of autism using cry features
PLOS ONE | 
December , 
 / 
.
Webb SJ, Bernier R, Henderson HA, Johnson MH, Jones EJ, Lerner MD, et al. Guidelines and best
practices for electrophysiological data collection, analysis and reporting in autism. J Autism Dev Disord.
; :–.  PMID: 
.
Engelhardt LE, Roe MA, Juranek J, DeMaster D, Harden KP, Tucker-Drob EM, et al. Children’s head
motion during fMRI tasks is heritable and stable over time. Dev Cogn Neurosci. ; :–. https://
doi.org/./j.dcn... PMID: 
.
Denisova K. Age attenuates noise and increases symmetry of head movements during sleep resting-
state fMRI in healthy neonates, infants, and toddlers. Infant Behav Dev. ; :. 
org/./j.infbeh... PMID: 
.
Brisson J, Martel K, Serres J, Sirois S, Adrien JL. Acoustic analysis of oral productions of infants later
diagnosed with autism and their mother. Infant Ment Health J. ; :–. 
/imhj. PMID: 
.
Nakai Y, Takiguchi T, Matsui G, Yamaoka N, Takada S. Detecting abnormal word utterances in
children with autism spectrum disorders: machine-learning-based voice analysis versus speech
therapists. Percept Mot Skills. ; :–. 
PMID: 
.
Santos JF, Brosh N, Falk TH, Zwaigenbaum L, Bryson SE, Roberts W, et al. Very early detection of
autism spectrum disorders based on acoustic analysis of pre-verbal vocalizations of -month old tod-
dlers. In: Proceedings of the International Conference on Acoustics, Speech and Signal Processing;
; Vancouver, BC, Canada: IEEE. . Doi: ./ICASSP..
.
Oller D, Niyogi P, Gray S, Richards J, Gilkerson J, Xu D, et al. Automated vocal analysis of naturalistic
recordings from children with autism, language delay, and typical development. Proc Natl Acad Sci.
; :–.  PMID: 
.
Pokorny FB, Schuller BW, Marschik PB, Brueckner R, Nystro
¨m P, Cummins N, et al. Earlier Identifica-
tion of Children with Autism Spectrum Disorder: An Automatic Vocalisation-Based Approach. In: Pro-
ceedings of the INTERSPEECH ; ; Stockholm, Sweden: ISCA. . Doi: ./
Interspeech.-
.
Little MA, Varoquaux G, Saeb S, Lonini L, Jayaraman A, Mohr DC, et al. Using and understanding
cross-validation strategies. Perspectives on Saeb et al. Gigascience. ; :–. 
/gigascience/gix PMID: 
.
Eyben F, Scherer KR, Schuller BW, Sundberg J, Andre
´ E, Busso C, et al. The Geneva minimalistic
acoustic parameter set (GeMAPS) for voice research and affective computing. IEEE Trans Affect Com-
put. ; :–.
.
Esposito G, Hiroi N, Scattoni ML. Cry, Baby, Cry: Expression of Distress As a Biomarker and Modulator
in Autism Spectrum Disorder. Int J Neuropsychopharmacol. ; :–. 
/ijnp/pyx PMID: 
.
Sheinkopf SJ, Iverson JM, Rinaldi ML, Lester BM. Atypical Cry Acoustics in -Month-Old Infants at Risk
for Autism Spectrum Disorder. Autism Res. ; :–.  PMID:

.
Orlandi S, Manfredi C, Bocchi L, Scattoni ML, editors. Automatic newborn cry analysis: a non-invasive
tool to help autism early diagnosis. In: Proceedings of the Annual International Conference of the IEEE
Engineering in Medicine and Biology Society; ; San Diego, CA, USA: IEEE. . Doi: ./
EMBC..
.
Motlagh SHRE, Moradi H, Pouretemad H, editors. Using general sound descriptors for early autism
detection:  th Asian Control Conference (ASCC) Control; ; Istanbul, Turkey: IEEE. .
Doi: ./ASCC..
.
Barrow WJ, Jaworski M, Accardo PJ. Persistent toe walking in autism. J Child Neurol. ; :–
.
Paul R, Norbury CF. Language disorders from infancy through adolescence: Elsevier; .
.
Jalilevand N, Ebrahimipour M. Pronoun acquisition in Farsi-speaking children from  to  months. J
Child Lang Acquis Dev. ; :–.
.
Goldstein S, Ozonoff S. Assessment of autism spectrum disorder: Guilford Publications; .
.
Lund NJ, Duchan JF. Assessing children’s language in naturalistic contexts: Prentice Hall; .
.
Gilliam JE. Gilliam autism rating scale: GARS : Pro-ed; .
.
Berk L. Development through the lifespan: Pearson Education India; .
.
Three ZT. Diagnostic classification of mental health and developmental disorders of infancy and early
childhood: Revised edition (DC: -R). Washington, DC: Zero To Three Press; .
PLOS ONE
Early screening of autism using cry features
PLOS ONE | 
December , 
 / 
.
Paliwal KK, Lyons JG, Wo
´jcicki KK, editors. Preference for – ms window duration in speech analy-
sis. In: Proceedings of the th International Conference on Signal Processing and Communication Sys-
tems; : Gold Coast, QLD, Australia: IEEE. Doi: ./ICSPCS..
.
Molau S, Pitz M, Schluter R, Ney H. Computing Mel-frequency cepstral coefficients on the power spec-
trum. In: Proceedings of the International Conference on Acoustics, Speech, and Signal Processing
Proceedings ; ; Salt Lake City, UT, USA: IEEE. Doi: ./ICASSP..

.
Esposito G, Venuti P. Developmental changes in the fundamental frequency  of infants’ cries: a
study of children with Autism Spectrum Disorder. Early Child Dev Care. ; :–.
.
Marchi E, Schuller B, Baron-Cohen S, Golan O, Bo
¨lte S, Arora P, et al. Typicality and emotion in the
voice of children with autism spectrum condition: Evidence across three languages. In: Proceedings of
the INTERSPEECH ; ; Dresden, Germany: ISCA. . p. –. Available from: https://
.
.
Belalca
´zar-Bolaños E.A., Orozco-Arroyave J.R., Vargas-Bonilla J.F., Haderlein T., No
¨th E. Glottal Flow
Patterns Analyses for Parkinson’s Disease Detection: Acoustic and Nonlinear Approaches. In: Sojka
P., Hora
´k A., Kopeček I., Pala K., editors. Text, Speech, and Dialogue: Proceedings of the th Interna-
tional Conference on Text, Speech, and Dialogue;  Sep –; Brno, Czech Republic. Cham:
Springer; . Doi: ./----_
.
Rabiner LR, Schafer RW. Introduction to digital speech processing. Found and trends in signal process.
; :–.
.
Peeters G. A large set of audio features for sound description (similarity and classification) in the CUI-
DADO project. CUIDADO IST Proj Rep. ; :–.
.
Bone D, Lee C-C, Black MP, Williams ME, Lee S, Levitt P, et al. The psychologist as an interlocutor in
autism spectrum diorder assessment: Insights from a study of spontaneous prosody. J Speech Lang
Hear Res. ; :–.  PMID: 
.
Ha
¨nsler E, Schmidt G. Speech and audio processing in adverse environments: Springer Science &
Business Media; .  PMID: 
.
Theodoridis S, Koutroumbas K. Pattern recognition: Elsevier; .
.
Li TL, Chan AB. Genre classification and the invariance of MFCC features to key and tempo. In: Lee
KT., Tsai WH., Liao HY.M., Chen T., Hsieh JW., Tseng CC., editors. Advances in Multimedia Modeling:
Proceedings of the th International MultiMedia Modeling Conference;  Jan –; Taipei, Taiwan.
Berlin, Heidelberg: Springer; . Doi: ./----_
.
Takahashi T, Yoshimura Y, Hiraishi H, Hasegawa C, Munesue T, Higashida H, et al. Enhanced brain
signal variability in children with autism spectrum disorder during early childhood. Hum Brain Mapp.
; :–.  PMID: 
.
Lester BM, Boukydis CZ. Infant crying: Theoretical and research perspectives: Springer; .
.
MacDonald M, Lord C, Ulrich D. The relationship of motor skills and adaptive behavior skills in young
children with autism spectrum disorders. Res Autism Spectr Disord. ; :–. 
org/./j.rasd... PMID: 
.
Hoy MB. Alexa, Siri, Cortana, and More: An Introduction to Voice Assistants. Med Ref Serv Q. ; 
:–.  PMID: 
PLOS ONE
Early screening of autism using cry features
PLOS ONE | 
December , 
 / 
RESEARCH ARTICLE
Digital Behavioral Phenotyping Detects Atypical Pattern of Facial
Expression in Toddlers with Autism
Kimberly L. H. Carpenter
, Jordan Hahemi, Kathleen Campbell, Steven J. Lippmann, Jeffrey P. Baker,
Helen L. Egger, Steven Espinosa, Saritha Vermeer, Guillermo Sapiro, and Geraldine Dawson
Commonly used screening tools for autism spectrum disorder (ASD) generally rely on subjective caregiver questionnaires.
While behavioral observation is more objective, it is also expensive, time-consuming, and requires signiﬁcant expertise to
perform. As such, there remains a critical need to develop feasible, scalable, and reliable tools that can characterize ASD
risk behaviors. This study assessed the utility of a tablet-based behavioral assessment for eliciting and detecting one type
of risk behavior, namely, patterns of facial expression, in  toddlers  and evaluated whether such patterns
differentiated toddlers with and without ASD. The assessment consisted of the child sitting on his/her caregiver’s lap and
watching brief movies shown on a smart tablet while the embedded camera recorded the child’s facial expressions. Com-
puter vision analysis (CVA) automatically detected and tracked facial landmarks, which were used to estimate head posi-
tion and facial expressions (Positive, Neutral, All Other). Using CVA, speciﬁc points throughout the movies were
identiﬁed that reliably differentiate between children with and without ASD based on their patterns of facial movement
and expressions . During these instances, children
with ASD more frequently displayed Neutral expressions compared to children without ASD, who had more All Other
expressions. The frequency of All Other expressions was driven by non-ASD children more often displaying raised eye-
brows and an open mouth, characteristic of engagement/interest. Preliminary results suggest computational coding of
facial movements and expressions via a tablet-based assessment can detect differences in affective expression, one of the
early, core features of ASD. Autism Res , : –. ©  International Society for Autism Research and Wiley
Periodicals LLC
Lay Summary: This study tested the use of a tablet in the behavioral assessment of young children with autism. Children
watched a series of developmentally appropriate movies and their facial expressions were recorded using the camera
embedded in the tablet. Results suggest that computational assessments of facial expressions may be useful in early detec-
tion of symptoms of autism.
Keywords: autism; risk behaviors; facial expressions; computer vision; early detection
Introduction
Autism spectrum disorder (ASD) can be reliably diagnosed
as early as  months old and the risk signs can be
detected
as
early
as
– months
old
[Dawson
&
Bernier, ; Luyster et al., ]. Despite this, the aver-
age age of diagnosis in the United States remains around
 years of age . While there is
mixed evidence for the stability of autism traits over early
childhood
[Bieleninik
et
al.,
;
Waizbard-Bartov
et al., ], the delay in diagnosis can still impact timely
intervention during a critical window of development. In
response to this, in  the American Academy of Pedi-
atrics published guidelines supporting the need for all
children to be screened for ASD between - and
-months of age as part of their well-child visits [Myers,
Johnson, & Council on Children With Disabilities, ].
Current screening typically relies on caregiver report,
such as the Modiﬁed Checklist of ASD in Toddlers—
Revised
with
Follow-up
(M-CHAT-R/F)
(Robins
et al., ). Evidence suggests that a two-tiered screen-
ing approach, including direct observational assessment
of the child, improves the positive predictive value of
M-CHAT
screening
by
%
[Khowaja,
Robins,
&
From the Duke Center for Autism and Brain Development, Department of Psychiatry and Behavioral Sciences, Duke University School of Medicine, Dur-
ham, North Carolina, USA (K.L.H.C., J.H., K.C., H.L.E., S.E., S.V., G.D.); Department of Electrical and Computer Engineering, Duke University, Durham,
North Carolina, USA (J.H., S.E.); Department of Pediatrics, University of Utah, Salt Lake City, Utah, USA (K.C.); Department of Population Health Sci-
ences, Duke University School of Medicine, Durham, North Carolina, USA (S.J.L.); Department of Pediatrics, Duke University School of Medicine, Dur-
ham, North Carolina, USA (J.P.B.); NYU Langone Child Study Center, New York University, New York, New York, USA (H.L.E.); Departments of
Biomedical Engineering Computer Science, and Mathematics, Duke University, Durham, North Carolina, USA (G.S.); Duke Institute for Brain Sciences,
Duke University, Durham, North Carolina, USA (G.D.)
Received April , ; accepted for publication August , 
Address for correspondence and reprints: Kimberly L. H. Carpenter, Duke Center for Autism and Brain Development, Department of Psychiatry and
Behavioral Sciences, Duke University School of Medicine,  Erwin Rd #, Durham, NC . E-mail: 
Published online  Month  in Wiley Online Library (wileyonlinelibrary.com)
DOI: ./aur.
©  International Society for Autism Research and Wiley Periodicals LLC
INSAR
Autism Research : –, 

Adamson, ] and may reduce ethnic/racial disparities
in general screening . Current tools
for observational assessment of ASD signs in infants and
toddlers, such as the Autism Observation Scale for Infants
(AOSI) and Autism Diagnostic Observation Schedule
(ADOS), take substantial time and training to administer,
resulting in a shortage of qualiﬁed diagnosticians to per-
form these observational assessments. As such, there
remains a critical need to develop feasible, scalable, and
reliable tools that can characterize ASD risk behaviors
and identify those children who are most in need of
follow-up by an ASD specialist. In an effort to address this
critical need, we have embarked on a program of research
using computer vision analysis (CVA) to develop tools for
digitally phenotyping early emerging risk behaviors for
ASD . If successful, such digital
screening tools have the opportunity to help existing
practitioners reach more children and assist in triaging
boundary cases for review by specialists.
One
of
the
early
emerging
signs
of
ASD
is
a
tendency to more often display a neutral facial expres-
sion. This pattern is evident in the quality of facial
expressions
and
in
sharing
emotional
expressions
with
others
[Adrien
et
al.,
;
Baranek,
;
S.
Clifford
&
Dissanayake,
;
S.
Clifford,
Young,
&
Williamson,
;
S.
M.
Clifford
&
Dissanayake, ; Maestro et al., ; Osterling, Daw-
son, & Munson, ; Werner, Dawson, Osterling, &
Dinno, ]. A restricted range of emotional expres-
sion and its integration with eye gaze (e.g., during social
referencing) have been found to differentiate children
with ASD from typically developing children, as well
as those who have other developmental delays, as
early
as
 months
of
age
[Adrien
et
al.,
;
S. Clifford et al., ; Filliter et al., ; Gangi,
Ibanez, & Messinger, ; Nichols, Ibanez, Foss-Feig, &
Stone, ]. While core features of ASD vary by age,
cognitive ability, and language, one of the most stable
symptoms from early childhood through adolescences
is increased frequency of neutral expression [Bal, Kim,
Fok, & Lord, ]. As such, differences in facial affect
may show utility in assessing early risk for ASD.
A recent meta-analysis of facial expression production
in autism found that individuals with ASD display facial
expressions less often than non-ASD participants and
that, when they did display facial expressions, the expres-
sions occurred for shorter durations and were of different
quality than non-ASD individuals (Trevisan, Hoskyn, &
Birmingham, ). Decreases in the frequency of both
emotional facial expressions and the sharing of those
expressions with others has been demonstrated across
naturalistic interactions [Bieberich &
Morgan, ;
Czapinski
&
Bryson,
;
Dawson,
Hill,
Spencer,
Galpert,
&
Watson,
;
Mcgee,
Feldman,
&
Chernin, ; Snow, Hertzig, & Shapiro, ; Tantam,
Holmes, & Cordess, ], in lab-based assessments such
as during the ADOS  or the AOSI
 and in response to emotion-eliciting
videos . Fur-
thermore, higher frequency of neutral expressions corre-
lates with social impairment in children with ASD
 and differentiates them from chil-
dren with other delays [Bieberich & Morgan, ;
Yirmiya, Kasari, Sigman, & Mundy, ]. As such, fre-
quency and duration of facial affect is a promising early
risk marker for young children with autism.
Previous research on atypical facial expressions in chil-
dren with ASD has relied on hand coding of facial expres-
sions,
which
is
time
intensive
and
often
requires
signiﬁcant training [Bieberich & Morgan, ; S. Clifford
et al., ; Dawson et al., ; Gangi et al., ;
Mcgee
et
al.,
;
Nichols
et
al.,
;
Snow
et al., ]. This approach is not scalable for use in gen-
eral ASD risk screening or as a behavioral biomarker or
outcome assessment for use in large clinical trials. As
such, the ﬁeld has moved toward automating the coding
of facial expressions. In one of the earliest studies of this
approach, Guha and colleagues demonstrated that chil-
dren with ASD have atypical facial expressions when
mimicking others. However, their technology required
the children to wear markers on their face for data cap-
ture [Guha, Yang, Grossman, & Narayanan, ; Guha
et al., ], which is both invasive and not scalable.
More recently, several groups have applied non-invasive
CVA technology to measuring affect in older children
and adults with ASD within the laboratory setting
[Capriola-Hall et al., ; Owada et al., ; Samad
et al., ]. This represents an important move toward
scalability as CVA approaches do not rely on the presence
of physical markers on the face to extract emotion infor-
mation. Rather, CVA relies on videos of the individual in
which features around speciﬁc regions on a face (e.g., the
mouth and eyes) are extracted. Notably, these features
mirror those used by the manually rated facial affect cod-
ing system (FACS) . Both our earlier work
 and that of others [Capriola-Hall
et al., ] have shown good concordance between
human coding and CVA rating of facial emotions. Fur-
thermore, previous research in adults has demonstrated
that CVA can detect neutral facial expressions more reli-
ably than human coders .
Building on previous work applying CVA in laboratory
settings, we have developed a portable tablet-based tech-
nology that uses the embedded camera and automatic
CVA to code ASD risk behaviors in < min across a range
of non-laboratory settings (e.g., pediatric clinics, at home,
etc.). We developed a series of movies designed to capture
children’s attention, elicit emotion in response to novel
and interesting events, and assess the toddler’s ability to
sustain attention and share it with others. By embedding
INSAR
Carpenter et al./Digital behavioral phenotyping in ASD

these movies in a fully automated system on a cost-
effective tablet whereby the elicited behaviors, in this
case the frequency of different patterns of facial affect,
are automatically encoded with CVA, we aim to create a
tool that is objective, efﬁcient, and accessible. The cur-
rent analysis focuses on preliminary results supporting
the utility of this tablet-based assessment for the detec-
tion of facial movement and affect in young children and
the use of facial affect to differentiate children with and
without ASD. Though facial affect is the focus of the cur-
rent analysis, the ultimate goal is to combine information
across autism risk features collected through the current
digital screening tool [e.g., delayed response to name as
described in Campbell et al., ], to develop a risk score
based on multiple behaviors .
This information could then be combined with addi-
tional measures of risk to enhance screening for ASD.
Methods
Participants
Participants were  children – months of age
. Children were recruited at their pediatric pri-
mary care visit by a research assistant embedded within
the clinic or via referral from their physician, as well as
through community advertisement (N =  in the non-
ASD group and N =  in the ASD group). For children
recruited
within
the
pediatric
clinics,
recruitment
occurred at the - or -month well-child visit at the
same time as they received standard screening for ASD
with the M-CHAT-R/F. A total of % of the participants
recruited in the clinic by a research assistant chose to par-
ticipate. Of the participants who chose not to participate,
% indicated that they were not interested in the study,
whereas the remainder declined due to not having
enough time, having another child to take care of, want-
ing to discuss with their partner, or their child was
already too distressed after the physician visit. All chil-
dren who enrolled in the study found the procedure
engaging enough that they were able to provide adequate
data for analysis. Because the administration is very brief
and non-demanding, data loss was not a signiﬁcant
problem.
Exclusionary criteria included known vision or hearing
deﬁcits, lack of exposure to English at home, or insufﬁ-
cient English language skills for caregiver’s informed con-
sent. Twenty-two children were diagnosed with ASD. The
non-ASD comparison group  was comprised of
 typically developing children and  children with a
non-ASD delay, which was deﬁned by a diagnosis of lan-
guage delay or developmental delay of clinical signiﬁ-
cance sufﬁcient to qualify for speech or developmental
therapy as recorded in the electronic medical record. All
caregivers/legal guardians gave written informed consent,
and the study protocol was approved by the Duke Uni-
versity Health System IRB.
Children recruited from the pediatric primary care
settings received screening with a digital version of the
M-CHAT-R/F as part of a quality improvement study
ongoing in the clinic . Participants
recruited from the community received ASD screening
with the digital M-CHAT-R/F prior to the tablet assess-
ment. As part of their participation in the study, children
who either failed the M-CHAT-R/F or for whom there was
caregiver or physician concern about possible ASD under-
went diagnostic testing using the ADOS-Toddler (ADOS-
T)  conducted by a licensed psycholo-
gist or research-reliable examiner supervised by a licensed
Table .
Sample Demographics
Typically developing (N = ; %)
Non-ASD delay (N = ; %)
ASD (N = ; %)
Age
Months [mean (SD)]
. 
Sex
Female
 
 
 
Male
 
 
 
Ethnicity/race
African American
 
 
 
Caucasian
 
 
 
Hispanic
 
 
 
Other/unknown
 
 
 
Insurancea
Medicaid
 
 
 
Non-Medicaid
 
 
 
MCHAT resultb
Positive
 
 
 
Negative
 
 
 
aInsurance status was unknown for  (%) of participants in this study.
bChildren for whom the MCHAT was negative but received and ASD diagnosis were referred for assessment due to concerns by either the parent or the
child’s physician.
INSAR
Carpenter et al./Digital behavioral phenotyping in ASD

psychologist.
The
mean
ADOS-T
score
was
.
. A subset of the ASD children  also
received
the
Mullen
Scales
of
Early
Learning
. The mean IQ based on the Early Learning
Composite
Score
for
this
subgroup
was
.
. None of the children in the non-ASD com-
parison group was administered the ADOS or Mullen.
Children’s demographic information was extracted
from the child’s medical record or self-reported by the
caregiver at their study visit. Children in the ASD group
were, on average,  months younger than the compari-
son group . Furthermore, as
would be expected, there was a higher proportion of
males in the ASD group than in the comparison group,
though the difference was not statistically signiﬁcant (χ
 = ., P = .). There were no differences in the
proportion of racial/ethnic minority children between
the two groups . When
looking only at the children for which Medicaid status
was known, there was no difference in the proportion of
children on Medicaid in the ASD and the non-ASD group
.
Stimuli and Procedure
A series of developmentally appropriate brief movies
designed to elicit affect and engage the child’s attention
were shown on a tablet while the child sat on a care-
giver’s lap. The tablet was placed on a stand approxi-
mately  ft away from the child to prevent the child from
touching the screen as depicted in previous publications
[Campbell et al., ; Dawson et al., ; Hashemi
et al., ; Hashemi et al., ]. Movies consisted of
cascading
bubbles
( ×  sec),
a
mechanical
bunny
( sec), animal puppets interacting with each other
( sec), and a split screen showing a woman singing
nursery rhymes on one side and dynamic, noise-making
toys on the other side . These movies
included stimuli that have been used in previous studies
of ASD symptomatology , as well as
developed speciﬁcally for the current tablet-based tech-
nology to elicit autism symptoms, based on Dawson
et
al.
,
Jones,
Dawson,
Kelly,
Estes,
and
Webb , Jones et al. , and Luyster et al. .
At three points during the movies, the examiner located
behind the child called the child’s name.
Prior to the administration of the app, caregivers were
clearly instructed not to direct their child’s attention or
in any way try to inﬂuence the child’s behavior during
the assessment. Furthermore, if the caregiver began to try
to direct their child’s attention, the examiner in the room
immediately asked the caregiver to refrain from doing
so. If the caregiver persisted, this was noted on our valid-
ity form and the administration would have been consid-
ered
invalid.
Researchers
stopped
the
task
for
one
comparison
participant
due
to
crying.
Researchers
restarted the task for three participants with ASD due to
difﬁculty remaining in view of the tablet’s camera for
more than half of the ﬁrst video stimulus. If other family
members were present during the well-child visit, they
were asked to stand behind the caregiver and child so as
to not distract the child during the assessment. Addition-
ally, for children assessed during a well-child visit,
research assistants were instructed to collect data prior to
any planned shots or blood draws.
Computer Vision Analysis
The frontal camera in the tablet recorded video through-
out
the
experiment
at
 × 
resolution
and
 frames per second. The CVA algorithm [Hashemi
et al., ] ﬁrst automatically detected and tracked
 facial landmarks on the child’s face [De la Torre
et al., ]. Head positions relative to the camera were
estimated by computing the optimal rotation parameters
between the detected landmarks and a D canonical face
model . A “not visible” tag was
assigned to frames where the face was not detected or the
face
exhibited
drastic
yaw
(>
from
center).
We
acknowledge that the current method used only indicates
whether the child is oriented toward the stimulus and
does not track eye movements. For each “visible” frame,
the probability of expressing three standard categories of
facial expressions, Positive, Neutral (i.e., no active facial
Figure .
Example of movie stimuli. Developmentally appropriate movies consisted of cascading bubbles, a mechanical bunny, animal
puppets interacting with each other, and a split screen showing a woman singing nursery rhymes on one side and dynamic, noise-making
toys on the other side.
INSAR
Carpenter et al./Digital behavioral phenotyping in ASD

action
unit),
or
Other
(all
other
expressions),
was
assigned . The model for automatic
facial expression is an extension of the pose-invariant
and cross-modal dictionary learning approach originally
described in Hashemi et al. . During training, the
dictionary representation is setup to map facial informa-
tion between D and D modalities and is then able to
infer discriminative facial information for facial expres-
sions recognition even when only D facial images are
available at deployment. For training, data from Bing-
hamton University D Facial Expression database [Yin,
Wei, Sun, Wang, & Rosato, ] were used, along with
synthesized faces images with varying poses [see Hashemi
et al.,  for synthesis details]. Extracted image features
and distances between a subset of facial landmarks were
used as facial features to learn the robust dictionary.
Lastly, using the inferred discriminative D and frontal
D facial features, a multiclass support vector machine
 was trained to classify the different
facial expressions.
In
recent
years,
there
has
been
progress
on
automatic facial expression analysis of both children
and
toddlers
[Dys
&
Malti,
;
Gadea,
Aliño,
Espert,
&
Salvador,
;
Haines
et
al.,
;
LoBue & Thrasher, ; Messinger, Mahoor, Chow,
& Cohn, ]. In addition to this, we have previously
validated our CVA algorithm against expert human rater
coding of facial affect in a subsample of  video record-
ings across  participants .
This represents % of the non-ASD sample and a mat-
ched group from the ASD sample. The selection of partici-
pants for this previously published validation study was
based on age distribution to ensure representation across
the range of ages for both non-ASD and ASD groups. This
previous work showed strong concordance between CVA-
and human-rated coding of facial emotion in this data
set, with high precision, recall, and F scores of .,
., and ., respectively .
Statistical Approach
For each video frame, the CVA algorithm produces a
probability value for each expression (Positive, Neutral,
Other). We calculated the mean of the probability values
for each of the three expression types within non-
overlapping -frame ( sec) intervals, excluding frames
when the face was not visible. A -sec window was
selected as it provided us with a continuous distribution
of the emotion probabilities, while still being within the
.–-sec window of a macroexpression .
Additionally, for each of the name call events we
removed the time window starting at cue for the name
call prompt through the point where % of the audible
name calls actually occurred, plus  frames ( sec). This
window
was
selected
based
on
our
previous
study
 showing that orienting tended to
occur within a few seconds after a name call. We calcu-
lated the proportion of frames the child was not attend-
ing to the movie stimulus, based on the “visible” and
“not visible” tags described above, within each -frame
interval, excluding name call periods. Thus, for each
child, we generated four variables (mean probabilities of
Positive, Neutral, or Other; and proportion of frames not
attending) for each -frame interval within each of the
ﬁve movies.
To evaluate differences between ASD and non-ASD
children at regular intervals throughout the assessment,
we ﬁt a series of bivariate logistic regressions to obtain
the odds ratios for the associations between the mean
expression probability or attending proportion during a
given interval, parameterized as increments of %
points, and ASD diagnosis. We then ﬁt a series of multi-
variable logistic regression models, separately for each
movie and variable, which included parameters for each
of the -sec intervals within the movie to predict ASD
diagnosis. Given the large number of intervals relative to
the small sample size, we used a Least Absolute Shrinkage
and Selection Operator (LASSO) penalized regression
approach  to select a parsimonious set
of parameters representing the intervals within each
movie and expression type that were most predictive of
ASD diagnosis.
For each of the ﬁve movies, we then combined the
LASSO-selected interval parameters into a full logistic
model. When more than one expression parameter was
selected for a given interval, we selected the one with the
stronger odds ratio estimate. Analyses were conducted
with and without age as a covariate. Since the small study
size precluded having separate training and validation
sets, we used leave-one-out cross-validation to assess
model
performance.
Receiver-operating
characteristics
(ROC) curves were plotted and the c-statistic for the area
under the ROC curve was calculated for each movie.
Results
Figure

depicts
the
odds
ratio
analysis
using
the
“Rhymes and Toys” movie as one illustrative example. As
shown by the variability in the odds ratio estimates, some
parts of the movies elicited strongly differential responses
in certain patterns of expression ,
while in other sections, there were not substantial differ-
ences between the two groups .
Overlaid on the plot are the odds ratios and conﬁdence
bands
for
the
interval
parameters
selected
by
the
expression-speciﬁc LASSO models. These selected param-
eters were then used in the movie-level logistic models
for which we calculated classiﬁcation metrics.
INSAR
Carpenter et al./Digital behavioral phenotyping in ASD

Figure  compares the ROC curves for the ﬁve ﬁnal
movie-level logistic models after leave-one-out cross-
validation. ROC curves analyses were performed for each
video individually. The model for the “Rhymes” movie
yielded the strongest predictive ability, with an area
under the curve (AUC) of . (% conﬁdence interval
[CI] .–.), followed by the “Puppets” (AUC = .;
% CI .–.) and the “Bunny” (AUC = .; %
CI .–.) videos. Finally, the two “Bubbles” movies
that bookend the stimulus sets were the least predictive,
with AUCs of .  and . (% CI
.–.), respectively. Because there was a signiﬁcant
difference in age between the ASD and non-ASD com-
parison groups, we ran a second set of ROC analyses
where age was included as a covariate, shown in Table .
Results remained signiﬁcant after including the age
covariate.
Given the preponderance for the Other emotional cate-
gory in our non-ASD comparison group, we explored:
(a) what speciﬁc facial movements are driving this cate-
gory of Other expressions and (b) how it differs from the
Neutral expression category. We focused on analyzing
movements of facial landmarks and head pose angles and
how they differ between the facial expression categories.
Our CVA algorithm aligns the facial landmarks of the
child to a canonical face model through an afﬁne trans-
formation, which normalizes the landmark locations
across all video frames to a common space. This normali-
zation process is commonly used across CVA tasks related
to facial analysis as it allows one to analyze/compare
landmark locations across different frames or partici-
pants. With this alignment step, we were able to quantify
the distances between the eye corners and the corners of
the eyebrows, the vertical distance between the inner lip
points, and the vertical distance between the outer lip
points (Fig. , right). To interpret features that differenti-
ated the Neutral from the Other facial expression cate-
gory,
we
assessed
differences
between
these
facial
landmark distances of a given child when they were pre-
dominately expressing Neutral versus Other facial expres-
sions. We also included yaw and pitch head pose angles
since they may play a role in the alignment process.
Figure .
Time series of odds ratios for the association between the mean expression probability or proportion attending and ASD
diagnosis. Using the “Rhymes” movie as one illustrative example, lines depict the odds of meeting criteria for ASD  or being in
the non-ASD comparison group  for each of the outcomes of interest for each  sec time bin across the movie. Points with error
bars are intervals that were selected by the LASSO regression models and included in the ﬁnal logistic model. The blue window depicts a
segment of the movie where there were differential emotional responses between the ASD and non-ASD children. The green window
depicts a segment of the movie in which there was no difference in emotional responses between the groups.
Figure .
Receiver-operating characteristics (ROC) curves. ROC
curves were calculated for predictive ability of expression-speciﬁc
LASSO selected interval parameters for facial expressions and
attention to stimulus for each movie independently.
INSAR
Carpenter et al./Digital behavioral phenotyping in ASD

We focused in on three stimuli in which participants
exhibited
high
probabilities
of
Other
expressions,
namely, the ﬁrst Bubbles, Puppets, and Rhymes and Toys
videos. Out of the  participants, all exhibited frames
where both Neutral and Other facial expressions were
dominant (probability of expression over %). A Wilson
signed-ranks
test,
reported
as
(median
difference,
P value), indicated that within individual participants for
each diagnostic group, the median differences between
the distances of Other versus Neutral facial expressions
were signiﬁcantly higher for inner right eyebrows (non-
ASD: diff = ., P < .e-; ASD: diff = ., P < .e-),
inner left eyebrows (non-ASD: diff = ., P < .e-; ASD:
diff = ., P < .e-), outer right eyebrows (non-ASD:
diff = ., P < .e-; ASD: diff = ., P < .e-), outer left
eyebrows (non-ASD: diff = ., P < .e-; ASD: diff = .,
P < .e-), and mouth height (non-ASD: diff = .,
P < .e-), as well as for pitch head pose angles (non-
ASD: diff = ., P < .e-; ASD: diff = ., P < .e-);
but not for eye heights, lips parting, nor yaw head pose
angles.
Discussion
The present study evaluated an application administered
on a tablet that was comprised of carefully designed
movies that elicited affective expressions combined with
CVA of recorded behavioral responses to identify patterns
of facial movement and emotional expression that differ-
entiate toddlers with ASD from those without ASD. We
demonstrated that the movies elicited a range of affective
facial expressions in both groups. Furthermore, using
CVA we found children with ASD were more likely to dis-
play a neutral expression than children without ASD
when watching this series of videos, and the patterns of
facial expressions elicited during speciﬁc parts of the
movies differed between the two groups. We believe this
ﬁnding has strong face validity that rests on both
research and clinical observations of a restricted range of
facial expression in children with autism. Furthermore,
this replicates a previous ﬁnding of our group reporting
increased
frequency
of
neutral
expression
in
young children who screened positive on the M-CHAT
. Together, these preliminary results
support the use of engaging brief movies shown on a
cost-effective tablet, combined with automated CVA
behavioral coding, as an objective and feasible tool for
measuring an early emerging symptom of ASD, namely,
increased frequency of neutral facial expressions.
While the predictive power of emotional expression in
some of the videos varied, all but one represent a medium
effect, equivalent to Cohen’s d = . or greater [Rice &
Harris, ]. Overall, the best predictor from our battery
of videos is the “Rhymes” video, which had an AUC with
a large effect size . While this may
suggest that presenting the “Rhymes” video alone is sufﬁ-
cient for differentiating between the ASD and non-ASD
groups, we caution readers from coming to this conclu-
sion for two reasons: First, it is possible that, had we had
a larger sample, the other videos would have had a larger
effect. Second, we anticipate that there will be variability
in the ASD group with regard to which features a single
child will express and different videos may be better
suited to elicit different features in any given individual.
As such, we believe that it is important to understand
how each independent feature, in this case facial affect,
performs across the different videos so that we can being
to build better predictive models from combinations of
features [e.g., facial affect and postural sway as described
in Dawson et al., ].
To further understand the difference of facial expres-
sion in the non-ASD group as compared to our ASD sam-
ple, we explored the facial landmarks differentiating
between the Other facial expression category that domi-
nated the non-ASD control group versus the Neutral
facial expression, which was more common in the ASD
group. Through this analysis, we identiﬁed the features of
raised eyebrows and open mouth to play a role in dis-
criminating between the Other vs. Neutral categories.
This facial pattern is consistent with an engaged/inter-
ested look displayed when a child is actively watching, as
described in young children by Sullivan and Lewis .
It is interesting to note a raised pitch angle was also statis-
tically signiﬁcant. Since the median difference of this
angle between the two facial expression is small (.),
this
may
be
a
natural
movement
of
raising
one’s
eyebrows.
Our results need to be considered in light of several
limitations. First, the CVA models of facial expressions
used in the current study were trained on adult faces
. Despite this, our previous ﬁndings
with young children demonstrate good concordance
between human and CVA coding on the designation of
facial expressions . Furthermore,
the Other facial expression category includes all non-
positive or negative expressions. As such, even though
we were able to determine the predominant feature driv-
ing those expressions was the raised eyebrows, which is
in line with our observations from watching the movies,
Table .
Comparison of ASD Versus Non-ASD: Area Under the
Curve (AUC) Analyses
AUC without covariates
AUC with age in the model
Bubbles 
.
Bunny
.
Puppets
.
Rhymes
.
Bubbles 
.
INSAR
Carpenter et al./Digital behavioral phenotyping in ASD

it is possible that there are a combination of facial expres-
sions in the non-ASD group driving this designation.
Future studies will need to train on the engaged/inter-
ested facial expression speciﬁcally and test the robustness
of this ﬁnding. Additionally, though we have previously
demonstrated good reliability between our CVA algo-
rithms
and
human
coding
of
emotions
[Hashemi
et al., ], future validation of our CVA analysis of
emotional facial expressions in larger datasets is currently
underway. Second, using the LASSO statistical approach
means our model may not select all features that have dif-
ferentiating
information.
However,
we
selected
this
approach because it minimizes over ﬁtting the model.
Third, our sample size was relatively small and we do not
have separate training and testing samples. To account
for this, we applied cross-validation on the ROC curves
even though this decreased the performance metrics of
the model. This suggests that our ROC results are poten-
tially conservative. Finally, our comparison group con-
tains
both
children
with
typical
development
and
children with non-ASD developmental delays, a factor
that can be viewed as both a weakness and a strength.
Previous research has demonstrated that increased fre-
quency of neutral expressions does differentiate children
with ASD from those with other developmental delays
. How-
ever, due to the small sample size of children with non-
ASD developmental delays, we were unable to directly
test this in our data. Furthermore, because only a subset
of the sample received an assessment of cognitive ability,
it is possible that there were additional children in the
non-ASD comparison group that also had a developmen-
tal delay that was undetected. Ongoing research in a pro-
spective,
longitudinal
study
with
larger
samples
is
underway to further parse the ability of our CVA tools to
differentiate between children with ASD, children with a
non-ASD developmental delay and/or attention deﬁcit
hyperactivity disorder, and typically developing children.
While a difference in facial expression is one core fea-
ture of ASD, the heterogeneity in ASD means we do not
expect all children with ASD to display this sign of ASD.
As such, our next step is to combine the current results
with other measures of autism risk assessed through the
current digital screening tool, including response to
name , postural sway [Dawson
et al., ], and differential vocalizations [Tenenbaum
et al., ], among other features, to develop a risk score
based on multiple behaviors .
Since no one child is expected to display every risk behav-
ior, a goal is to determine thresholds based on the total
number of behaviors, regardless of which combination of
behaviors, to asses for risk. This is similar to what is done
in commonly used screening and diagnostic tools, such
as the M-CHAT , Autism Diagnostic
Figure .
Analysis of Other Facial Expression. The  panels on the left depict heat maps of aligned landmarks across ASD and non-ASD
participants when they were exhibiting Neutral and other facial expressions (the color bar indicates proportion of frames where land-
marks were displayed in a given image location). The single panel on the right is an example of the landmark distances explored.
INSAR
Carpenter et al./Digital behavioral phenotyping in ASD

Interview , and ADOS
.
In summary, we evaluated an integrated, objective tool
for the elicitation and measurement of facial movements
and expressions in toddlers with and without ASD. The
current study adds to a body of research supporting digi-
tal behavioral phenotyping as a viable method for
assessing autism risk behaviors. Our goal is to further
develop and validate this tool so that it can eventually be
used within the context of current standard of care to
enhance autism screening in pediatric populations.
Acknowledgments
Funding for this work was provided by NIH R-MH

(Sapiro,
Dawson
PIs),
NIH
RO-MH
(Dawson, Sapiro PIs), NICHD PHD (Dawson,
Kollins, PIs), Simons Foundation (Sapiro, Dawson, PIs),
Duke Department of Psychiatry and Behavioral Sciences
PRIDe award (Dawson, PI), Duke Education and Human
Development Initiative, Duke-Coulter Translational Part-
nership Grant Program, National Science Foundation, a
Stylli Translational Neuroscience Award, and the Depart-
ment of Defense. Some of the stimuli used for the movies
were created by Geraldine Dawson, Michael Murias, and
Sara Webb at the University of Washington. This work
would not have been possible without the help of Eliza-
beth Glenn, Elizabeth Adler, and Samuel Marsan. We also
gratefully acknowledge the participation of the children
and families in this study. Finally, we could not have
completed this study without the assistance and collabo-
ration of Duke pediatric primary care providers.
Conﬂict of interests
Guillermo Sapiro has received basic research gifts from
Amazon, Google, Cisco, and Microsoft and is a consul-
tant for Apple and Volvo. Geraldine Dawson is on the Sci-
entiﬁc
Advisory
Boards
of
Janssen
Research
and
Development, Akili, Inc., LabCorp, Inc., Tris Pharma, and
Roche Pharmaceutical Company, a consultant for Apple,
Inc, Gerson Lehrman Group, Guidepoint, Inc., Teva Phar-
maceuticals, and Axial Ventures, has received grant
funding from Janssen Research and Development, and is
CEO of DASIO, LLC (with Guillermo Sapiro). Dawson
receives royalties from Guilford Press, Springer, and
Oxford University Press. Dawson, Sapiro, Carpenter,
Hashemi, Campbell, Espinosa, Baker, and Egger helped
develop aspects of the technology that is being used in
the study. The technology has been licensed and Daw-
son, Sapiro, Carpenter, Hashemi, Espinosa, Baker, Egger,
and Duke University have beneﬁted ﬁnancially.
References
Adrien, J. L., Faure, M., Perrot, A., Hameury, L., Garreau, B.,
Barthelemy, C., & Sauvage, D. . Autism and family
home movies: Preliminary ﬁndings. Journal of Autism and
Developmental Disorders, , –.
Adrien, J. L., Lenoir, P., Martineau, J., Perrot, A., Hameury, L.,
Larmande, C., & Sauvage, D. . Blind ratings of early
symptoms of autism based upon family home movies. Jour-
nal of the American Academy of Child and Adolescent Psy-
chiatry, , –. 
-
Bal, V. H., Kim, S. H., Fok, M., & Lord, C. . Autism spec-
trum disorder symptoms from ages  to  years: Implications
for
diagnosing
adolescents
and
young
adults.
Autism
Research, , –. 
Baranek, G. T. . Autism during infancy: a retrospective
video analysis of sensory-motor and social behaviors at –
months of age. Journal of Autism and Developmental Disor-
ders, , –.
Bieberich, A. A., & Morgan, S. B. . Self-regulation and
affective expression during play in children with autism or
Down syndrome: A short-term longitudinal study. Journal of
Autism
and
Developmental
Disorders,
,
–.

Bieleninik, Ł., Posserud, M.-B., Geretsegger, M., Thompson, G.,
Elefant, C., & Gold, C. . Tracing the temporal stability
of autism spectrum diagnosis and severity as measured by the
Autism Diagnostic Observation Schedule: A systematic review
and meta-analysis. PLoS One, , e–e.

Campbell, K., Carpenter, K. L., Hashemi, J., Espinosa, S.,
Marsan, S., Borg, J. S., … Dawson, G. . Computer vision
analysis captures atypical attention in toddlers with autism.
Autism,
,
–.


Campbell, K., Carpenter, K. L. H., Espinosa, S., Hashemi, J.,
Qiu, Q., Tepper, M., … Dawson, G. . Use of a Digital
Modiﬁed Checklist for Autism in Toddlers—Revised with
follow-up to improve quality of screening for autism. The
Journal of Pediatrics, , –.e. 
/j.jpeds...
Capriola-Hall, N. N., Wieckowski, A. T., Swain, D., Tech, V.,
Aly, S., Youssef, A., … White, S. W. . Group differences
in facial emotion expression in autism: Evidence for the util-
ity
of
machine
classiﬁcation.
Behavior
Therapy,
,
–. 
Chang, C.-C., & Lin, C.-J. . LIBSVM: A library for support
vector machines. ACM Transactions on Intelligent Systems
and Technology , –.
Christensen, D. L., Baio, J., Braun, K. V., Bilder, D., Charles, J.,
Constantino, J. N., … Yeargin-Allsopp, M. . Prevalence
and characteristics of autism spectrum disorder among chil-
dren aged  years—Autism and Developmental Disabilities
Surveillance Summaries, , –.
Clifford, S., & Dissanayake, C. . Dyadic and triadic behav-
iours in infancy as precursors to later social responsiveness in
young children with autistic disorder. Journal of Autism and
INSAR
Carpenter et al./Digital behavioral phenotyping in ASD

Developmental Disorders, , –. 
org/./s---x
Clifford, S., Young, R., & Williamson, P. . Assessing the
early characteristics of autistic disorder using video analysis.
Journal of Autism and Developmental Disorders, ,
–. 
Clifford, S. M., & Dissanayake, C. . The early development
of joint attention in infants with autistic disorder using home
video observations and parental interview. Journal of Autism
and Developmental Disorders, , –. 
org/./s---
Czapinski, P., & Bryson, S. . Reduced facial muscle move-
ments in autism: Evidence for dysfunction in the neuromus-
cular pathway? Brain and Cognition, , –.
Dawson, G., & Bernier, R. . A quarter century of progress
on the early detection and treatment of autism spectrum
disorder. Development and Psychopathology, ,
–. 
Dawson, G.,
Campbell,
K.,
Hashemi,
J.,
Lippmann,
S.
J.,
Smith, V., Carpenter, K., … Sapiro, G. . Atypical pos-
tural control can be detected via computer vision analysis in
toddlers with autism spectrum disorder. Scientiﬁc Reports, 
, . 
Dawson, G., Hill, D., Spencer, A., Galpert, L., & Watson, L.
. Affective exchanges between young autistic-children
and their mothers. Journal of Abnormal Child Psychology, 
, –. 
Dawson, G., & Sapiro, G. . Potential for digital behavioral
measurement tools to transform the detection and diagnosis
of
autism
spectrum
disorder.
JAMA
Pediatrics,
,
–. 
Dawson, G., Toth, K., Abbott, R., Osterling, J., Munson, J.,
Estes, A., & Liaw, J. . Early social attention impairments
in autism: social orienting, joint attention, and attention to
distress. Developmental Psychology, , –. https://
doi.org/./-...
De la Torre, F., Chu, W. S., Xiong, X., Vicente, F., Ding, X., &
Cohn, J. . IntraFace. IEEE International Conference on
Automatic Face Gesture Recognition Workshops. .
Ljubljana, Slovenia: IEEE. 

Dys, S. P., & Malti, T. . It’s a two-way street: Automatic
and controlled processes in children’s emotional responses
to moral transgressions. Journal of Experimental Child Psy-
chology,
,
–.

.
Egger, H. L., Dawson, G., Hashemi, J., Carpenter, K. L. H.,
Espinosa, S., Campbell, K., … Sapiro, G. . Automatic
emotion and attention analysis of young children at home: A
ResearchKit autism feasibility study. NPJ Digital Medicine, 
, . 
Ekman, P. . Emotions revealed (nd ed.). New York, NY:
Times Books.
Ekman, R. . What the face reveals: Basic and applied stud-
ies of spontaneous expression using the Facial Action Coding
System (FACS). New York, NY: Oxford University Press.
Filliter, J. H., Longard, J., Lawrence, M. A., Zwaigenbaum, L.,
Brian, J., Garon, N., … Bryson, S. E. . Positive affect in
infant siblings of children diagnosed with autism spectrum
disorder. Journal of Abnormal Child Psychology, ,
–. 
Fischler, M. A., & Bolles, R. C. . Random sample consen-
sus: A paradigm for model ﬁtting with applications to image
analysis and automated cartography. Communications of the
Association
for
Computing
Machinery,
,
–.

Gadea, M., Aliño, M., Espert, R., & Salvador, A. . Deceit
and facial expression in children: The enabling role of the
“poker face” child and the dependent personality of the
detector. Frontiers in Psychology, , –. 
org/./fpsyg..
Gangi, D. N., Ibanez, L. V., & Messinger, D. S. . Joint atten-
tion initiation with and without positive affect: Risk group
differences and associations with ASD symptoms. Journal of
Autism and Developmental Disorders, , –.

Gotham, K., Risi, S., Pickles, A., & Lord, C. . The Autism
Diagnostic Observation Schedule: Revised algorithms for
improved diagnostic validity. Journal of Autism and Develop-
mental Disorders, , –. 
s---
Guha, T., Yang, Z., Grossman, R. B., & Narayanan, S. S. .
A computational study of expressive facial dynamics in
children
with
autism.
IEEE
Transactions
on
Affective
Computing, , –. 

Guha, T., Yang, Z., Ramakrishna, A., Grossman, R. B., Darren, H.,
Lee, S., & Narayanan, S. S. . On quantifying facial
expression-related atypicality of children with autism spec-
trum disorder. Proceedings of IEEE International Conference
on Acoustics, Speech, and Signal Processing, , –.

Guthrie, W., Wallis, K., Bennett, A., Brooks, E., Dudley, J.,
Gerdes, M., … Miller, J. S. . Accuracy of autism screen-
ing
in
a
large
pediatric
network.
Pediatrics,
,
e. 
Haines,
N.,
Bell,
Z.,
Crowell,
S.,
Hahn,
H.,
Kamara,
D.,
McDonough-Caplan, H., … Beauchaine, T. P. . Using
automated computer vision and machine learning to code
facial expressions of affect and arousal: Implications for emo-
tion dysregulation research. Development and Psychopathol-
ogy,
,
–.


Hashemi, J., Campbell, K., Carpenter, K., Harris, A., Qiu, Q.,
Tepper, M., … Calderbank, R. . A scalable app for mea-
suring autism risk behaviors in young children: A technical validity
and feasibility study. Paper presented at the Proceedings of the
th EAI International Conference on Wireless Mobile Com-
munication and Healthcare, Dublin, Ireland.
Hashemi, J., Dawson, G., Carpenter, K. L. H., Campbell, K.,
Qiu, Q., Espinosa, S., … Sapiro, G. . Computer vision
analysis for quantiﬁcation of autism risk behaviors. IEEE
Transactions on Affective Computing, –. 
./taffc..
Jones, E. J., Dawson, G., Kelly, J., Estes, A., & Webb, S. J. .
Parent-delivered early intervention in infants at risk for ASD:
Effects on electrophysiological and habituation measures of
social attention. Autism Research, , –.
INSAR
Carpenter et al./Digital behavioral phenotyping in ASD

Jones, E. J., Venema, K., Earl, R., Lowy, R., Barnes, K., Estes, A., …
Webb, S. . Reduced engagement with social stimuli in
-month-old infants with later autism spectrum disorder: A
longitudinal prospective study of infants at high familial risk.
Journal of Neurodevelopmental Disorders, , .
Khowaja, M., Robins, D. L., & Adamson, L. B. . Utilizing
two-tiered screening for early detection of autism spectrum
disorder.
Autism,
,
–.


Lewinski, P. . Automated facial coding software outper-
forms people in recognizing neutral faces as neutral from
standardized datasets. Frontiers in Psychology, , .

LoBue, V., & Thrasher, C. . The Child Affective Facial
Expression (CAFE) set: Validity and reliability from untrained
adults. Frontiers in Psychology, , . 
/fpsyg..
Lord, C., Risi, S., Lambrecht, L., Cook, E. H., Jr., Leventhal, B. L.,
DiLavore, P. C., … Rutter, M. . The autism diagnostic
observation schedule-generic: A standard measure of social
and communication deﬁcits associated with the spectrum of
autism. Journal of Autism and Developmental Disorders, 
, –.
Lord, C., Rutter, M., & Le Couteur, A. . Autism Diagnostic
Interview-Revised: A revised version of a diagnostic interview
for caregivers of individuals with possible pervasive develop-
mental disorders. Journal of Autism and Developmental Dis-
orders, , –.
Luyster, R., Gotham, K., Guthrie, W., Cofﬁng, M., Petrak, R.,
Pierce, K., … Lord, C. . The Autism Diagnostic Observa-
tion
Schedule-toddler
module:
A
new
module
of
a
standardized diagnostic measure for autism spectrum disor-
ders. Journal of Autism and Developmental Disorders, ,
–. 
Maestro, S., Muratori, F., Cavallaro, M. C., Pei, F., Stern, D.,
Golse, B., & Palacio-Espasa, F. . Attentional skills during
the ﬁrst  months of age in autism spectrum disorder. Journal
of the American Academy of Child and Adolescent Psychia-
try, , –. 
-
Mcgee, G. G., Feldman, R. S., & Chernin, L. . A compari-
son of emotional facial display by children with autism and
typical preschoolers. Journal of Early Intervention, ,
–. 
Messinger, D. S., Mahoor, M. H., Chow, S. M., & Cohn, J. F.
. Automated measurement of facial expression in
infant-mother interaction: A pilot study. Infancy, ,
–. 
Mullen, E. M. . Mullen scales of early learning. Circle
Pines, MN: American Guidance Service Inc.
Murias, M., Major, S., Compton, S., Buttinger, J., Sun, J. M.,
Kurtzberg, J., & Dawson, G. . Electrophysiological bio-
markers predict clinical improvement in an open-label trial
assessing efﬁcacy of autologous umbilical cord blood for treat-
ment of autism. Stem Cells Translational Medicine, ,
–. 
Myers, S. M., Johnson, C. P., & Council on Children With Dis-
abilities.
.
Management
of
children
with
autism
spectrum disorders. Pediatrics, , –. https://
doi.org/./peds.-
Nichols, C. M., Ibanez, L. V., Foss-Feig, J. H., & Stone, W. L.
. Social smiling and its components in high-risk infant
siblings without
later
ASD
symptomatology.
Journal
of
Autism
and
Developmental
Disorders,
,
–.

Osterling, J. A., Dawson, G., & Munson, J. A. . Early recog-
nition of -year-old infants with autism spectrum disorder
versus mental retardation. Development and Psychopathol-
ogy, , –.
Owada, K., Kojima, M., Yassin, W., Kuroda, M., Kawakubo, Y.,
Kuwabara, H., … Yamasue, H. . Computer-analyzed
facial expression as a surrogate marker for autism spectrum
social core symptoms. PLoS One, , e. https://
doi.org/./journal.pone.
Rice, M. E., & Harris, G. T. . Comparing effect sizes in
follow-up studies: ROC area, Cohen’s d, and r. Law and
Human Behavior, , –. 
s---
Robins, D. L., Casagrande, K., Barton, M., Chen, C. M., Dumont-
Mathieu, T., & Fein, D. . Validation of the modiﬁed
checklist for autism in toddlers, revised with follow-up (M-
CHAT-R/F).
Pediatrics,
,
–.

/peds.-
Samad, M. D., Diawara, N., Bobzien, J. L., Harrington, J. W.,
Witherow,
M.
A.,
&
Iftekharuddin,
K.
M.
.
A
feasibility study of autism behavioral markers in spontaneous
facial, visual, and hand movement response data. IEEE
Transactions on Neural Systems and Rehabilitation Engineer-
ing,
,
–.


Snow, M. E., Hertzig, M. E., & Shapiro, T. . Expression
of emotion in young autistic children. Journal
of the
American Academy of Child and Adolescent Psychiatry, 
, –. 

Sullivan, M. W., & Lewis, M. . Emotional expressions of
young infants and children—A practitioner’s primer. Infants
and Young Children, , –. 
/--
Tantam, D., Holmes, D., & Cordess, C. . Nonverbal expres-
sion in autism of Asperger type. Journal of Autism and Devel-
opmental Disorders, , –.
Tenenbaum, E. J., Carpenter, K. L. H., Sabatos-DeVito, M.,
Hashemi, J., Vermeer, S., Sapiro, G., & Dawson, G. . A
six-minute measure of vocalizations in toddlers with autism
spectrum
disorder.
Autism
Research,
,
–.

Tibshirani, R. . Regression shrinkage and selection via the
lasso. Journal of the
Royal Statistical
Society: Series B
, –. 
-..tb.x
Trevisan, D. A., Bowering, M., & Birmingham, E. .
Alexithymia, but not autism spectrum disorder, may be
related to the production of emotional facial expressions.
Molecular Autism, , . 
-
INSAR
Carpenter et al./Digital behavioral phenotyping in ASD

Trevisan, D. A., Hoskyn, M., & Birmingham, E. . Facial
expression production in autism: A meta-analysis. Autism
Research, , –. 
Waizbard-Bartov, E., Ferrer, E., Young, G. S., Heath, B., Rogers, S.,
Wu Nordahl, C., … Amaral, D. G. . Trajectories of
autism symptom severity change during early childhood.
Journal of Autism and Developmental Disorders. 
org/./s---z
Werner, E., Dawson, G., Osterling, J., & Dinno, N. . Brief
report: Recognition of autism spectrum disorder before one
year of age: A retrospective study based on home videotapes.
Journal of Autism and Developmental Disorders, ,
–.
Yin, L., Wei, X., Sun, Y., Wang, J., & Rosato, M. J. . A D
facial expression database for facial behavior research. Paper pres-
ented at the th International Conference on automatic face
and gesture recognition , University of Southampton,
Southampton, UK.
Yirmiya, N., Kasari, C., Sigman, M., & Mundy, P. . Facial
expressions of affect in autistic, mentally retarded and normal
children. Journal of Child Psychology and Psychiatry, ,
–.
INSAR
Carpenter et al./Digital behavioral phenotyping in ASD


SCiENtifiC REPOrtS |  : | DOI:./s---

Atypical postural control can be 
detected via computer vision 
analysis in toddlers with autism 
spectrum disorder
Geraldine Dawson   
, Kathleen Campbell, Jordan Hashemi, Steven J. Lippmann   
, 
Valerie Smith, Kimberly Carpenter, Helen Egger, Steven Espinosa, Saritha Vermeer, 
Jeffrey Baker & Guillermo Sapiro
Evidence suggests that differences in motor function are an early feature of autism spectrum disorder 
(ASD). One aspect of motor ability that develops during childhood is postural control, reflected in the 
ability to maintain a steady head and body position without excessive sway. Observational studies have 
documented differences in postural control in older children with ASD. The present study used computer 
vision analysis to assess midline head postural control, as reflected in the rate of spontaneous head 
movements during states of active attention, in  toddlers between – months of age (Mean =  
months),  of whom were diagnosed with ASD. Time-series data revealed robust group differences 
in the rate of head movements while the toddlers watched movies depicting social and nonsocial 
stimuli. Toddlers with ASD exhibited a significantly higher rate of head movement as compared to 
non-ASD toddlers, suggesting difficulties in maintaining midline position of the head while engaging 
attentional systems. The use of digital phenotyping approaches, such as computer vision analysis, 
to quantify variation in early motor behaviors will allow for more precise, objective, and quantitative 
characterization of early motor signatures and potentially provide new automated methods for early 
autism risk identification.
Although the core symptoms of autism spectrum disorder (ASD) are defined by atypical patterns of social inter-
action and the presence of stereotyped and repetitive behaviors and interests, evidence suggests that differences 
in motor function are also an important early feature of autism. Motor delays could contribute to early hall-
mark autism symptoms, including difficulties in orienting to name involving the eyes and head turns, coordinat-
ing head and limb movements involved in gaze following and other joint attention behavior, such as pointing. 
Teitelbaum et al. found that atypical movements (e.g. shape of mouth, patterns of lying, righting, sitting) were 
present by – months of age in infants later diagnosed with ASD. Another study of videotapes taken of infants 
– weeks of age detected lower levels of positional symmetry among infants later diagnosed with ASD sug-
gesting atypical development of cerebellar pathways that control balance and symmetry. Six-month-old infants 
who later were diagnosed with ASD tend to exhibit head lag when pulled to sit, reflecting early differences in 
motor development. A study of home videos taken between birth and six months of age found that some infants 
who were later diagnosed with ASD showed postural stiffness, slumped posture, and/or head lag. Other motor 
symptoms observed in infants later diagnosed with ASD include fluctuating muscle tone and oral-motor abnor-
malities, such as insufficient opening of the mouth in anticipation of the approaching spoon during feeding. 
Longitudinal research with very low birth weight infants revealed that infants who are later diagnosed with ASD 
had poorer ability in maintaining midline position of the head at – weeks of age. The authors used visual 
Duke Center for Autism and Brain Development, Department of Psychiatry and Behavioral Sciences, Duke 
University, Durham, North Carolina, USA. University of Utah, Salt Lake City, Utah, USA. Department of Electrical 
and Computer Engineering, Duke University, Durham, North Carolina, USA. Department of Population Health 
Sciences, Duke University, Durham, North Carolina, USA. NYU Langone Child Study Center, New York University, 
New York, New York, USA. Department of Pediatrics, Duke University, Durham, NC, USA. Departments of 
Biomedical Engineering, Computer Science, and Mathematics, Duke University, Durham, NC, USA. Correspondence 
and requests for materials should be addressed to G.D. (email: )
Received:  July 
Accepted:  October 
Published online:  November 
OPEN
Corrected: Author Correction
/

SCiENtifiC REPOrtS |  : | DOI:./s---
inspection to classify head position in each video frame to yield a measure of midline head position and number 
of changes in position.
The development of postural control is an index of neuromuscular reactions to the motion of body mass in 
order to retain stability. Previous studies have documented the developmental progression of the ability to main-
tain an upright posture that is accompanied by decreases in postural sway. Several studies with older children 
with ASD have documented deficiencies in postural control, reflected in the presence of postural sway, which is 
accentuated when children with ASD are viewing arousing stimuli, including complex multi-sensory and social 
stimuli–. Less is known about the presence of postural sway in young children with ASD.
Studies of motor and other behaviors in young children have typically relied on subjective and labor-intensive 
human coding to rate and measure behavior. The recent use of digital phenotyping approaches, such as computer 
vision analysis (CVA) of videotaped recordings of behavior, has allowed for automated, precise and quantitative 
measurement of subtle, dynamic differences in motor behavior. We reported previously on a result using CVA to 
more precisely measure toddlers’ orienting response to a name call, noting that, compared to toddlers without 
ASD, toddlers with ASD oriented less frequently; when they did orient, their head turn was a full second slower, on 
average. Such differences in motor speed would likely not be detected with the naked eye during a typical clinical 
evaluation. Anzulewicz et al. used smart tablet computers with touch-sensitive screens and embedded inertial 
movement sensors to record movement kinematics and gesture forces in –-year-old children with and without 
ASD. Children with ASD used greater force and faster and larger gesture kinematics. Machine learning analysis of 
the children’s motor patterns classified the children with ASD with a high level of accuracy. In another study using 
automated methods, differences in head movement dynamics were found between .–.-year-old children 
with and without ASD while they watched movies of social and nonsocial stimuli. Children with ASD showed 
more frequent head turning, especially while watching social stimuli. The authors suggested that the children 
with ASD might be using head movement to modulate their arousal while watching social stimuli. Wu et al.  
used electromagnetic sensors to analyze continuous movements at millisecond time scale in older children with 
ASD versus typical development. They applied a triangular smoothing algorithm to the D positional raw move-
ment data that preserved the local speed fluctuations. They found that individuals with ASD exhibited signifi-
cantly more “sensorimotor noise” when compared to individuals with typical development.
The present study used CVA to characterize head movements that didn’t involve spontaneous or volitional 
orienting or turning away from the stimuli. Rather, we were interested in subtler midline head movements that 
are more likely related to postural stability. The study compared the behaviors of toddlers with ASD versus those 
without ASD while the children watched a series of dynamic movies involving different types of stimuli, includ-
ing stimuli of both a social and nonsocial nature. While the children watched the movies, their head movements 
were automatically detected and tracked using landmarks on the participant’s face. The goal of this analysis was to 
quantify the rate of spontaneous head movements and to determine whether there were differences in this motor 
feature between young children with and without ASD.
Methods
Participants. 
Participants were  children between – months of age (Mean =  months). 
Exclusionary criteria included known vision or hearing deficits, lack of exposure to English at home and/or car-
egivers who did not speak and read English sufficiently for informed consent. Twenty-two of the children had 
autism spectrum disorder. The non-ASD comparison group was comprised of  typically developing children 
and  children with language delay or developmental delay of clinical significance sufficient to qualify for speech 
or developmental therapy. Participants in the comparison group had a mean age of . months  and 
those in the ASD group had a mean age of . months . Ethnic/racial composition of the ASD and 
comparison groups, respectively, was % and % white, % and % African American, % and % Asian, 
and % and % multi-racial/other. Percent males was % in the ASD group and % in the comparison group.
Participants were recruited from primary care pediatric clinics by a research assistant, referral from their phy-
sician, and by community advertisement. All caregivers/legal guardians of participants gave written, informed 
consent, and the study protocol was approved by the Duke University Health System Institutional Review Board. 
Methods were carried out in accordance with institutional, state, and federal guidelines and regulation.
Diagnostic Assessments. 
Diagnostic evaluations to confirm ASD were based on the Autism Diagnostic 
Observation Scale-Toddler (ADOS-T), which were conducted by a licensed psychologist or trained 
research-reliable examiner overseen by a licensed psychologist. The mean ADOS-T score was . . 
The mean IQ based on the Mullen Scales of Early Learning Composite Score for the ASD group was . 
. Developmental and/or language delay was determined based on the Mullen Scales (> SD below 
the mean in overall learning composite or receptive/expressive language).
Stimuli. 
A series of stimuli, comprised of brief movies, were shown on a smart tablet while the child sat on 
a caregiver’s lap. The tablet was placed on a stand approximately  feet away from the child to prevent the child 
from touching the screen. The stimuli consisted of a series of brief developmentally-appropriate movies designed 
to elicit positive affect and engage the child’s attention. The movies consisted of cascading bubbles, a mechanical 
bunny, animal puppets interacting with each other, and a split screen showing on one side a woman singing 
nursery rhymes and on the other side dynamic, noise-making toys. The lengths of the movies were  seconds 
(Bubbles),  seconds (Rhyme), and ∼ seconds (Bunny and Puppets). Each movie was shown once except for 
Bubbles which was shown at the beginning and end of the series. The entire series of movies lasted  minutes. 
Examples of the stimuli and experimental setup are presented in Fig.  and described in two previous publi-
cations. Examples of clips from the movies are provided in the Supplementary Material. During three of the 
movies, the examiner, standing behind the child, called the child’s name. A failure to orient to name is an early 
/

SCiENtifiC REPOrtS |  : | DOI:./s---
symptom of autism, and results of our analysis of the orienting results have previously been published. However, 
all segments when children looked away from the movie, including to orient to name, as well as all  second seg-
ments post the name-call stimulus, were automatically removed from the present analyses. Specifically, in order to 
remove any influence on head movement due to the child orienting when his or her name was called, we removed 
the time window starting at cue for the name call prompt (a subtle icon used to prompt the examiner to call the 
name) through the point where % of the audible name calls actually occurred, plus  frames ( seconds). 
Since previous studies have shown that orienting tends to occur within a few seconds after a name call, this elim-
inated segments influenced by the name call.
Parents were asked to attempt to keep the child seated in their lap, but to allow the child to get off their lap if 
the child became too distressed to stay seated. Researchers stopped the task for  child due to crying. Researchers 
restarted the task for three participants due to noncompliance.
Computer Vision Analysis. 
The frontal camera in the tablet recorded video of the child’s face throughout 
the experiment at  ×  spatial resolution and  frames per second. The fully automatic CVA algorithm 
detects and tracks  facial landmarks on the child’s face  and estimates head pose angles relative to 
the camera by computing the optimal rotation parameters between the detected landmarks and a D canonical 
face model. For each video frame the algorithm outputted D positional coordinates of the facial landmarks and 
 head pose angles: yaw (left-right), pitch (up-down), and roll (tilting left-right). The yaw head pose angle was 
used to determine the frames when the child was engaged with the movie stimuli, where frames exhibiting a yaw 
pose with a magnitude less than ° were considered as the child being engaged.
Following the work of, to quantify head movement when the child is engaged (less than ° yaw), per-frame 
pixel-wise displacements of  central facial landmarks were computed and normalized with respect to the child’s 
eye width, thus head movement was measured as a (normalized) proportion of the child’s eye width per frame. 
The pixel-wise displacements of the central facial landmarks are dependent on the child’s distance to the camera 
in the tablet. Although the tablet was placed approximately  feet away from the child at the start of the experi-
ment, the child is free to move throughout the experiment, thus affecting the magnitude of landmark displace-
ments (when the child is near to the camera the pixel displacements are larger than if the child did the same 
movement but farther away from the camera). Normalizing the displacements with respect to the eye-width 
diminishes this distance to camera dependency. More formally, the head movement between frame n and n- is 
defined as the average Euclidean displacements of the central nose, left inner eye, and right inner eye landmarks 
 normalized by a ± second windowed-average, centered around frame n, of the Euclidean distances 
between the inner left and right eye landmarks,
−
−
+
d
w
,
n
n
n
n
 ,
 ,

Figure .  iPad movie task and facial landmark detection: (A) Two examples of facial landmark points detected 
by CVA and estimated head pose (indicated by the three arrows). The landmarks colored in red are the inner 
left, inner right, and central nose landmarks that are used for head movement computation. The left example 
depicts landmarks and head pose of a participant engaged in the movie stimuli; while in the right example, the 
participant is looking away. Both states are automatically detected. (B) Example frames from movie stimuli. 
Each row displays a frame from corresponding movie stimuli show in the columns (going from left-to-right): 
Bubbles ( seconds, two repetitions), Bunny ( seconds), Rhymes ( seconds), and Puppet show ( seconds).
/

SCiENtifiC REPOrtS |  : | DOI:./s---
where 
−
dn
n
 ,  is the average landmark displacement of the three central landmarks between frame n and n-, 
and 
−
+
w
n
n
,
 is the average Euclidean distance between the left and right eye landmarks when the child is 
engaged between a half-second ( frames) before and after frame n.
Results evaluating the validity of the CVA methods, which rely on landmark identification and tracking on the 
face, have been previously published. One study demonstrated high reliability between the automatic methods 
and the expert human rater of head movements, with agreement between the computer and expert clinical rater 
occurring .% of the time with interrater reliability based on Cohen’s kappa = .. A second study compared 
the automatic classification based on landmarks to human coders for head movement, demonstrated inter-rater 
reliability based on intraclass correlation coefficient (ICC) = .. Other papers report high reliability between 
CVA and human coding for head turning in response to name  and positive affective expression 
(happy; ICC = . and . for ASD and non-ASD toddlers).
The original dataset consisted of frame-by-frame measurements of head movement, with observations for 
each /th of a second. Groups interested in direct use of the data can do so via collaboration with the authors 
due to privacy and consent considerations as well as backend designs, and the data will be stored in a separate 
partition at Duke University. In order to prepare the data for statistical analysis, we first aggregated the move-
ment measurements by calculating the head movement rate, defined as the moving sum of the cumulative 
frame-by-frame movement measurements for each  frame period (representing /rd of a second). If any indi-
vidual frames within a -frame set were set to missing, such as when the facial landmarks were not visible or 
during the name-call period, the moving sum was also set to missing. Outliers were addressed by Winsorizing to 
the th percentile prior to aggregation.
All statistical analyses were performed separately for each of the movie stimuli. To visualize the time series, we 
calculated and plotted the median head movement rate as well as the st and rd quartiles at each / second time 
interval for both ASD and non-ASD children.
Unadjusted and adjusted rate ratios for the association between ASD diagnosis and the rate of head move-
ment in each / second time interval were estimated using a generalized linear mixed log-gamma regression 
model. Adjusted estimates controlled for ethnicity/race (white; other), age (in months), and sex (male; female). To 
account for potential within-subject correlations due to repeated measurement, we included a random intercept 
for each participant.
Results
The time series data depicting the rate of head movement, defined as the distance traveled per / second ( 
videoframes), for the ASD and non-ASD groups are shown in Fig. .
Based on a generalized linear mixed regression model with a log link and gamma distribution (adjusting for 
ethnicity/race, age, and sex), significant associations between diagnostic group (ASD versus non-ASD) and rate of 
head movement were found during all movies except for the Bubbles , the last movie. For Bubbles , the shorter 
duration might have affected power to detect a result as there was nevertheless a trend toward a group difference 
in the same direction as all other movies. Results of the analysis are shown in Table .
Robust group differences in the rate of head movement were evident during  out of  of the movies. For 
example, the rate of head movement among participants with ASD was . times that of non-ASD participants 
during the Bunny movie, after adjusting for age, ethnicity/race, and sex . The 
rate ratio was higher for all movies that had animated and more complex stimuli (Bunny, Puppets, Rhymes and 
Toys), as compared to the less complex Bubbles videos.
Although the LD/DD group was too small to conduct independent analyses of that group, as a sensitivity anal-
ysis, the  patients with LD/DD were from the main regression model and re-estimated the associations, as shown 
in Table . Overall, the results are consistent with those reported in the main analysis; in fact, the associations are 
slightly stronger when the LD/DD group is removed from the non-ASD group.
Discussion
The present study adds to a large and growing body of literature indicating that differences in early motor devel-
opment are an important feature of ASD. We found highly significant differences in postural control, reflected 
in differences in the rate of spontaneous movement of the head between toddlers with ASD versus those without 
ASD. Using an automated, objective approach, we analyzed data comprised of video-frame-level measurements 
of head movements with observation for each /th of a second and created -frame moving sums to capture 
movement. Time-series data revealed group differences in the rate of head movement across all movies repre-
senting a wide range of stimuli, such as bubbles, a hopping bunny, and a woman singing a nursery rhyme paired 
with dynamic toys. An increase in the rate of head movement observed in young children with ASD during 
states of engaged attention might indicate underlying differences in the ability to maintain midline postural con-
trol and/or atypical engagement of attentional systems in young toddlers with ASD. These movements were not 
defined by spontaneous looking away from the stimulus, as was reported by Martin et al.. Rather, they were 
characterized by a failure to keep the head in a still midline position while viewing the movie. This is distinct 
from the feature studied by Martin et al., which was characterized by greater yaw angular displacement and 
greater yaw and roll angular velocity, which was primarily present during the presentation of social stimuli and 
might reflect sensory modulation. The movements we describe in this paper may be similar to those described 
in previous studies of postural sway in older children with ASD, as well as school aged children with attention 
deficit hyperactivity disorder (ADHD) by Heiser et al.. Heiser et al. used infrared motion analysis to record 
head movements during a continuous performance task and found that boys with ADHD moved their head . 
times as far as typically-developing boys performing the same task. In a study of siblings of children with ASD, 
Reiersen and colleagues found that siblings who have impaired motor coordination, features of attention deficit 
hyperactivity disorder (ADHD), or both are much more likely to have ASD than are other siblings. They suggest 
/

SCiENtifiC REPOrtS |  : | DOI:./s---
that identification of nonspecific traits that can amplify risk for ASD, such as attention and motor differences, 
could allow for earlier identification and targeted therapy that modify these traits and potentially reduce later 
risk for ASD.
Delays and differences in sensorimotor development have been noted across the lifespan in individuals with 
ASD from early infancy through adulthood. For example, Lim et al. showed that postural sway and attention 
demands of postural control were larger in adults with ASD than in typically developed adults. Morris et al. 
found that adults with ASD did not use visual information to control standing posture, in contrast to adults 
without ASD. Brain imaging studies suggest that atypical motor function in autism may be related to increased 
Figure .  Time series of head movement rate, measured as the distance traveled per / seconds ( video 
frames), by ASD diagnosis. Solid lines are the median values at each time point. Bands represent the first and 
third quartiles at each time point. Blank sections represent name calls, which were removed from this analysis.
Table .  Unadjusted and adjusted rate ratios for the associations between diagnostic group and rate of head 
movement.
/

SCiENtifiC REPOrtS |  : | DOI:./s---
sensitivity to proprioceptive error and a decreased sensitivity to visual error, aspects of motor learning dependent 
on the cerebellum. Atypical presentation of motor functions of the cerebellum has been noted in children with 
ASD as young as  months of age. Esposito et al. identified significant differences in gait pattern, reflected in 
postural asymmetry, in toddlers with ASD as compared to those without ASD.
The sample of toddlers with ASD was recruited from primary pediatric care where children suspected of 
having autism were then evaluated using gold-standard diagnostic methods. Although this method of recruit-
ment increases the likelihood of obtaining a more representative population-based sample, it also results in a 
comparison group of toddlers without ASD that is much larger than the ASD sample. Because the sample of ASD 
toddlers in this study was relatively small, it will be important to replicate these findings with a larger group of 
children. A larger sample would also provide the statistical power to examine whether differences in postural 
control exist based on individual characteristics of children with ASD, such as age, sex, and co-morbid intellectual 
disability and/or ADHD.
Previous analyses of motor differences associated with ASD often have required labor-intensive coding of 
patterns of behavior that are recognizable by the naked eye. Moreover, such studies typically use a “top down” 
approach in which specific behaviors of interest are defined and then rated by more than one person (for relia-
bility assessments). The use of digital phenotyping offers multiple advantages over previous methods that rely on 
human coding, namely, the ability to automatically and objectively measure dynamic features of behavior on a 
spatiotemporal scale that is not easily perceptible to the naked eye. Because digital approaches are scalable, they 
also allow for collection of larger data sets that can be analyzed using machine learning. We anticipate that the 
use of digital phenotyping will reveal a number of objective biomarkers, such as the head movements described 
in this report, which can be used as early risk indices and targets for intervention. By combining multiple fea-
tures that reflect different aspects of sensorimotor function, including patterns of facial expressiomn, orienting, 
midline head movements, reaching behavior, and others, it might be possible to create a reliable, objective and 
automated risk profile for ASD and other neurodevelopmental disorders.
References
	 .	 Teitelbaum, P., Teitelbaum, O., Nye, J., Fryman, J. & Maurer, R. G. Movement analysis in infancy may be useful for early diagnosis of 
autism. Proc Natl Acad Sci USA , – .
	 .	 Esposito, G., Venuti, P., Maestro, S. & Muratori, F. An exploration of symmetry in early autism spectrum disorders: analysis of lying. 
Brain & development , –,  .
	 .	 Flanagan, J. E., Landa, R., Bhat, A. & Bauman, M. Head lag in infants at risk for autism: a preliminary study. The American journal of 
occupational therapy: official publication of the American Occupational Therapy Association , –, 
ajot.. .
	 .	 Zappella, M. et al. What do home videos tell us about early motor and socio-communicative behaviours in children with autistic 
features during the second year of life–An exploratory study. Early human development , –, 
earlhumdev... .
	 .	 Dawson, G., Osterling, J., Meltzoff, A. N. & Kuhl, P. Case Study of the Development of an Infant with Autism from Birth to Two Years 
of Age. Journal of applied developmental psychology , –, 
	 .	 Brisson, J., Warreyn, P
., Serres, J., Foussier, S. & Adrien-Louis, J. Motor anticipation failure in infants with autism: a retrospective analysis 
of feeding situations. Autism: the international journal of research and practice , –,  
.
	 .	 Gima, H. et al. Early motor signs of autism spectrum disorder in spontaneous position and movement of the head. Experimental 
brain research , –,  .
	 .	 Hytonen, M., Pyykko, I., Aalto, H. & Starck, J. Postural control and age. Acta oto-laryngologica , – .
	 .	 Ghanouni, P., Memari, A. H., Gharibzadeh, S., Eghlidi, J. & Moshayedi, P. Effect of Social Stimuli on Postural Responses in 
Individuals with Autism Spectrum Disorder. J Autism Dev Disord , –,  
.
	
.	 Minshew, N. J., Sung, K., Jones, B. L. & Furman, J. M. Underdevelopment of the postural control system in autism. Neurology , 
– .
	
.	 Gouleme, N. et al. Postural Control and Emotion in Children with Autism Spectrum Disorders. Translational neuroscience , 
–,  .
	
.	 Campbell, K. et al. Computer vision analysis captures atypical attention in toddlers with autism. Autism, , https://
doi.org/./ .
	
.	 Anzulewicz, A., Sobota, K. & Delafield-Butt, J. T. Toward the Autism MotorSignature: Gesture patterns during smart tablet gameplay 
identify children with autism. Scientific reports , ,  .
	
.	 Martin, K. B. et al. Objective measurement of head movement differences in children with and without autism spectrum disorder. 
Molecular autism , ,  .
	
.	 Wu, D., Jose, J. V., Nurnberger, J. I. & Torres, E. B. A Biomarker Characterizing Neurodevelopment with applications inAutism. 
Scientific reports , ,  .
Table .  Adjusted rate ratios for the associations between diagnostic group and rate of head movement after 
removing LD/DD participants.
/

SCiENtifiC REPOrtS |  : | DOI:./s---
	
.	 Gotham, K., Risi, S., Pickles, A. & Lord, C. The Autism Diagnostic Observation Schedule: revised algorithms for improved 
diagnostic validity. J Autism Dev Disord , –,  .
	
.	 Hashemi, J. et al. In Proceedings of the EAI International Conference on Wireless Mobile Communication and Healthcare. MobiHealth 
.
	
.	 De La Torre, F. IntraFace. Proceedings of the IEEE International Conference on Automatic Face and Gesture Recognition Workshops 
.
	
.	 Dementhon, D. D. L.D. Model-based object pose in  lines of code. International Journal of Computer Vision , – .
	
.	 Hashemi, J. et al. Computer vision tools for low-cost and noninvasive measurement of autism-related behaviors in infants. Autism 
Res Treat. , ,  .
	
.	 Hashemi, J. et al Computer vision analysis for quantification of autism risk behaviors. IEEE Transactions on Affective Computing, – 
.
	
.	 Heiser, P. et al. Objective measurement of hyperactivity, impulsivity, and inattention in children with hyperkinetic disorders before 
and after treatment with methylphenidate. European child & adolescent psychiatry , –, 
- .
	
.	 Reiersen, A. M., Constantino, J. N. & Todd, R. D. Co-occurrence of motor problems and autistic symptoms in attention-deficit/
hyperactivity disorder. J Am Acad Child Adolesc Psychiatry , –,  .
	
.	 Cook, J. L., Blakemore, S. J. & Press, C. Atypical basic movement kinematics in autism spectrum conditions. Brain: a journal of 
neurology , –,  .
	
.	 Lim, Y. H. et al. Effect of Visual Information on Postural Control in Adults with Autism Spectrum Disorder. J Autism Dev Disord, 
 .
	
.	 Morris, S. L. et al. Differences in the use of vision and proprioception for postural control in autism spectrum disorder. Neuroscience 
, –,  .
	
.	 Marko, M. K. et al. Behavioural and neural basis of anomalous motor learning in children with autism. Brain , –, https://
doi.org/./brain/awu .
	
.	 Esposito, G., Venuti, P., Apicella, F. & Muratori, F. Analysis of unsupported gait in toddlers with autism. Brain & development , 
–,  .
Acknowledgements
Funding for this work was provided by NICHD PHD, Duke Department of Psychiatry and Behavioral 
Sciences PRIDe award, Duke Education and Human Development Initiative, Duke-Coulter Translational 
Partnership Grant Program, National Science Foundation, and the Department of Defense. Some of the stimuli 
used for the movies were created by Geraldine Dawson, Michael Murias, and Sara Webb at the University of 
Washington. We gratefully acknowledge the editorial assistance of Elizabeth Sturdivant and the participation of 
the children and families in this study.
Author Contributions
G.D. and G.S. were responsible for conceptualizing and drafting the manuscript. All other authors reviewed and 
contributed to the manuscript. G.S. and J.H. were responsible for carrying-out the computer vision analyses. S.L. 
and V.S. were responsible for the statistical analyses. G.D., K.C., K.C. and S.V. were responsible for collection 
of the data and diagnostic confirmations. All authors were responsible for the design of the study and/or data 
analyses. K.C. and H.E. performed this work while employed at Duke University,
Additional Information
Supplementary information accompanies this paper at 
Competing Interests: Geraldine Dawson is on the Scientific Advisory Boards of Janssen Research and 
Development, Akili, Inc., LabCorps, and Roche Pharmaceutical Company, has received grant funding from 
Janssen Research and Development, L.L.C. and PerkinElmer, speaker fees from ViaCord, and receives royalties 
from Guilford Press and Oxford University Press. Geraldine Dawson and Guillermo Sapiro are affiliated with 
DASIO, LLC.
Publisher’s note: Springer Nature remains neutral with regard to jurisdictional claims in published maps and 
institutional affiliations.
Open Access This article is licensed under a Creative Commons Attribution . International 
License, which permits use, sharing, adaptation, distribution and reproduction in any medium or 
format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Cre-
ative Commons license, and indicate if changes were made. The images or other third party material in this 
article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the 
material. If material is not included in the article’s Creative Commons license and your intended use is not per-
mitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the 
copyright holder. To view a copy of this license, visit 
 
© The Author(s) 
sensors
Letter
Deep-Learning-Based Detection of Infants with
Autism Spectrum Disorder Using Auto-Encoder
Feature Representation
Jung Hyuk Lee , Geon Woo Lee , Guiyoung Bong , Hee Jeong Yoo  and Hong Kook Kim ,*

School of Electrical Engineering and Computer Science, Gwangju Institute of Science and Technology,
Gwangju , Korea;  (J.H.L.);  (G.W.L.)

Department of Psychiatry, Seoul National University Bundang Hospital, Seongnam-si,
Gyeonggi-do , Korea;  (G.B.);  (H.J.Y.)

Department of Psychiatry, College of Medicine, Seoul National University, Seoul , Korea
*
Correspondence: 
Received:  October ; Accepted:  November ; Published:  November 


Abstract: Autism spectrum disorder (ASD) is a developmental disorder with a life-span disability.
While diagnostic instruments have been developed and qualiﬁed based on the accuracy of the
discrimination of children with ASD from typical development (TD) children, the stability of such
procedures can be disrupted by limitations pertaining to time expenses and the subjectivity of
clinicians. Consequently, automated diagnostic methods have been developed for acquiring objective
measures of autism, and in various ﬁelds of research, vocal characteristics have not only been reported
as distinctive characteristics by clinicians, but have also shown promising performance in several
studies utilizing deep learning models based on the automated discrimination of children with
ASD from children with TD. However, diﬃculties still exist in terms of the characteristics of the
data, the complexity of the analysis, and the lack of arranged data caused by the low accessibility
for diagnosis and the need to secure anonymity. In order to address these issues, we introduce
a pre-trained feature extraction auto-encoder model and a joint optimization scheme, which can
achieve robustness for widely distributed and unreﬁned data using a deep-learning-based method
for the detection of autism that utilizes various models. By adopting this auto-encoder-based
feature extraction and joint optimization in the extended version of the Geneva minimalistic acoustic
parameter set (eGeMAPS) speech feature data set, we acquire improved performance in the detection
of ASD in infants compared to the raw data set.
Keywords: auto-encoder; bidirectional long short-term memory (BLSTM); joint optimization; acoustic
feature extraction; autism spectrum disorder
. Introduction
Autism spectrum disorder (ASD) is a developmental disorder with a high probability of causing
diﬃculties in social interactions with other people . According to the Diagnostic and Statistical
Manual of Mental Disorders, Fifth Edition , ASD involves several characteristics such as being
conﬁned to speciﬁc interests or behaviors, delayed linguistic development, and poor functionality in
terms of communicating or functioning in social situations . As there is wide variation in terms of
the types and severities of ASD based on its characteristics, the disorder is referred to as a spectrum .
Not only does ASD have the characteristics of a developmental disorder with a life-span disability, but
its prevalence is also increasing—from  in  children in  to  in  children in  . As diverse
evidence has been obtained from previous research showing that the chance of improvement in the
Sensors , , ; doi:./s

Sensors , , 
 of 
social abilities of people with ASD increases when an earlier clinical intervention is performed , the
early detection of ASD characteristics has become a key point of current ASD research.
Various instruments for discriminating ASD have been developed, and the commonly accepted
gold standard schemes are behavioral assessments, which are time-consuming procedures and require
multidisciplinary teams (MDTs). However, most behavioral assessments suﬀer in terms of the stability
of their ASD diagnosis as a result of the issues of accessibility or subjectivity and interpretive bias
between professions . Therefore, several attempts to develop objective and precise diagnostic
methods have been made in multiple ﬁelds, such as genetic determination , principle analysis of
brain images , and physiological approaches .
One prominent area of behavioral observations is that of infants’ vocal characteristics. Children
with ASD are known to have abnormalities in their prosody resulting from deﬁcits in their ability to
recognize the inherent mental conditions of others , and their atypical vocalizations are known to be
monotonous or exaggerated, which can be revealed using various acoustic characteristics, followed
by engineering approaches for the discrimination of ASD or typical development (TD) in children
based on the vocal and acoustic features. For example, in , the researchers estimated deﬁcits in the
vocalization of children with ASD at an average age of  months, such as “ﬂat” intonation, atypical
pitch, or control of volume based on the variability of pitch and the long-term average spectrum
(LTAS) using fast Fourier transform, where signiﬁcant diﬀerences were observed in the spectral
components at low-band frequencies, as well as spectral peaks and larger pitch ranges and standard
deviations. The development of linguistic abilities is also considered to be a distinguishable feature of
delayed development in children with ASD. Earlier vocal patterns at age – months were proven
to be diﬀerentiable in a study  that aimed to conﬁrm the hypothetical vocal patterns and social
quality of vocal behavior in order to diﬀerentiate between ASD and TD cohorts in groups of children
aged –, –, and – months in terms of categorized speech patterns consisting of vocalization,
long reduplicated babbling, two-syllable babbling, and ﬁrst words. Evidence of abnormalities in
children with ASD were shown, in these cases, as a signiﬁcant decrease in vocalization and ﬁrst word
rate, while the diﬀerence in babbling ability between children with ASD and TD was negligible.
Given the development and improvement of machine learning algorithms, as the achievement
in the performance of state-of-the-art classiﬁcation and discrimination tasks , recent attempts to
develop automated classiﬁcation methods based on machine learning techniques have been based
on the distinctiveness of vocal characteristics, and have been shown to be promising alternatives to
the conventional methods in many publications . For examples of machine learning classiﬁcation,
the researchers of  employed various acoustic–prosodic features, including fundamental frequency,
formant frequencies, harmonics, and root mean square signal energy. In their research, support vector
machines (SVMs) and probabilistic neural networks (PNNs) were adopted as classiﬁers, which showed
eﬀectual accuracy in discriminating children with ASD from children with TD. Meanwhile, the authors
of  employed more recent deep learning techniques, such as convolutional neural networks (CNNs)
and recurrent neural networks (RNNs) with spectral features from short-time Fourier transform (STFT)
and constant Q transform (CQT), to classify children diagnosed using the autism diagnostic observation
schedule (ADOS), also showing promising results in multiple outcomes from SVMs, RNNs, and a
combination of CNN and RNN classiﬁers.
A generalized acoustic feature set, an extended version of the Geneva minimalistic acoustic
parameter set (eGeMAPS) , and the bidirectional long short-term memory (BLSTM) model were
adopted to diﬀerentiate between children with ASD and children with TD in , showing that % of
the subjects’ utterances were correctly classiﬁed with the simple application of a deep learning model
and feature sets. While the quality of previous research based on various acoustic features has proven
the eﬀectiveness of acoustic features and classiﬁcation algorithms for the detection of abnormalities in
children’s voices in ASD group compared to those of TD group, the complexity and relationship being
inherent between the features will remain uncertain until a large amount of data can be accumulated.
Furthermore, a limitation still remains in terms of the problems regarding data collection, since there are
Sensors , , 
 of 
diﬃculties pertaining to the need to secure the anonymity of infant subjects, as well as the unintended
ignorance of parents at earlier stages of their infant’s development. The data of infants are, accordingly,
dispersed by gender, age, and number of vocalizations, or consist of comparably small volumes of
audio engineering data in general. These problems were typically overlooked by previous research
with controlled and small amounts of data.
In order to provide suggestions for a method to overcome the abovementioned restrictions, we focus
on examining the feasibility of neural networks as a feature extractor, employing an auto-encoder (AE),
which can modify acoustic features into lowered and separable feature dimensions . We construct a
simple six-layered stacked AE that contains an input layer, three fully connected (FC) layers, an output
layer, and one auxiliary output layer, which has categorical targets for ASD and TD for the optimization
of the latent feature space of the AE. We train the AE and deep learning models and compare the
results for each model based on SVMs and vanilla BLSTM, while adopting the same model parameters
from the method suggested in .
The remainder of this paper is organized as follows. Section  describes the speciﬁcations of the
participants’ data, data processing, feature extraction, statistical analysis, and experimental setup.
Section  presents the performance evaluations for each algorithm of the SVMs and vanilla BLSTM.
Lastly, Section  concludes the paper.
.. Data Collection and Acoustic Feature Extraction
This study was based on the audio data from video recordings of ASD diagnoses, which were
collected from  to  at Seoul National University Bundang Hospital (SNUBH). We received
approval from the Institutional Review Board (IRB) at SNUBH to use fully anonymized data for
retrospective analysis  from existing research .
We collected the audio data of  infants who were assessed using seven multiple instruments,
consisting of  ADOS, second edition ,  the autism diagnostic interview, revised (ADI-R),
 the behavior development screening for toddlers interview  the behavior development
screening for toddlers play  the Korean version of the childhood autism rating scale
 the social communication questionnaire  the social
responsiveness scale (SRS) . The ﬁnal diagnosis was based on the best clinical estimate diagnosis
according to the DSM- ASD criteria by a licensed child psychiatrist using all of the available participant
information. The participants’ ages ranged between  and  months, where the average age was
. months with a standard deviation (SD) of . months. Note here that the age means the age
at the time when each infant visited the hospital to undergo an initial diagnosis examination. There
were four males and six females diagnosed with ASD, whose average age was . months with a
SD of .. The remaining participants consisted of TD children ( males and  females). Table 
displays the collected data distribution, while Table  shows detailed information of collected data
from the infants.
Table . Distribution of age and gender (male/female).
Ages (Month)
No. of Subjects
Diagnosed as ASD
No. of Subjects
Diagnosed as TD
No. of Infant Subjects
– months

 M/ F
 M/ F
– months
 M/ F
 M/ F
 M/ F
– months
 M/ F

 M/ F
Age (average ± SD)
. ± .
Sensors , , 
 of 
Table . Detailed information on the age, gender, and initial and deﬁnite diagnosis dates of each infant
in Table .
Infant ID
Age (Months) on
Initial Diagnosis
Date
Gender
Initial Diagnosis
Date
(Year/Month/Day)
Deﬁnite Final
Diagnosis Date
(Year/Month/Day)
ASD/TD


Male
//
//
TD


Male
//
//
TD


Male
//
//
TD


Male
//
//
TD


Female
//
//
ASD


Male
//
//
TD


Female
//
//
TD


Female
//
//
TD


Male
//
//
TD


Male
//
//
TD


Female
//
//
ASD


Female
//
//
TD


Male
//
//
TD


Female
//
//
ASD


Male
//
//
TD


Female
//
//
ASD


Male
//
//
ASD


Male
//
//
ASD


Female
//
//
TD


Male
//
//
TD


Female
//
//
TD


Male
//
//
ASD


Male
//
//
ASD


Female
//
//
TD


Male
//
//
TD


Male
//
//
TD


Female
//
//
TD


Male
//
//
ASD


Male
//
//
TD


Male
//
//
TD


Male
//
//
TD


Male
//
//
TD


Male
//
//
TD


Female
//
//
TD


Female
//
//
TD


Male
//
//
TD


Male
//
//
ASD


Male
//
//
TD


Male
//
//
TD
As each infant’s audio data were recorded during the clinical procedure to elicit behaviors from
infants, with the attendance of one doctor or clinician and one or both parents with the child in the
clinical area, the audio components consisted of various speeches from the child, the clinician, and the
parent(s), as well as noises from toys or dragging chairs. Note here that the recordings were done in one
of two typical clinical rooms in SNUBH, where the room dimensions were  cm ×  cm ×  cm
and  cm ×  cm ×  cm, and the hospital noise level was around  dB. In order to analyze
the vocal characteristics of the infants, each audio clip was processed and split into audio segments
containing the infant’s voice, not disturbed by music or clattering noises from toys or overlapped
by the voices of the clinician or parent(s). Each segment was classiﬁed into one of ﬁve categories,
labeled from  to , for measuring the data distribution. Each label was intended to show diﬀerentiable
characteristics relative to the children’s linguistic development:   for one syllable, which is a short,
Sensors , , 
 of 
momentary single vocalization such as “ah” or “ba”;   for two syllables, commonly denoted as
canonical babbling, as a reduplication of clear babbling of two identical or variant syllables such as
“baba” or “baga”;   for babbling, not containing syllables;   for ﬁrst word, such as “mother” or
“father”; and   for atypical voice, including screaming or crying. The distribution of each type of
vocalization in seconds is shown in Table . The number of vocalizations per category is presented
along with a rational value considering the diﬀerence between the ASD and TD groups. While the
data were unbalanced and very small, the distribution of ASD and TD vocalizations show the same
tendency as reported in , where the ASD group showed a signiﬁcantly lower ratio of ﬁrst words
and an increased ratio of atypical vocalizations, revealing developmental delay in linguistic ability.
Table . Amount (ratio) of each type of vocalization in seconds.
Vocal Label
ASD
TD

. 

. 

. 

. 

. 
Total
.
For acquiring qualiﬁed and eﬀective feature sets for the vocal data, eGeMAPS was employed
for voice feature extraction. GeMAPS is a popular feature set providing minimalistic speech features
generally utilized for automatic voice analysis rather than as a large brute force parameter set. As an
extended version, eGeMAPS contains  acoustic features that were fully utilized in this experiment.
Each recorded set of audio data stored as a  kHz stereo ﬁle was down-sampled and down-mixed into a
 kHz mono-audio ﬁle, taking into consideration its usability and resolution in mel-frequency cepstral
coeﬃcients (MFCCs). To extract the speech features for ASD classiﬁcation, each infant’s utterances
were segmented into  ms frames with a  ms overlap between frames. Then,  diﬀerent features of
the eGeMAPS were extracted for each frame with open source speech and music interpretation using
the large-space extraction (OpenSMILE) toolkit , and these features were normalized by mean and
standard deviation. The normalization scaling was acquired and ﬁxed by normalizing the factors of
the training data set. The features were grouped for each ﬁve frames considering the time-relevant
characteristics of the speech data.
.. Pre-Trained AE for Acoustic Features
To further process and reﬁne the acoustic data, a feature-extracting AE was introduced. An AE is a
hierarchical structure that is trained as a regression model for reproducing the input parameters. The AE
takes inputs and converts them into latent representations, and then reconstructs the input parameters
from the latent values . If we consider an input of AE, x ∈Rd, then the latent representation z ∈Rd′
and the reconstruction of the input y ∈Rd are obtained by applying a nonlinear activation function
f to the weight sum of z using a weighting matrix W ∈Rd×d′ and a bias vector b ∈Rd′, such as
z = f

WTx + b


y = f

WTz + b′

where T is a matrix transpose operator. When the latent dimension d′ < d, the output from the latent
layer is considered to be a compressed, meaningful value extracted from the input, which is also noted
as a bottleneck feature .
The normalized eGeMAPS features were applied to train the feature-extracting AE, applying
the same data as the input and the target. The AE model contained a latent layer with a lowered,
compacted feature dimension compared to the input layer to achieve the useful bottleneck feature.
Sensors , , 
 of 
The model was symmetrically structured, centering around the latent layer, and the model could be
divided into two components: the encoder, consisting of layers from the input to the latent layers,
and a decoder, consisting of layers from the bottleneck to the output layers.
The AE structure is depicted in Figure . Our AE model consisted of FC layers, with the dimensions
of , , , , and  nodes for the input, hidden, latent, hidden, and output layers, respectively.
The hidden dimension was selected experimentally and the bottleneck feature dimension was used for
comparison with previous research , where  features were selected considering the statistical
dissimilarity of the distributions between the ASD and TD features based on the Mann–Whitney U
test . We additionally introduced an auxiliary output as the binary categorical target for ASD
and TD, which is known as the semi-supervised method, to train the AE model eﬀectively . The
auxiliary output is depicted as Aux in Figure . The reconstructed features and auxiliary classiﬁcation
can be written as
zi = f(Wi−,izi− + bi−,i)

where z = f, and
yrec = Wz + b

yaux = ∂(W,az + b,a)

where yrec refers to the reconstructed eGeMAPS features, yaux is the auxiliary classiﬁcation result, f is
the activation function, and ∂is the softmax activation.
 
layer is considered to be a compressed, meaningful value extracted from the input, which is also 
noted as a bottleneck feature . 
The normalized eGeMAPS features were applied to train the feature-extracting AE, applying the 
same data as the input and the target. The AE model contained a latent layer with a lowered, 
compacted feature dimension compared to the input layer to achieve the useful bottleneck feature. 
The model was symmetrically structured, centering around the latent layer, and the model could be 
divided into two components: the encoder, consisting of layers from the input to the latent layers, 
and a decoder, consisting of layers from the bottleneck to the output layers. 
The AE structure is depicted in Figure . Our AE model consisted of FC layers, with the 
dimensions of , , , , and  nodes for the input, hidden, latent, hidden, and output layers, 
respectively. The hidden dimension was selected experimentally and the bottleneck feature 
dimension was used for comparison with previous research , where  features were selected 
considering the statistical dissimilarity of the distributions between the ASD and TD features based 
on the Mann–Whitney U test . We additionally introduced an auxiliary output as the binary 
categorical target for ASD and TD, which is known as the semi-supervised method, to train the AE 
model effectively . The auxiliary output is depicted as Aux in Figure . The reconstructed features 
and auxiliary classification can be written as 
𝒛𝑖= 𝑓(𝑾𝑖−,𝑖𝒛𝑖− + 𝒃𝑖−,𝑖) 
 
where 𝒛 = 𝑓, and 
𝒚𝑟𝑒𝑐= 𝑾𝒛 + 𝒃 
 
  𝒚𝑎𝑢𝑥= 𝜕(𝑾,𝑎𝒛 + 𝒃,𝑎) 
 
where 𝒚𝑟𝑒𝑐 refers to the reconstructed eGeMAPS features, 𝒚𝑎𝑢𝑥 is the auxiliary classification result, 
𝑓 is the activation function, and 𝜕 is the softmax activation. 
 
Figure . Structure of a semi-supervised auto-encoder (AE) model. eGeMAPS, extended version of 
the Geneva minimalistic acoustic parameter set; ASD, autism spectrum disorder; TD, typical 
development. 
The losses of the reconstruction error for main AE target are measured using the mean absolute 
error, while the auxiliary ASD/TD target loss is the binary cross-entropy, and they are added and 
simultaneously optimized with rational hyper-parameters. The overall loss equation is 
Figure . Structure of a semi-supervised auto-encoder (AE) model. eGeMAPS, extended version of the
Geneva minimalistic acoustic parameter set; ASD, autism spectrum disorder; TD, typical development.
The losses of the reconstruction error for main AE target are measured using the mean absolute
error, while the auxiliary ASD/TD target loss is the binary cross-entropy, and they are added and
simultaneously optimized with rational hyper-parameters. The overall loss equation is
Lrecon = 
N
N
X
i=


yirec −yigt




Laux = −ygt log(yaux) −( −t)ygt log(yaux)

Ltotal = Lrecon + αLaux

where Lrecon, Laux, and Ltotal denote the reconstruction error, auxiliary loss using a binary cross-entropy
loss function, and total loss, respectively.
Sensors , , 
 of 
For our stacked AE model, a rational value of α = . was selected experimentally, considering
the proportion of each loss. In order to train the AE eﬀectively, both L normalization for weight
normalization and batch normalization were adopted . After the training was completed,
we fetched the encoder of the AE as the feature extraction part for the joint optimization model in the
training procedures of the deep learning model.
.. Establishing and Training the Deep Learning Model for ASD Detection
As the eGeMAPS data were set and the AE was trained through semi-supervised learning,
the machine learning models, such as SVMs, BLSTM, and joint optimized BLSTM were constructed.
Each model had its own input parameter dimensions and the same output targets as ASD and TD
classiﬁcation labels. The eGeMAPS feature data were paired with the diagnostic results for the
supervised learning of the neural network models. For the binary decision, ASD was labeled as
a positive data point, with a label of , while TD was labeled as a negative data point .
We composed four kinds of models with the paired data: SVMs with linear kernel, the vanilla BLSTM
with  eGeMAPS features, the vanilla BLSTM with  eGeMAPS features, and the jointly optimized
BLSTM layer with the AE. The joint optimization model is depicted in Figure . As the data set was
prepared as the input with ﬁve sequential frames, i.e., the grouped eGeMAPS features in Figure ,
the SVMs received a single frame parameter of  dimension which was ﬂattened from the original
ﬁve input frames. For the deep learning models, batch normalization, rectangular linear unit (ReLU)
activation, and dropout were applied for each layer, except for the output layer , and the
adaptive momentum (ADAM) optimizer  was used to train the network. The training procedure
was controlled by early stopping for minimizing the validation error with  epoch patience, while
saving the best models for improvement of the validation loss by each epoch. Because the amount
of speech data was relatively small for a deep learning model compared to the disparate ﬁeld of
audio engineering, we grouped the data into ﬁve segments, while the test utterances were separated
formerly, which were selected randomly for % of the total data, were evenly distributed across each
vocalization type, and underwent ﬁve-fold cross-validation for training; then, the best-performing
model was chosen. Our model was trained with the TensorFlow framework . For comparison,
an SVM model with linear kernel was trained with the same data split as the proposed deep learning
model, and as well as the vanilla BLSTM suggested in , which has single BLSTM with eight cells.
Sensors , , x FOR PEER REVIEW 
 of  
 
Figure . Structure of a joint optimization model of an auto-encoder (AE) and bidirectional long short-
term memory (BLSTM). 
. Performance Evaluation 
The performance of each method was evaluated through five-fold cross validation, where  
average ASD utterances and  average TD utterances were proportionally distributed over five 
cases of vocalizations for the generalized estimation of unconcentrated utterance data. The averaged 
performances of the five validation splits of each model are described in Table . The labeled names 
of the BLSTM were used as the features for training the BLSTM model, where eGeMAPS- denotes 
 features of eGeMAPS, eGeMAPS- denotes  features selected by the Mann–Whitney U test, and 
AE-encoded denotes the joint optimized model. In the classification stage, one utterance was 
d i
th f
i
th d
d th
ft
t
t
t d t
l
i di

d 
Figure . Structure of a joint optimization model of an auto-encoder (AE) and bidirectional long
short-term memory (BLSTM).
. Performance Evaluation
The performance of each method was evaluated through ﬁve-fold cross validation, where
 average ASD utterances and  average TD utterances were proportionally distributed over ﬁve
Sensors , , 
 of 
cases of vocalizations for the generalized estimation of unconcentrated utterance data. The averaged
performances of the ﬁve validation splits of each model are described in Table . The labeled names
of the BLSTM were used as the features for training the BLSTM model, where eGeMAPS- denotes
 features of eGeMAPS, eGeMAPS- denotes  features selected by the Mann–Whitney U test,
and AE-encoded denotes the joint optimized model. In the classiﬁcation stage, one utterance was
processed in the frame-wise method and the softmax output was converted to class indices  and , and
if the average of class indices of the frames was over ., then the utterance was considered an ASD
child’s utterance. The performances were scored with conventional measures, as well as unweighted
average recall (UAR) and weighted average recall (WAR), chosen in the INTERSPEECH  Emotion
challenge, which considered imbalanced classes . In the experiment, the SVM model showed very
low precision, which was extremely biased toward the TD class. The BLSTM classiﬁer with  features
of eGeMAPS and the AE model showed considerable quality in terms of classifying ASD and TD
children, while the AE model showed only marginal improvement in correctly classifying children
with ASD compared to eGeMAPS-. The  selected features showed degraded quality compared to
eGeMAPS-, obtaining more biased results toward children with TD.
Table . Classiﬁcation results from the support vector machine (SVM), BLSTM with  or  eGeMAPS
features,  selected eGeMAPS features, and BLSTM with AE-encoded features.
Models
SVM
BLSTM

BLSTM

BLSTM
(AE-Encoded)
Predicted To
ASD
TD
ASD
TD
ASD
TD
ASD
TD
ASD








TD








Accuracy
.
Precision
.
Recall
.
F score
.
UAR
.
UAR, unweighted average recall.
. Discussion
The vanilla BLSTM model presented in  conducted discrimination on well-classiﬁed subjects
with -month-old children and sorted  features from eGeMAPS that had a distinctive distribution
between ASD and TD selected by the Mann–Whitney U test using the three-fold cross-validation
method. However, because the diﬀerence in the data distribution failed to achieve the same eGeMAPS
feature selection between the test and classiﬁcation results with the speciﬁed feature set presented
herein, the application of an identical model structure and the adoption of the same feature domain
will allow both approaches to be indirectly comparable.
These results can be interpreted by the data distributions, and we performed t-stochastic neighbor
embedding (t-SNE) analysis  on the training data set, which can nonlinearly squeeze the data
dimension based on a machine learning algorithm. Figure  shows each data distribution as a
two-dimensional scatter plot. In the ﬁgure, the eGeMAPS features from eGeMAPS- and eGeMAPS-
showed almost identical distribution, except for the amount of ASD outliers, which implies that the
ASD and TD features in the eGeMAPS features show similar distributions in this experiment. As shown
in , eGeMAPS includes temporal features that are relevant to vocalizations and utterances; thus, these
features might cause confusion regarding the discrimination between ASD and TD. The AE-encoded
features, however, showed a redistributed feature map with a more characteristic distribution compared
to the eGeMAPS features. This is because the AE-encoded features were compressed into a bottleneck
feature, which was derived by weighting the matrix, paying attention to the signiﬁcant parameters
Sensors , , 
 of 
while reducing the inﬂuence from the ambiguous parameters. While the joint optimization model
achieved only marginally improved results compared to eGeMAPS-, the distribution of the feature
map would be more noticeable in improved feature extraction models, as well as more diﬀerentiable in
complex models, although BLSTM with eight cells was employed for a comparison with conventional
research in this experiment.
 
distribution compared to the eGeMAPS features. This is because the AE-encoded features were 
compressed into a bottleneck feature, which was derived by weighting the matrix, paying attention 
to the significant parameters while reducing the influence from the ambiguous parameters. While the 
joint optimization model achieved only marginally improved results compared to eGeMAPS-, the 
distribution of the feature map would be more noticeable in improved feature extraction models, as 
well as more differentiable in complex models, although BLSTM with eight cells was employed for a 
comparison with conventional research in this experiment. 
While the overall performance scores were comparably low for general classification problems 
on account of the subjectivity and complexity of problems, and the limitation in terms of the shortage 
of data, the results of the jointly optimized model imply the possibility of deep-learning-based feature 
extraction for the improvement of automated ASD/TD diagnosis under restricted circumstances. 
 
 
 
(a) 
(b) 
(c) 
Figure . Two-dimensional scatter plot for (a) eGeMAPS-, (b) eGeMAPS-, and (c) the AE 
processed by t-stochastic neighbor embedding (t-SNE). 
. Conclusions 
In this paper, we conducted experiments for discovering the possibility of auto-encoder-based 
feature extraction and a joint optimization method for the automated detection of atypicality in voices 
of children with ASD during early developmental stages. Under the condition of an insufficient and 
dispersed data set, the classification results were relatively poor in comparison to the general 
classification tasks based on deep learning. Although our investigation used a limited number of 
subjects and an unbalanced data set, the suggested auto-encoder-based feature extraction and joint 
optimization method revealed the possibility of feature dimension and a slight improvement in 
model-based diagnosis under such uncertain circumstances. 
Figure . Two-dimensional scatter plot for (a) eGeMAPS-, (b) eGeMAPS-, and (c) the AE processed
by t-stochastic neighbor embedding (t-SNE).
While the overall performance scores were comparably low for general classiﬁcation problems on
account of the subjectivity and complexity of problems, and the limitation in terms of the shortage of
data, the results of the jointly optimized model imply the possibility of deep-learning-based feature
extraction for the improvement of automated ASD/TD diagnosis under restricted circumstances.
. Conclusions
In this paper, we conducted experiments for discovering the possibility of auto-encoder-based
feature extraction and a joint optimization method for the automated detection of atypicality in voices
of children with ASD during early developmental stages. Under the condition of an insuﬃcient
and dispersed data set, the classiﬁcation results were relatively poor in comparison to the general
classiﬁcation tasks based on deep learning. Although our investigation used a limited number of
subjects and an unbalanced data set, the suggested auto-encoder-based feature extraction and joint
optimization method revealed the possibility of feature dimension and a slight improvement in
model-based diagnosis under such uncertain circumstances.
In future work, we will focus on increasing the reliability of the proposed method by addition
of a number of infants’ speech data, reﬁnement of the acoustic features, an auto-encoder for feature
extraction, and better, deeper, and up-to-date model structures. This research can also be extended to
children with the age of  or  who can speak several sentences. In this case, we will investigate the
linguistic features, as well as acoustic features, such as we have done in this paper. In addition to ASD
detection, this research can be applied to the detection of infants with development delays.
Author Contributions: All authors discussed the contents of the manuscript. H.K.K. contributed to the research
idea and the framework of this study; G.B. and H.J.Y. provided the database and helped with the discussion; J.H.L.
performed the experiments; G.W.L. contributed to the data collection and pre-processing. All authors have read
and agreed to the published version of the manuscript.
Funding: This work was supported by the Institute of Information & communications Technology Planning &
evaluation (IITP) grant funded by the Korea government (MSIT) (No. --, Development of AI Technology
for Early Screening of Infant/Child Autism Spectrum Disorders based on Cognition of the Psychological Behavior
and Response).
Conﬂicts of Interest: The authors declare no conﬂict of interest.
Sensors , , 
 of 
References
.
National Institute of Mental Health. Autism Spectrum Disorder. Available online: 
gov/health/topics/autism-spectrum-disorders-asd/index.shtml .
.
American Psychiatric Association. Diagnostic and Statistical Manual of Mental Disorders: DSM-; American
Psychiatric Publishing: Washington, DC, USA, .
.
Centers for Disease Control and Prevention (CDC). Data & Statistics on Autism Spectrum Disorder. Available
online:  .
.
Fenske, E.C.; Zalenski, S.; Krantz, P.J.; McClannahan, L.E. Age at intervention and treatment outcome for
autistic children in a comprehensive intervention program. Anal. Interv. Devel. Disabil. , , –.
[CrossRef]
.
Falkmer, T.; Anderson, K.; Falkmer, M.; Horlin, C. Diagnostic procedures in autism spectrum disorders:
A systematic literature review. Eur. Child Adolesc. Psychiatry , , –. [CrossRef] [PubMed]
.
Bailey, A.; Le Couteur, A.; Gottesman, I.; Bolton, P.; Simonoﬀ, E.; Yuzda, E.; Rutter, M. Autism as a strongly
genetic disorder: Evidence from a British twin study. Physiol. Med. , , –. [CrossRef] [PubMed]
.
Duﬀy, F.H.; Als, H. A stable pattern of EEG spectral coherence distinguishes children with autism from
neuro-typical controls—A large case control study. BMC Med. , , . [CrossRef] [PubMed]
.
Chaspari, T.; Lee, C.-C.; Narayanan, S.S. Interplay between verbal response latency and physiology of
children with autism during ECA interactions. In Proceedings of the Annual Conference of the International
Speech Communication Association (Interspeech), Portland, OR, USA, – September ; pp. –.
.
Baron-Cohen, S. Social and pragmatic deﬁcits in autism: Cognitive or aﬀective? J. Autism Dev. Disord. ,
, –. [CrossRef] [PubMed]
.
Bonneh, Y.S.; Levanon, Y.; Dean-Pardo, O.; Lossos, L.; Adini, Y. Abnormal speech spectrum and increased
pitch variability in young autistic children. Front. Hum. Neurosci. , , . [CrossRef] [PubMed]
.
Chericoni, N.; de Brito Wanderley, D.; Costanzo, V.; Diniz-Gonçalves, A.; Gille, M.L.; Parlato, E.; Cohen, D.;
Apicella, F.; Calderoni, S.; Muratori, F. Pre-linguistic vocal trajectories at – months of age as early markers
of autism. Front. Psychol. , , . [CrossRef] [PubMed]
.
Alom, M.Z.; Taha, T.M.; Yakopcic, C.; Westberg, S.; Sidike, P.; Nasrin, M.S.; Hasan, M.; van Essen, B.C.;
Awwal, A.A.S.; Asari, V.K. A state-of-the-art survey on deep learning theory and architectures. Electronics
, , . [CrossRef]
.
Song, D.-Y.; Kim, S.Y.; Bong, G.; Kim, J.M.; Yoo, H.J. The use of artiﬁcial intelligence in screening and
diagnosis of autism spectrum disorder: A literature review. J. Korean Acad. Child. Adolesc. Psychiatry , ,
–. [CrossRef] [PubMed]
.
Santos, J.F.; Brosh, N.; Falk, T.H.; Zwaigenbaum, L.; Bryson, S.E.; Roberts, W.; Smith, I.M.; Szatmari, P.;
Brian, J.A. Very early detection of autism spectrum disorders based on acoustic analysis of pre-verbal
vocalizations of -month old toddlers. In Proceedings of the IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), Vancouver, BC, Canada, – May ; pp. –.
.
Li, M.; Tang, D.; Zeng, J.; Zhou, T.; Zhu, H.; Chen, B.; Zou, X. An automated assessment framework for
atypical prosody and stereotyped idiosyncratic phrases related to autism spectrum disorder. Comput. Speech
Lang. , , –. [CrossRef]
.
Eyben, F.; Scherer, K.R.; Schuller, B.W.; Sundberg, J.; André, E.; Busso, C.; Devillers, L.Y.; Epps, J.; Laukka, P.;
Narayanan, S.S.; et al. The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and
aﬀective computing. IEEE Trans. Aﬀect. Comput. , , –. [CrossRef]
.
Pokorny, F.B.; Schuller, B.W.; Marschik, P.B.; Brueckner, R.; Nyström, P.; Cummins, N.; Bölte, S.;
Einspieler, C.; Falck-Ytter, T. Earlier identiﬁcation of children with autism spectrum disorder: An automatic
vocalisation-based approach.
In Proceedings of the Annual Conference of the International Speech
Communication Association (Interspeech), Stockholm, Sweden, – August ; pp. –.
.
Xing, C.; Ma, L.; Yang, X. Stacked denoise autoencoder based feature extraction and classiﬁcation for
hyperspectral images. J. Sens. , , . [CrossRef]
.
Bong, G.; Kim, J.; Hong, Y.; Yoon, N.; Sunwoo, H.; Jang, J.; Oh, M.; Lee, K.; Jung, S.; Yoo, H. The feasibility and
validity of autism spectrum disorder screening instrument: Behavior development screening for toddlers
(BeDevel)—A pilot study. Autism Res. , , –. [CrossRef] [PubMed]
Sensors , , 
 of 
.
Center for Autism Research. Social Communication Questionnaire (SCQ). Available online: 
carautismroadmap.org/social-communication-questionnaire-scq/?print=pdf .
.
Center for Autism Research. Childhood Autism Rating Scale, nd Edition . Available online: https:
// .
.
Center for Autism Research. Social Responsiveness Scale, nd Edition . Available online: https:
// .
.
Eyben, F.; Wöllmer, M.; Schuller, B. OpenSMILE—The Munich versatile and fast open-source audio feature
extractor. In Proceedings of the th ACM International Conference on Multimedia, Firenze, Italy, –
October ; pp. –.
.
Masci, J.; Meier, U.; Cire¸
san, D.; Schmidhuber, J. Stacked Convolutional Auto-Encoders for Hierarchical
Feature Extraction. In Artiﬁcial Neural Networks and Machine-ICANN ; Honkela, T., Duch, W., Girolami, M.,
Kaski, S., Eds.; Springer: Berlin/Heidelberg, Germany, ; pp. –.
.
Sainath, T.; Kingsbury, B.; Ramabhadran, B. Auto-encoder bottleneck features using deep belief networks. In
Proceedings of the  IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
Kyoto, Japan, – March .
.
Nachar, N. The Mann-Whitney U: A test for assessing whether two independent samples come from the
same distribution. Tutor. Quant. Methods Psychol. , , –. [CrossRef]
.
Le, L.; Patterson, A.; White, M. Supervised autoencoders: Improving generalization performance with
unsupervised regularizers. In Advances in Neural Information Processing Systems; Bengio, S., Wallach, H.,
Larochelle, H., Grauman, K., Cesa-Bianchi, N., Garnett, R., Eds.; Curran Associates, Inc.: New York, NY,
USA, ; pp. –.
.
van Laarhoven, T. L regularization versus batch and weight normalization. arXiv , arXiv:..
.
Ioﬀe, S.; Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate
shift. In Proceedings of the International Conference on Machine Learning, Lille, France, – July ;
pp. –.
.
Nair, V.; Hinton, G.E. Rectiﬁed linear units improve restricted Boltzmann machines. In Proceedings of the
th International Conference on Machine Learning, Haifa, Israel, – June ; pp. –.
.
Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; Salakhutdinov, R. Dropout: A simple way to prevent
neural networks from overﬁtting. J. Mach. Learn. Res. , , –.
.
Kingma, D.P.; Ba, J.L. ADAM: A method for stochastic optimization. In Proceedings of the rd International
Conference on Learning Representations, San Diego, CA, USA, – May ; pp. –.
.
Abadi, M.; Barham, P.; Chen, J.; Chen, Z.; Davis, A.; Dean, J.; Devin, M.; Ghemawat, S.; Irving, G.; Isard, M.;
et al. TensorFlow: A system for large-scale machine learning. In Proceedings of the th USENIX Symposium
on Operating Systems Design and Implementation, Savannah, GA, USA, – November ; pp. –.
.
Schuller, B.; Steidl, S.; Batliner, A. The Interspeech  emotion challenge. In Proceedings of the Annual
Conference of the International Speech Communication Association (Interspeech), Brighton, UK, –
September ; pp. –.
.
van der Maaten, L.; Hinton, G. Visualizing data using t-SNE. J. Mach. Learn. Res. , , –.
Publisher’s Note: MDPI stays neutral with regard to jurisdictional claims in published maps and institutional
aﬃliations.
©  by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access
article distributed under the terms and conditions of the Creative Commons Attribution
(CC BY) license (
diagnosed until preschool or kindergarten age . Research targeting early detection has
primarily focused on behaviors exhibited during toddlerhood (– months) and preschool
years (– months) (e.g., Matson, Fodstad, & Dempsey, ; Volkmar & Chawarska,
) after diagnosis has been made. Use of retrospective video analyses and studies of
infant siblings of children diagnosed with ASD has allowed examination of possible
indicators of ASD in the first year of life (e.g., Baranek, ; Osterling, Dawson, &
Munson, ; Sheinkopf, Iverson, Rinaldi, & Lester, ; Zwaigenbaum et al., ).
Still, the most widely used autism screening tool for young children, the Modified Checklist
for Autism Toddlers  is recommended for
ages – months.
We sought to identify potential communication markers of ASD that might be observed
within the first year of life in a retrospective evaluation of data from infants recorded at
home and later diagnosed with ASD. We focused on presumed precursors to language for
two reasons: First, communication impairment is a core deficit in ASD, and second,
evaluation of very early vocal behaviors in typically developing infants has already
established markers that are critical to normal vocal communicative development. One
robust pre-speech vocal milestone is the onset of canonical babbling. A canonical syllable
(e.g., [ba]) is comprised of a consonant-like sound and a vowel-like sound, with a rapid
transition between them . A second potentially important vocal measure
that we considered is volubility, the rate of infant vocalization independent of vocal type
.
Canonical babbling as a key milestone
In typical development, infants from birth produce vegetative vocalizations (e.g., coughs,
burps, etc.) and cry, as well as vowel-like sounds that become more elaborate with time,
incorporating supraglottal articulations until canonical syllables emerge, usually by early in
the second half-year of life. Robust onset of canonical babbling has been well documented
in typically developing infants by not later than  months (Koopmans-van Beinum & van
der Stelt, ; Oller, ; Stark, ). The impression of robustness has been reinforced
by the fact that no delay in onset of canonical babbling has been discerned in infants
anticipated to be at-risk for communication deficits due to premature birth or low
socioeconomic status .
Even infants with Down syndrome usually show normal ages of onset, although a group
level delay of a month or more is detectable . Furthermore, infants
tracheostomized at birth to provide an artificial airway that prevents or substantially inhibits
vocalization for many months tend to produce age-appropriate canonical syllables within a
short period after decannulation (Bleile, Stark, & McGowan, ; Locke & Pearson, ;
Ross, ; Simon, Fowler, & Handler, ).
Only profound hearing impairment and Williams syndrome have been shown to produce
consistent substantial delays in the onset of canonical babbling (Kent, Osberger, Netsell, &
Hustedde, ; Koopmans-van Beinum, Clement, & van den Dikkenberg-Pot, ;
Masataka, ; Oller & Eilers, ; Stoel-Gammon & Otomo, ). Further supporting
the idea that restricted hearing prevents experiences critical to onset of canonical babbling,
Patten et al.
Page 
J Autism Dev Disord. Author manuscript; available in PMC  October .
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
age of onset in severely or profoundly hearing impaired infants has been reported to be
positively correlated with age of amplification .
In infants without known disorders, onset of canonical babbling after ten months has been
shown to be a significant predictor of language delay or other developmental disabilities
.
But late onset of canonical babbling is a rare occurrence in infants without easily diagnosed
physical or mental limitations. The seeming resistance to derailment of this developmental
milestone suggests that canonical babbling is of such importance in human development that
it has been evolved to emerge within a relatively tightly constrained time period in spite of
substantial variations in home environments and perinatal events. The importance of
canonical babbling in predicting later language functioning is assumed to be due to the fact
that words are overwhelmingly composed of canonical syllables, and thus lexical learning
depends on control of canonical syllables.
To date, only two studies of which we are aware have targeted canonical babbling in ASD
and neither specifically examined the onset of canonical babbling, But reasons for optimism
that delays in onset of canonical babbling could constitute an early ASD marker can be
found in research showing that various aspects of vocalization appear to be disrupted in
young children with ASD (Paul, Augustyn, Klin, & Volkmar, ; Peppe, McCann,
Gibbon, O’Hara, & Rutherford, ; Sheinkopf, Mundy, Oller, & Steffens, ; Warren,
Gilkerson, Richards, & Oller, ; Wetherby et al., ). Research using automated
analysis of all-day recordings based on the automated LENA (Language ENvironment
Analysis) system of classification has shown clear indications that young children with ASD
(– months) display low rates of canonical syllable production compared with typically
developing infants, even after matching of subgroups for expressive language (Oller et al.,
). Even more to the point, one recent study has assessed the usage of canonical syllables
(though not the onset of canonical babbling) in infants at high-risk for ASD because they
were siblings of children with ASD; seven of  participants in the study received a
provisional diagnosis of ASD at  months (Paul, Fuerst, Ramsay, Chawarska, & Klin,
). As a group, the at-risk infants  produced significantly lower mean canonical
babbling ratios (canonical syllables divided by all “speech-like” vocalizations, i.e., those
deemed “transcribable” by the researchers) compared to low-risk infants at nine-months of
age, but there were no significant differences at  months. “Non-speech” vocalizations
(those deemed “not transcribable” e.g., yells, squeals, growls) were not included in the
evaluation of canonical babbling. Other vocal measures—especially number of consonant-
like elements and number of speech-like and proportion of non-speech-like vocalizations—
also appeared to be potentially useful indicators of emergent ASD.
Volubility in ASD
Volubility, or rate of vocalization, measured in terms of frequency of syllable or utterance
production, may be limited in ASD, a possibility that is supported by automated analysis of
data showing low volubility in ASD from all-day recordings on children from  to 
months of age based on the LENA system . Volubility in infants with
severe or profound hearing loss and in infants with Down syndrome has not been found to
Patten et al.
Page 
J Autism Dev Disord. Author manuscript; available in PMC  October .
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
be depressed compared with typically developing infants; however, infants from lower
socio-economic status (SES) have been shown consistently to produce fewer utterances per
minute than their middle or high SES peers . Research
suggests that children in low SES experience less communication from caregivers (Hart &
Risley, ; Snow, ). The lower volubility of these infants may be a product of
decreased social-communication from adults, potentially resulting in lower levels of social
motivation in the infants.
Variability in moment-to-moment parental interactivity clearly does affect infant volubility
by the middle of the first year of life, as indicated by research on parent-infant interaction in
the “still-face” paradigm. The work suggests a strong tendency in the particular case of
parent still-face for infants to increase vocalization rate. Specifically, volubility during a
baseline period of one to three minutes of face-to-face vocal interaction is substantially
lower than during a following still-face period of one to three minutes where the parent
withholds any facial or vocal reaction while continuing to look directly at the infant. This
pattern is seen in infants after  months, but not at  months, where volubility does not
change at the shift from face-to-face interaction to still-face (Delgado, Messinger, & Yale,
; Goldstein, Schwade, & Bornstein, ; Yale, Messinger, Cobo-Lewis, Oller, &
Eilers, ). The results from the still-face paradigm are interpreted to mean that infants
seek to re-engage the withdrawn parent during the still-face period, having learned by the
middle of the first year that their vocalizations can have impact . This effect
raises the question of whether infants with emergent ASD similarly increase their volubility
to re-engage their caregivers after a period of withdrawn caregiver attention, or whether they
decrease volubility, possibly due to diminished motivation to engage socially with others.
Frequency of vocalizations directed at others has been reported to be significantly lower in
infants later diagnosed with ASD compared to typically developing infants at  months but
not at  months . It is also notable that
frequency of vocalization based on parent report is predictive of language abilities in
toddlers with ASD . Paul et al.  assessed frequency of
vocalization in infants at high-risk and low-risk for developing ASD and found no difference
between groups. However, the study did not actually test for volubility the way volubility is
defined here and in much prior research. Frequency of vocalization was tallied in a special
way in the Paul et al. study, by counting all speech-like (phonetically transcribable) and
nonspeech-like (not phonetically transcribable) vocalizations that occurred within the first
 speech-like vocalizations of each recorded sample. But not all participants produced 
speech-like utterances, and in the ones who did, the length of recording required to reach the
 speech-like utterance criterion was variable. Thus, rate of vocalizations per unit of time
was not examined in this study; consequently, given the common usage of the term
volubility, it is not possible to determine whether there was a difference in volubility
between the groups. In addition, participants in this study were at high-risk for ASD—some
were later diagnosed with ASD while some were not. This mixture may have attenuated
group differences. It should also be noted that Weismer et al. included only child
vocalizations directed at others while Paul et al. included vocalizations. Although ASD has
roots in social impairments, vocalizations directed at others as well as independent vocal
play might well be abnormal in ASD.
Patten et al.
Page 
J Autism Dev Disord. Author manuscript; available in PMC  October .
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
A new study of early vocal development in ASD
One reason the development of pre-speech vocal behaviors in ASD has not been well
documented may be that ASD is not reliably diagnosed until long after canonical syllables
are expected to emerge, thus making prospective analyses challenging. Retrospective
interviews with parents whose children have been diagnosed with ASD regarding age at
which canonical syllables emerged may be hindered by poor parent recall, given that parents
are generally asked to remember the nature of child babbling that occurred one or more
years prior to the time of the interview; also, parents’ awareness of the diagnosis may bias
their recall of the onset of canonical babbling. The effort by Paul et al.  cited above
represents a key advancement in methodology because they assessed infants known to be at-
risk in a prospective fashion. Our approach seizes an additional opportunity afforded by the
fortuitous existence of home video data from the first year of life that can be analyzed after
diagnosis of ASD for comparison with similar video data from infants who did not receive
the diagnosis.
As indicated in studies cited above, emergence of canonical syllables is a critical milestone
in the development of spoken language, and delayed onset has been shown to be predictive
of significant communication impairment. Canonical babbling and volubility have not been
well characterized in infants with ASD. To arrive at a better understanding of these two
variables as potential indicators of ASD risk in infants, we investigated vocalizations of
infants later diagnosed with ASD and typically developing (TD) infants at two age ranges,
– months and – months, using retrospective video analysis methods. Previous
research has suggested that nearly all TD infants reach the canonical babbling stage by –
months , and on the assumption that a delay might be present in the
children later diagnosed with ASD, we predicted such delay would be observed in this age
range. We took the opportunity also to evaluate the available data at – months because
any infant with a failure to show canonical babbling at that age would be greatly delayed in
canonical babbling onset and would be considered at very high risk for a variety of
disorders.
The coding scheme for this study is based on a widely applied method for laboratory-based
evaluation of canonical babbling . In accord with this method, infants are
assumed to be in the canonical stage if they show a canonical babbling ratio (canonical
syllables divided by all syllables) of at least ., a value based on coding by trained listeners
of a recording. A value of . or greater from such laboratory coding has been empirically
determined in prior research as corresponding to parent judgments that infants are in the
canonical stage . It has been reasoned that parent judgments constitute the
most appropriate standard for establishing this criterion value . This reasoning
is based on three points: ) Parents respond to interview questions by providing very
consistent and accurate information about canonical babbling in their infants (Papoušek,
; Oller, Eilers, & Basinger, ); ) this parental capability is predictable, given that
recognizing canonical babbling represents nothing more than being able to recognize
syllables as being well-formed enough that they could form parts of words in real speech
(and of course normal adults can easily recognize vocalizations of humans as speech or non-
speech); and ) parents appear to intuitively understand that the onset of canonical babbling
Patten et al.
Page 
J Autism Dev Disord. Author manuscript; available in PMC  October .
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
is an emergent foundation for speech, as evidenced by the fact that they initiate intuitive
lexical teaching as soon as they begin to recognize canonical babbling in their infants
. Consistent parent recognition of the onset of canonical babbling runs in
parallel with recognition of other developmental milestones (e.g., sitting unsupported,
crawling, walking). In our study we could not use parents as informants about the age of
onset of canonical babbling since that onset had occurred a very long time before our first
contact with them. Consequently, the canonical babbling ratio, determined from recordings
coded in our laboratory, provided the best available measure upon which to base inference
about whether infants had reached the canonical stage.
In the present study the following hypotheses were tested:
.
Infants later diagnosed with ASD will be less likely than TD infants to be in the
canonical stage at each age (– and – months), as determined by whether
their canonical babbling ratios exceed the . criterion.
.
Infants later diagnosed with ASD will demonstrate significantly lower canonical
babbling ratios (independent of the canonical stage criterion) compared to TD
infants.
.
Infants later diagnosed with ASD will demonstrate significantly fewer total
vocalizations (lower volubility) at both age ranges compared to TD infants.
.
A combined analysis using both volubility and canonical babbling status will
significantly predict group membership.
Method
Participants
A total of  participants were included in the present study,  individuals later diagnosed
with ASD and  individuals in the TD group . There was one set of fraternal twins
in the ASD group. Participants were drawn from a larger study conducted at the University
of North Carolina-Chapel Hill based on availability of video recordings; participants must
have had two five-minute edited video segments at – months and at least one edited
video segment at – months. As part of the larger study, participants were recruited from
the Midwest and Southeast over a -year time period. Recruitment criteria included: 
child age between two and seven years at the time of recruitment;  available home
videotapes of the child between birth and two years of age that parents were willing to share;
and  enough video footage for at least one -minute codable segment (see video editing
section below) of the child at either – or – months of age.
All participants included in the ASD group received a clinical diagnosis of ASD from a
licensed psychologist and/or physician at a point after the recordings were made. Thus, our
design is a retrospective analysis similar to others that have used home movies of children
later diagnosed with ASD . A
trained research staff member validated diagnoses for each participant using criteria from
the Diagnostic and Statistical Manual IV  and
from one or more ASD screening and diagnostic tools, including: the Childhood Autism
Patten et al.
Page 
J Autism Dev Disord. Author manuscript; available in PMC  October .
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
Rating Scale , the Autism Diagnostic
Observation Schedule , and/or the Autism Diagnostic Interview-
Revised . All participants had CARS scores and
each participant in the ASD group had ADI/ADI-R scores and  of the  ASD participants
had ADOS scores.
Typically developing group membership was based in part on scores within normal limits
(i.e., not more than one standard deviation below the mean) on the Mullen Scales of Early
Learning  and/or the Vineland Adaptive Behavior Scales (VABS; Sparrow,
Balla, & Cicchetti, ). An additional exclusionary criterion for any participants in the TD
group was any history of learning or developmental difficulties per parent report. Individuals
with significant physical, visual or hearing impairments or known genetic conditions (e.g.,
Fragile X or Rett’s Syndrome) associated with ASD were excluded. As indicated in Table ,
mean age (in months) was very similar across groups, gender was balanced, and the two
groups were also similar with regard to SES based on maternal education. Our families were
mostly middle SES with access to videotaping equipment.
The University of North Carolina-Chapel Hill Institutional Review Board approved the
study, and all families signed informed consents. For more information regarding
recruitment and inclusion criteria see Baranek .
Video Editing Procedures
Families provided home videos of their child from birth to two years as available. The
videotapes included footage from a variety of contexts including family play situations,
vacations, outings, special events, and familiar routines (e.g., mealtimes), with individual
variation in situational content of each family’s videotapes as would be expected in home
videotapes. All videotapes were copied, transformed to digital formats, and originals were
returned to participating families.
Video editing guidelines first focused on the identification of video footage during which the
child was consistently visible and for which the parents felt they could accurately identify
the child’s age. The two age ranges were originally selected for another study on early
behavior in ASD . At the same time, the two age ranges are well-suited to
our current purposes. The – month age range is the earliest age range in which parents
had sufficient videotape footage for it to be useful in our research and represents the time
period when a number of communicative behaviors emerge. Further, this is a time frame
during which the vast majority of TD children would be expected to already be in the
canonical babbling stage. The – month range provided follow-up on the same children
with the expectation that monitored behaviors would be more consistent and would allow for
confirmation or clarification of data from the earlier age. In TD children, canonical babbling
is usually well consolidated by the – month age range .
In editing tapes for the larger study, the aim was to compile two -minute video segments
for each child in the – age range, and two -minute segments in the – month age
range. On average, each -minute segment consisted of  scenes. Research assistants who
were blind to the research questions and not informed of the diagnostic status of the
Patten et al.
Page 
J Autism Dev Disord. Author manuscript; available in PMC  October .
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
participants edited the videotapes and coded each scene for the following content variables:
(a) number of people present; (b) amount of physical restriction on child’s freedom to move,
rated as low, medium, or high; (c) the amount of social intrusion another person was using
to engage the child in interaction, rated as low, medium, or high; (d) and the types of events
. The assistants were
instructed to quasi-randomly select a cross-section of scenes from the available footage in
the designated age ranges, purposely including scenes from each one-month age interval for
which video footage was available within each age range, provided that the child was visible
in each selected scene. All participants included in the current study had two -minute
compilations (i.e.,  minutes total) for the – month age range, but at the – month
age range, there were three TD infants and one infant with ASD for whom only a single -
minute segment was assembled due to insufficient video footage. As a result, the mean
duration of samples at the – month age range was . minutes rather than .
Although vocalization from the infants was common in these scenes, the segments were not
specifically selected to capture vocal behavior. Therefore, volubility estimated from the
present study may be lower than in prior works where infants have been observed in settings
designed to maximize vocal interaction. Similarly, the video segment selection procedure
may yield differences in canonical babbling from prior studies. In most studies, –
minutes of vocal interaction have been recorded, whereas here we had less than half that
amount of data per sample. Our procedure can be predicted to produce greater variability in
canonical babbling ratios than in studies with longer sampling periods (Molemans, ;
Molemans et al., ). Additionally, the audio-video quality of these home movies was not
as good as would be expected in laboratory studies, another factor that could reduce
perceived canonical babbling and volubility.
To ensure that the contexts in which children were recorded were comparable, specific
content parameters were identified and compared . No differences were
found between the groups on any content parameter including: number of people present,
level of physical restriction (i.e., amount of physical confinement such as a highchair versus
free play; rated as low, medium or high), amount of social intrusion (rated as low, medium
or high), and the total number of event types (e.g., meal time, active play). The number of
times each event type (e.g., bath time, playtime) was represented in the ASD group versus
the TD group for each age was compared using chi-square analyses. Results for the omnibus
chi-square test failed to reach significance in the – month age group , but did
reach significance in the – months age group . Typically developing children
were more likely to be engaged in passive activities at the – month age range (p =
.; TD = .%, ASD = .%) according to follow-up analysis of the six event
categories. See Tables  and  for the percentage in each category. For a comprehensive
description of the coding procedures that yielded the data on situational context see Watson,
Crais, Baranek, Dykstra, and Wilson .
Coding Procedure and Observer Agreement
The videotapes analyzed in this study were coded for infant production of all syllables in
speech-like vocalizations by two certified speech-language pathologists who were not
Patten et al.
Page 
J Autism Dev Disord. Author manuscript; available in PMC  October .
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
informed of the diagnostic group of the infants. The intent was, of course, for the coders to
be blind to diagnostic category, and with the exception of one infant to be discussed below,
the coders reported they saw no reason to suspect any infant of having ASD.
We defined speech-like vocalizations (as in the primary literature on canonical babbling) to
include both canonical and precanonical infant vocalizations (regardless of whether they
would be deemed “transcribable”). Training of the two coders was provided by the last
author, who originated the definition of “canonical syllable” used in this study, and who has
conducted and collaborated on numerous studies on onset of canonical babbling, rate of
canonical babbling, and volubility in infants (Cobo-Lewis, Oller, Lynch, & Levine, ;
Lynch et al., ; Oller & Eilers, , ; Oller, Eilers, & Basinger, ; Oller et al.,
; Oller, Eilers, Neal, & Cobo-Lewis, ). The two observers were trained in
identifying canonical syllables and in counting all syllables independent of their canonical
status. The video samples used during training were separate (although drawn from similar
materials based on the home recordings) and not included in the analyses for this
investigation.
Syllables were defined as rhythmic units of speech-like vocalization, excluding raspberries,
effort “grunt” sounds (i.e., a schwa-like sounds produced as an artifact of physical exertion),
ingressive sounds, sneezes, hiccups, crying and laughing. Within an “utterance”, which was
defined as a vocal breath group , it was possible to
identify syllables as corresponding to sonority peaks (high points of pitch and/or amplitude)
that are intuitively recognized by mature listeners. These rhythmic events occur in time
frames typical of syllables in real speech (usually with durations of – ms). A
canonical syllable is defined as including a vowel-like nucleus, at least one margin (or
consonant-like sound) and a transition between margin and nucleus that is rapid and
uninterrupted. In general, transitions that are too fast to be tracked auditorily (too fast to be
heard “as transitions”) are instead heard as gestalt syllables. Auditory tracking of these
transitions focuses on formant (acoustic energy) transitions that can be measured on
spectrograms as typically <  ms . Formants are audible bands of energy
corresponding to resonant frequencies of the vocal tract that change as the tract changes
shape or size. Audible formant transitions occur, then, when the vocal tract moves during
opening from a consonantal closure into a vowel or vice versa.
Examples of canonical utterances (which must include at least one canonical syllable) are
syllables that a listener might perceive as ba, taka, or gaga. Vocalizations produced while
mouthing objects (e.g., toys or fingers) or eating were excluded from our analyses on the
grounds that we could not be sure what role movement of the hands may have played in the
apparent syllabification.
Videos were randomized and randomly distributed across the two coders with regard to
diagnostic group. The  participants’ videos were randomly split between the coders by
participant and included both age ranges. The coders independently watched the videos,
counting both syllables and canonical syllables in real time. This procedure is utilized
regularly in the laboratories of the last author in accord with reasoning presented in recent
Patten et al.
Page 
J Autism Dev Disord. Author manuscript; available in PMC  October .
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
papers, especially Ramsdell et al. . This naturalistic listening approach mimics how a
mother would hear her child, listening to each utterance only once.
The measure of canonical babbling ratio used here (number of canonical syllables divided
by number of all syllables) is the measure utilized in the bulk of research on onset of
canonical babbling to date. However, some studies have used a different ratio (number of
canonical syllables divided by number of utterances). The former procedure is generally
preferred nowadays because the resulting value can be interpreted as a proportion with
values varying from  to , whereas the latter procedure yields a ratio with no effective
upper limit .
In a coder agreement test, both observers independently coded twenty samples consisting of
two five-minute segments of ten participants’ video footage. A research assistant unaware of
the study goals selected these test samples, and they represented both diagnostic groups and
both ages. Reliability was gauged in accord with the degree to which coders agreed upon
canonical syllables, total syllables, and whether the child was in the canonical babbling stage
(i.e., had a canonical babbling ratio > ., the standard criterion). Inter-rater agreement
ranged from good to excellent for canonical syllables  and for
total syllables . Reliability for canonical babbling ratios was
also good , with agreement on the canonical stage criterion at
% for the twenty samples. Additionally, the coders differed by an average of only % of
the total range of canonical babbling ratios obtained, and the correlation across the ratios for
the twenty samples for the two coders was .. For volubility, the coders differed by an
average of % of the total range for volubility values, and the correlation across the twenty
sample videos for the two coders was ..
Results
Analyses were performed to confirm that the groups were matched on demographic
variables. These analyses did not reveal significant differences between groups on any
variable .
Initial descriptive statistics for within- and between-group variables revealed two outliers in
the ASD group. Both cases produced very high canonical babbling ratios in the – month
range  relative to the mean for both groups (ASD = . for the  cases, TD = .
) . Based on prior research, the canonical babbling ratios observed for these
two ASD cases were substantially higher than would be expected in TD infants in the –
month age range—infants grouped as having English or Spanish at home, as high or low
SES, and as born at term or prematurely all showed mean canonical babbling ratios under .
from  to  months of age (Oller, Eilers, Urbano, & Cobo-Lewis, ; Oller, Eilers,
Steffens, Lynch, & Urbano, ). Analysis of z-scores revealed that infant  was .
standard deviations above the mean for the present sample, and infant  was . standard
deviations above the mean, further suggesting outlier status. On this basis we decided to
eliminate these two cases in the primary analyses on canonical babbling; the remaining 
cases ( ASD,  TD) were analyzed to address our research questions regarding canonical
babbling (see Figures  and  for canonical babbling ratios by participant at both ages, with
Patten et al.
Page 
J Autism Dev Disord. Author manuscript; available in PMC  October .
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
the two outliers indicated). However, there were no significant outliers with regard to
volubility, and thus we included data from all  cases for that analysis (see Figures  and 
for syllable volubility by participant at both ages).
Hypothesis : Infants later diagnosed with ASD will be less likely than typically developing
infants to be in the canonical stage at each age (– and – months)
Log odds ratios (log OR) were calculated to compare the classifications of both ASD and
typically developing children with regard to their canonical babbling. The criterion for
canonical babbling stage was set at % or greater canonical syllables compared to all
syllables; this is a common criterion in studies of canonical babbling, and is based on data
reviewed in Oller . TD infants were significantly more likely to have reached the
canonical babbling stage based on the criterion than were infants later diagnosed with ASD
at the – months age range , and
remained more likely at the – month age range (N = , log OR = ., CI = −. to
., p = .). As an easily interpretable effect size measure, the simple odds ratios (as
opposed to the log odds ratio, which is statistically preferable for significance testing with
small N’s) can be considered; the simple ORs indicated TD infants were  times more
likely  to be categorized as in the canonical stage than ASD infants at –
months and  times more likely  at – months.
Hypothesis : Infants later diagnosed with ASD will demonstrate significantly lower
canonical babbling ratios (independent of the canonical stage criterion) compared to
typically developing infants
Canonical babbling ratios of infants later diagnosed with ASD and TD infants were
contrasted using a Mixed ANOVA. The between-subjects variable was diagnostic category
(ASD vs. TD) and the within-subjects variable was age range (– months and –
months). The mean canonical babbling ratios at – months were .  for the 
infants later diagnosed with ASD and .  for the  TD infants; at – months
the values were .  and .  respectively . Analyses revealed
a significant main effect for diagnostic category  = ., p = ., ŋp = .), with
infants later diagnosed with ASD producing significantly lower canonical babbling ratios,
and a significant main effect for age  = ., p < ., ŋp = .), with higher
canonical babbling ratios at the older age. The effect size between groups for – months
was d = . (a large effect) and for – months was . (a moderate effect; Cohen,
). The age by diagnosis interaction was not significant .
Hypothesis : Infants later diagnosed with ASD will demonstrate significantly fewer total
vocalizations (lower volubility) at both age ranges compared to typically developing
infants
For this analysis, all  infants were included because there were no significant outliers.
Volubility of infants later diagnosed with ASD and TD infants were contrasted using a
Mixed ANOVA. The between-subjects variable was diagnostic category (ASD vs. TD), and
the within-subjects variable was age range (– months and – months). Infants later
diagnosed with ASD produced a mean of .  syllables per minute while TD
Patten et al.
Page 
J Autism Dev Disord. Author manuscript; available in PMC  October .
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
infants produced a mean of .  syllables per minute at – months. At –
months, infants later diagnosed with ASD produced a mean of .  syllables per
minute while TD infants produced a mean of .  syllables per minute (see
Figure ). Analyses revealed a significant main effect for diagnostic category  =
., p = ., ŋp = .), and for age  = ., p = ., ŋp = .). Thus, infants
later diagnosed with ASD displayed significantly lower volubility than TD infants. The
effect size for group at – months was d = . (large) and at – months was .
(large).
Hypothesis : A combined analysis using both volubility and canonical babbling status
will significantly predict group membership
Logistic regression analysis was conducted to test whether canonical babbling status
(whether each participant met the . criterion) and volubility at age ranges of – months
and – months could reliably predict later diagnosis status (group membership). This test
was conducted with all  cases included, partly in order to match the number of cases for
the two predictor variables and partly because the goal of the analysis was to determine the
potential practical utility of identification of these children without any information other
than volubility and canonical babbling ratio. This test may thus be the one of primary
clinical interest, since it evaluates the circumstance that screening implies, where there
would be no basis for knowing whether an infant might be an outlier on any variable.
Without this evaluation there would be no direct indication in our results of the degree of
group discriminability.
Statistical significance was reached in a test of the full model against a constant-only model,
which indicated that, as a set, canonical babbling status and volubility reliably predicted
later diagnosis . A small-to-moderate relationship between
prediction and grouping was observed , with an overall prediction
success of % (% for TD and % for ASD). However, further examination of the
predictors using the Wald criterion revealed that when all four predictor variables were
included in the model, none significantly contributed to prediction of group membership at
an individual level . The status of infants with regard to canonical babbling stage
at the – months age range provided the largest observed predictive contribution, Wald =
., p = ., EXP(B) = .. The contribution to group discriminability by volubility at
– and – months age ranges approached nil, EXP(B) = . and . respectively.
Examination of the correlations among the predictor variables showed that all but volubility
at – months were significantly correlated with all other predictors , with
volubility at – months significantly correlated with only canonical babbling at –
months. This inter-relation among the predictor variables suggests that, to some degree, they
account for some of the same variance in diagnosis. However, the observed EXP(B) values
(odds ratios of the outcomes given the value of an individual predictor) more strongly
suggest that canonical babbling at – months accounted for the bulk of the variability in
diagnosis.
It seems clear that significance of the individual predictors in the logistic regression may
have been hampered by the high level of relation among them. Individually, the volubility
Patten et al.
Page 
J Autism Dev Disord. Author manuscript; available in PMC  October .
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
variables did not appear to have much influence given small Betas and high p-values. When
predictors were entered into the model in a hierarchical fashion, no matter how predictor
entry was ordered (– month variables at step  and – month variables at step , or
CB variables at step  and volubility at step ), only the – month CB variable was a
significant independent predictor. R changes and diagnostic ability for all of the regression
and regression step iterations suggested little was added to the R by adding variables in the
second step , nor did
these additions substantially alter the ability of the model to predict later diagnosis. The
most efficient model appeared to be a logistic regression with – month CB as the only
predictor.
Discussion
The importance of early intervention for children with ASD has resulted in attempts to
quantify behaviors in infancy that may lead to early detection. Substantial effort has
addressed gestural and social development and their potential roles in detection within the
first year of life . The present
results offer parallel findings in the domain of vocal development by demonstrating
significant group differences in canonical babbling status, canonical babbling ratio, and total
syllables produced (volubility) during the first year of life.
In our study, infants later diagnosed with ASD were significantly less likely to be classified
as being in the canonical babbling stage, and demonstrated significantly reduced canonical
babbling ratios compared to TD peers. Although significant group differences were apparent
in both age ranges (– and – months), the effect sizes for canonical babbling were
larger at – months. Paul et al.  demonstrated similar results in infants at high-risk
for developing ASD who produced significantly lower canonical babbling ratios compared
to low risk infants at  months, though at  months the differences were not statistically
significant. Combined with the finding from Oller et al.  that children with ASD up to
 months of age show low canonical syllable production, the data here suggest that low
production of canonical syllables may be a helpful marker for ASD from infancy into early
childhood.
Since canonical babbling is well established in the vast majority of TD infants by  months
, it might seem odd that several of the TD infants ( at – months
and  at – months) in the present study provided samples that did not meet the .
canonical babbling ratio criterion for assignment to the canonical stage of vocal
development. However, it is important to consider the fact that even infants who are clearly
in the canonical stage based on parent report often fail to reach the criterion in a single
laboratory sample of – minutes . In addition, unlike the samples in
prior research on canonical babbling, the samples here were not designed to elicit
vocalizations, and consequently they may have been less rich in quantity and variety of
vocalization than the samples that were used to develop the criterion. Further, our samples at
– months were only  minutes in duration, and at – months an average of slightly
less than  minutes; it has been shown that variability in obtained canonical babbling ratios
increases as the length of samples decreases (Molemans, ; Molemans, Van den Berg,
Patten et al.
Page 
J Autism Dev Disord. Author manuscript; available in PMC  October .
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
Van Severen, & Gillis, ). Finally, our samples were based on home recordings with
considerable noise and variable camera management that may have impeded our ability to
recognize vocalizations in the samples. Consequently, we are not surprised that some of the
TD infants failed to reach the criterion used to determine canonical status based on
laboratory samples.
Given the strong links between the onset of canonical babbling and language development
, delayed onset of
canonical babbling in infants with ASD may reflect latent communication impairment. It
also may be that delayed canonical babbling directly contributes to communication
symptoms in ASD. Canonical babbling requires motor ability as well as motivation to
produce syllables, and practice in babbling may lay critical foundations for speech.
Prospective research on motor development in infants later diagnosed with ASD is sparse
and often limited to high-risk groups, but available research does indicate that early motor
impairment may be present (e.g., Matson, Mahan, Fodstad, Hess, & Neal, ; Manjiviona
& Prior ; Page & Boucher, ; Teitelbaum, Teitelbaurm, Nye, Fryman, & Maurer,
). Thus delayed canonical babbling may reflect an immature or disordered motor
system with specific implications for speech.
If language develops as a consequence of social reinforcement of speech-like sounds that
eventually evolve into true words (consider behavioral models of language development as
in Hulit & Howard, ; Goldstein, King, & West, ; Goldstein & Schwade, ;
Goldstein & West, ), then social reinforcement may encourage the production of
canonical babbling. Children with ASD may be less motivated by social reinforcement,
yielding less frequent vocal exploration and production of canonical syllables than in TD
infants. To add to the problem, a delay in canonical babbling may result in reduction in
caregiver social-communication directed toward the infant. On average, by six to seven
months and very rarely later than ten months, canonical babbling emerges in TD infants
. In response to recognition of canonical babbling, caregivers alter
their communication pattern, sometimes attempting to direct the infant toward using
canonical syllables meaningfully—for example, the parent who hears [baba] may reply,
“Yes, that’s a bubble” . Therefore, infants who are
delayed in canonical babbling may also be delayed in their exposure to important linguistic
input, and thus may be given less opportunity to learn words. A final point is that infants
with ASD may simply have lower motivation to vocalize socially in the first place. This
lower motivation could provide a further basis for slow vocabulary learning.
Our results on volubility included two statistically reliable findings. First, children in both
groups had lower volubility at the second age than at the first. We attribute no particular
theoretical importance to this finding but we take note of the fact that the lower level of
volubility at – months compared to – months did correspond to greater physical
movement of the children at the older age. In both groups combined, level of physical
restriction during the selected recording samples was significantly less at the older age (p < .
). As reported earlier, level of physical restriction was not significantly different between
diagnostic groups.
Patten et al.
Page 
J Autism Dev Disord. Author manuscript; available in PMC  October .
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
The second volubility finding is that infants later diagnosed with ASD produced
significantly fewer vocalizations deemed to be relevant for the emergence of speech (both
canonical and non-canonical sounds) at both age ranges (– and – months) compared
to TD peers. Other research has demonstrated that infants with ASD direct fewer
vocalizations to others ; our study extends this finding to a more
general measure of volubility in terms of total vocalizations (syllables) rather than only ones
directed to others. Our finding is also congruent with results from automated analysis of all-
day recordings indicating low volubility in children with ASD at – months of age
. The results may seem to run counter to Paul et al.  whose
sample of high-risk infants were reported to not produce significantly fewer vocalizations
than low-risk infants. However, as described in the introduction above, the Paul et al. study
did not report data in a way that can be directly compared with the volubility data reported
here.
Some disability groups (e.g., hearing impaired infants and infants with cleft palate) have
been reported to exhibit volubility similar to that of TD infants (Clement, ; Chapman,
Hardin-Jones, Schulte, & Halter, ; Van den Dikkenberg-Pot, Koopmans-van Beinum, &
Clement, ; Nathani, Oller, & Neal, ; Davis, Morrison, von Hapsburg, & Warner-
Czyz, ); however, infants from low SES households have been reported to have
significantly decreased volubility in comparison to those from higher SES households
. Children from low SES backgrounds are
often presumed to be at-risk for language deficits. Although it would be impossible to
identify and quantify all of the mechanisms through which poverty may affect language
development, research has demonstrated that the amount of communication caregivers direct
toward their children is decreased in low SES situations . This
impoverished linguistic environment may result in decreased dyadic social and
communicative interactions and thus in a decrease in overall volubility of infants.
It is important to note that the relatively well-matched SES between our two groups suggests
that the differences in volubility were not attributable to differences in SES. In the case of
low SES households, an impoverished linguistic environment due to lack of parent
responsiveness might be expected to lead to decreased volubility of the infant and later
language difficulty. For infants later diagnosed with ASD, reduced volubility may be
affected by multiple factors, not related to inherent parental responsiveness, but related
instead to the social impairments of ASD. One issue is that these children may experience
less linguistic stimulation due to having disrupted sensory processing systems corresponding
to sensory hyporesponsiveness; children with ASD are less likely to respond, or require
substantially more stimulation to respond to environmental events (Baranek, ; Baranek
et al., ); Miller, Reisman, McIntosh, & Simon, ; Rogers & Ozonoff, ). This
characteristic of ASD is also reflected in the tendency for infants as young as eight months
who will later be diagnosed with ASD to be less likely than TD infants to respond to their
name being called . This lack of
responsiveness may indicate that infants with ASD are less affected by vocal
communication from caregivers than TD infants. If so, the lack of responsiveness may
reflect an effectively impoverished linguistic environment because of attenuated reception of
caregiver input by infants with ASD and subsequent communication impairments. Indeed,
Patten et al.
Page 
J Autism Dev Disord. Author manuscript; available in PMC  October .
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
sensory hyporesponsiveness has been shown to be associated with poorer language
functioning in children with ASD .
An additional way that the environment for children with ASD may be impoverished could
involve a social feedback loop  that is under investigation using
automated analysis of vocalizations of parents and infants from all-day home recordings.
Since infants with ASD produce fewer canonical syllables than TD infants, and since
parents respond strongly with language stimulation to canonical syllables, an infant with
ASD may actually hear less language from parents, because parents provide input that is tied
to the infant’s output. The infant’s low volubility may then be aggravated by lower input
levels resulting from the infant’s own anomalous pattern of vocalization.
Finally, the logistic regression analysis with four independent variables (age  and age 
canonical babbling classification and age  and age  volubility) demonstrated that
classification of diagnostic category (ASD vs. TD) could be predicted with % accuracy,
even when the two outliers were included. The model more accurately classified infants later
diagnosed with ASD (Sensitivity = .%) than TD infants (Specificity = .%). The
strongest predictor of group membership was canonical babbling classification at –
months as it alone correctly classified % of infants later diagnosed with ASD and % of
TD infants. Thus, in the search for markers of ASD risk in infancy, canonical babbling
status at – months appears to be the single best candidate among the variables considered
in the current study. The utility of the measure as a group marker is age dependent, since a
larger proportion of infants in the ASD group at – months had reached the canonical
stage than at – months.
To help better understand the high canonical babbling ratios of the two outliers, the coders,
both certified speech-language pathologists, viewed the videos from those infants again after
their outlier status was identified. We speculated that the outlier status of these two infants
may be related to the phenomenon of motor stereotypy that is common in ASD, that is, that
the two infants were engaged, at least in the – month samples, in a motor stereotypy
focused precisely on canonical babbling. In re-examining the videos of the two outliers, the
coders looked for qualitative evidence that might speak to the credibility of this speculation.
In the second viewing of the recordings, the coders noticed that the first outlier infant
produced the majority of the canonical syllables during a single scene while walking
outside. He repeatedly produced a [da] syllable during this brief episode, but did not direct
his vocalizations to the caregiver. The sense that a prelinguistic vocal stereotypy may have
been operating was enhanced by the fact that the same syllable was repeated throughout.
The stereotypy of canonical babbling in this infant was reported by the coders as
constituting the only evidence either had noticed as specifically suggesting the possibility of
ASD while they were coding, and thus, this was the single case where the intended blinding
of the coders to diagnostic group seems to have been foiled. The coders did not observe any
other stereotypic behaviors vocal or otherwise in these samples. The second infant engaged
in high canonical babble production while roughhousing with his father, but to our clinical
eyes, that behavior did not seem particularly unusual. Further research on the possibility that
babbling can be a focus of motor stereotypy in ASD seems in order. It may be worthy of
Patten et al.
Page 
J Autism Dev Disord. Author manuscript; available in PMC  October .
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
note that the two outliers’ CARS scores  fell within the range of the scores for
the ASD group .
In addition to the findings suggesting possible clinically useful markers for ASD, the present
results provide a new scientific view on the robustness of canonical babbling. There has
been no prior empirical indication that canonical babbling onset is delayed in ASD, nor that
volubility is low in infants later diagnosed with ASD. Our results thus suggest that the
development of vocalization in infancy is affected by whatever the fundamental disorders of
ASD may be. Assuming ASD to be a social disorder, it is not obvious that babbling would
necessarily be disturbed in the disorder because the extent to which babbling is a social (as
opposed to an endogenously generated) phenomenon is itself an empirical question. Our
results can then be thought to provide a new empirical perspective on the possible social
nature of babbling. The results also suggest that the vocal differentiation of the two groups is
robust, given the relative clarity of the results indicating low canonical babbling and
volubility in the infants in the ASD group, even though we had samples of low recording
quality and very limited duration. The results seem especially significant in the context of a
broad body of research cited above on the robustness of canonical babbling as a foundation
for language and on the robust resistance of canonical babbling to delay as seen in prior
studies cited in our paper—no delay has been found in cases of prematurity, low SES, or
multilingual exposure.
Future Directions and Limitations
This study provides a proof of concept regarding the notion of atypical emergence of the
canonical babbling stage in the developing infant who will later be diagnosed with ASD and
the possibility that tracking canonical babbling in infancy may add to our repertoire of
markers for ASD prior to one year of age. Future research to address some of the limitations
of the current study and advance our understanding of the development of canonical
babbling among infants with ASD is warranted by the findings of the current study. One
limitation in the current study was the lack of a comparison group of infants with later
diagnoses of non-ASD disabilities, which prevents us from definitively attributing the
differences found in this study to ASD rather than general impairments in cognition or
communication. Our working hypothesis to test in future studies will be that these
differences in canonical babbling onset and in volubility are specific to ASD.
Another limitation was that our study used only short video segments from each time point,
which surely impacted our ability to precisely assess important aspects of vocalization,
because it has been shown that variability in obtained canonical babbling ratios increases as
the length of samples decreases (Molemans, ; Molemans, Van den Berg, Van Severen,
& Gillis, ). The low canonical babbling ratios obtained for a few of the TD infants
presumably would not have occurred with larger sample sizes. In future studies we hope to
obtain longer samples, and if possible to more precisely identify canonical babbling onset
through longitudinal laboratory assessments paired with caregiver report of onset. But of
course to make this possible, prospective studies may be necessary, with several years of
follow-up, presumably taking advantage of the opportunity presented by sibling studies.
Such studies would also afford the opportunity to obtain much better recordings than are
Patten et al.
Page 
J Autism Dev Disord. Author manuscript; available in PMC  October .
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
available in retrospective studies such as the present one. Indeed, sibling studies can now
capitalize on all-day recording, yielding the opportunity to assess vocal development in ASD
with much greater ecological validity and representativeness.
Onset of canonical babbling usually occurs between  and  months in TD infants. It
appears from the present data that onset may occur within a much wider range in ASD.
Quantification of onset in ASD may yield prognostic value regarding core communication
symptoms. For example, if canonical stage onset is delayed beyond a certain threshold, the
infant may be at especially high-risk for remaining nonverbal. Discovery of such a delay
could allow specific interventions to be tailored based on prognosis earlier in development.
Future research should also focus on caregivers and their roles in canonical stage
development and its identification. Prior work suggests that with TD infants, parents are
extremely accurate in their reports of the onset of canonical babbling . If
caregivers of infants with ASD are similarly capable of identifying onset of canonical
babbling, it may be possible to use canonical babbling onset as part of a parent-report
screening tool for early identification. In addition, alterations in communication directed to
infants by caregivers as canonical babbling emerges may help to elicit and maintain social-
communicative interaction, and subsequently impact language development.
Our findings on volubility represent another potential avenue for understanding early social-
communication development processes in ASD. Perhaps the most intriguing aspect of this
possibility is suggested by the proposal that there may be a feedback loop involving low
canonical syllable production in ASD followed by low parental rate of vocalization to
infants, aggravating the low volubility and low rate of canonical syllables in ASD
. We anticipate rapid growth of studies tracking this possibility,
especially since there is a rapidly growing possibility of conducting some aspects of such
analysis based on automated classification of vocalizations in all-day recordings as indicated
by the growth of LENA system studies.
Clinical Implications
Our findings suggest that canonical babbling should be considered an important milestone in
infancy that may be delayed in infants who are later diagnosed with ASD. If infants
demonstrate delays in canonical babbling, a developmental assessment that includes
evaluation of early warning signs for ASD should be administered. Although volubility
appears less promising as a marker for ASD, it may be useful in combination with other
items in the context of early identification screening tools. For infants demonstrating either
low canonical babbling ratios or low volubility, interventions to draw infants’ attention to
social-communicative stimuli in that context of dyadic interactions may help stimulate
growth of vocal communication.
Acknowledgments
This research was made possible through a grant from the National Institute for Child Health and Human
Development  and a grant from Cure Autism Now Foundation (Sensory-Motor and Social-
Communicative Symptoms of Autism in Infancy). We thank the families whose participation made this study
possible and the staff who collected and processed data for this project.
Patten et al.
Page 
J Autism Dev Disord. Author manuscript; available in PMC  October .
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
References
Acevedo MC. The role of acculturation in explaining ethnic differences in the prenatal health-risk
behaviors, mental health, and parenting beliefs of Mexican American and European American at-
risk women. Child Abuse & Neglect. ; :–. 
American Psychiatric Association. Diagnostic and statistical manual of mental disorders. .
Washington, DC: Author; . text rev
Baranek GT. Autism during infancy: A retrospective video analysis of sensory-motor and social
behaviors at – months of age. Journal of Autism and Developmental Disorders. ; :–
. 
Baranek GT, David FJ, Poe MD, Stone WL, Watson LR. Sensory Experiences Questionnaire:
Discriminating sensory features in young children with autism, developmental delays, and typical
development. Journal of Child Psychology and Psychiatry. ; :–. [PubMed:
]
Baranek GT, Watson LR, Boyd BA, Poe MD, David FJ, McGuire L. Hyporesponsiveness to social and
nonsocial sensory stimuli in children with autism, children with developmental delays, and typically
developing children. Development and Psychopathology. ; :–. [PubMed:
]
Bleile KM, Stark RE, McGowan JS. Speech development in a child after decannulation: Further
evidence that babbling facilitates later speech development. Clinical Linguistics and Phonetics.
; :–.
Center for Disease Control and Prevention. Prevalence of autism spectrum disorders –Autism and
developmental disabilities monitoring network,  Sites, United States, . Morbidity and
Mortality Weekly Report Surveillance Summaries. ; :–. Retrieved from http://
.
Clarke E, Reichard U, Zuberbühler K. The anti-predator behaviour of wild white-handed gibbons
(Hylobates lar). Behavioral Ecology and Sociobiology. ; :–../
s---
Chapman K, Hardin-Jones M, Schulte J, Halter K. Vocal development of  month-old babies with cleft
palate. Journal of Speech, Language and Hearing Research. ; :–.
Chawarska K, Klin A, Paul R, Macari Volkmar F. A prospective study of toddlers with ASD: Short-
term diagnostic and cognitive outcomes. Journal of Autism and Developmental Disorders. ;
:–.
Clement, CJ. PhD Dissertation. Netherlands Graduate School of Linguistics; Amsterdam: .
Development of vocalizations in deaf and normally hearing infants.
Cobo-Lewis AB, Oller DK, Lynch MP, Levine SL. Relations of motor and vocal milestones in
typically developing infants and infants with Down syndrome. American Journal on Mental
Retardation. ; :–. 
Cohen, J. Statistical Power Analysis for the Behavioral Sciences. Hillsdale,, NJ: Erlbaum Associates;
.
Davis BL, Morrison HM, von Hapsburg D, Warner AD. Early vocal patterns in infants with varied
hearing levels. Volta Review. ; :–.
Delgado CEF, Messinger DS, Yale ME. Infant responses to direction of parental gaze: A comparison
of two still-face conditions. Infant Behavior and Development. ; :–.
Eilers RE, Oller DK, Levine S, Basinger D, Lynch MP, Urbano R. The role of prematurity and
socioeconomic status in the onset of canonical babbling in infants. Infant Behavior and
Development. ; :–.
Eilers RE, Oller DK. Infant vocalizations and the early diagnosis of severe hearing impairment. The
Journal of Pediatrics. ; :–. 
Goldstein MH, Schwade JA, Bornstein MH. The value of vocalizing: Five-month-old infants associate
their own noncry vocalizations with responses from adults. Child Development. ; :–
. 
Patten et al.
Page 
J Autism Dev Disord. Author manuscript; available in PMC  October .
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
Goldstein MH, King AP, West MJ. Social interaction shapes babbling: Testing parallels between
birdsong and speech. Proceedings of the National Academy of Sciences. ; :–
.
Goldstein MH, Schwade JA. Social feedback to infants’ babbling facilitates rapid phonological
learning. Psychological Science. ; :–. 
Goldstein MH, West MJ. Consistent responses of human mothers to prelinguistic infants: The effect of
prelinguistic repertoire size. Journal of Comparative Psychology. ; :–. [PubMed:
]
Hart, B.; Risley, TR. Meaningful differences in the everyday experience of young American children.
Baltimore: Paul H. Brookes; .
Hulit, LM.; Howard, MR. Born to talk: An introduction to speech and language development. Boston:
Allyn and Bacon; .
Kent R, Osberger MJ, Netsell R, Hustedde CG. Phonetic development identical twins differing in
auditory function. Journal of Speech and Hearing Disorders. ; :–. 
Koopmans-van Beinum, FJ.; Clement, CJ.; van den Dikkenberg-Pot, I. Influence of lack of auditory
speech perception on sound productions of deaf infants. Berne, Switzerland: International Society
for the Study of Behavioral Development; .
Koopmans-van Beinum, FJ.; van der Stelt, JM. Early stages in the development of speech movements.
In: Lindblom, B.; Zetterstrom, R., editors. Precursors of early speech. New York: Stockton Press;
. p. -.
Lewedag, VL. Doctoral Dissertation. University of Miami; Coral Gables, FL: . Patterns of onset
of canonical babbling among typically developing infants.
Locke JL, Pearson D. Linguistic significance of babbling: Evidence from a tracheostomized infant.
Journal of Child Language. ; :–. 
Lord C. Follow-up of two-year-olds referred for possible autism. Journal of Child Psychology and
Psychiatry, and Allied Disciplines. ; :–.
Lord, C.; Rutter, M.; DiLavore, P.; Risi, S. Autism Diagnostic Observation Schedule (ADOS). Los
Angeles, CA: Western Psychological Services; .
Lynch MP, Oller DK, Steffens ML, Levine SL, Basinger D, Umbel V. The onset of speech-like
vocalizations in infants with Down syndrome. American Journal of Mental Retardation. ;
:–. 
Lynch MP, Oller DK, Steffens ML, Buder EH. Phrasing in prelinguistic vocalizations. Developmental
Psychobiology. ; :–. 
Manjiviona J, Prior M. Comparison of Asperger syndrome and high-functioning autistic children on a
test of motor impairment. Journal of Autism and Developmental Disorders. ; :–.

Masataka N. Why early linguistic milestones are delayed in children with Williams syndrome: Late
onset of hand banging as a possible rate-limiting constraint on the emergence of canonical
babbling. Developmental Science. ; :–.
Matson JL, Fodstad JC, Dempsey T. What symptoms predict the diagnosis of autism or PDD-NOS in
infants and toddlers with developmental delays using the Baby and Infant Screen for Autism
Traits. Developmental Neurorehabilitation. ; :–. 
Matson JL, Mahan S, Hess JA, Fodstad JC, Neal D. Convergent validity of the Autism Spectrum
Disorder-Diagnostic for Children (ASD-DC) and Childhood Autism Rating Scales (CARS).
Research in Autism Spectrum Disorders. ; :–.
Miller, LJ.; Reisman, JE.; McIntosh, DN.; Simon, J. An ecological model of sensory modulation:
Performance in children with fragile X syndrome, autistic disorder, attention–deficit/hyperactivity
disorder, and sensory modulation dysfunction. In: Smith-Roley, S.; Blanche, EI.; Schaaf, RC.,
editors. Understanding the Nature of Sensory Integration with Diverse Populations. San Antonio,
TX: Therapy Skill Builders; . p. -.
Molemans, I. PhD. University of Antwerp; Antwerp, Belgium: . Sounds like babbling: A
Longitudinal investigation of aspects of the prelexical speech repertoire in young children
acquiriing Dutch: Normally hearing children and hearing impaired children with a cochlear
implant.
Patten et al.
Page 
J Autism Dev Disord. Author manuscript; available in PMC  October .
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
Molemans I, Van den Berg R, Van Severen L, Gillis S. How to measure the onset of babbling reliably.
Journal of Child Language. ; :–. 
Mullen, EM. Mullen Scales of Early Learning: AGS Edition. Circle Pines, MN: American Guidance
Service; .
Nathani S, Oller DK, Neal AR. On the robustness of vocal development: An examination of infants
with moderate-to-severe hearing loss and additional risk factors. Journal of Speech, Language, and
Hearing Research. ; :–.
Obenchain P, Menn L, Yoshinaga-Itano C. Can speech development at  months in children with
hearing loss be predicted from information available in the second year of life? Volta Review.
; :–.
Oller, DK. The emergence of the sounds of speech in infancy. In: Yeni-Komshian, G.; Kavanagh, J.;
Ferguson, C., editors. Child phonology, Vol : Production. New York: Academic Press; . p.
-.
Oller, DK. The Emergence of the Speech Capacity. Mahwah, NJ: Lawrence Erlbaum Associates;
.
Oller DK, Eilers RE. Similarities of babbling in Spanish- and English-learning babies. Journal of Child
Language. ; :–. 
Oller DK, Eilers RE. The role of audition in infant babbling. Child Development. ; :–.

Oller DK, Eilers RE, Steffens ML, Lynch MP, Urbano R. Speech-like vocalizations in infancy: an
evaluation of potential risk factors. Journal of Child Language. ; :–. [PubMed:
]
Oller DK, Eilers RE, Urbano R, Cobo-Lewis AB. Development of precursors to speech in infants
exposed to two languages. Journal of Child Language. ; :–. 
Oller DK, Eilers RE, Basinger D. Intuitive identification of infant vocal sounds by parents.
Developmental Science. ; :–.
Oller DK, Eilers RE, Basinger D, Steffens ML, Urbano R. Extreme poverty and the development of
precursors to the speech capacity. First Lang. ; :–.
Oller DK, Eilers RE, Neal AR, Cobo-Lewis AB. Late onset canonical babbling: a possible early
marker of abnormal development. American Journal on Mental Retardation. ; :–.

Oller DK, Eilers RE, Neal AR, Schwartz HK. Precursors to speech in infancy: the prediction of speech
and language disorders. Journal of Communication Disorders. ; :–. [PubMed:
]
Oller DK, Niyogi P, Gray S, Richards JA, Gilkerson J, Xu D, Warren SF. Automated Vocal Analysis
of Naturalistic Recordings from Children with Autism, Language Delay and Typical
Development. Proceedings of the National Academy of Sciences. ; :–.
Osterling JA, Dawson G, Munson JA. Early recognition of -year-old infants with autism spectrum
disorder versus mental retardation. Development and Psychopathology. ; :–.

Ozonoff S, Iosif A, Baguio F, Cook IC, Hill MM, Hutman T, Rogers SJ, Rozga A, Sangha S, Sigman
M, Steinfeld MB, Young GS. A prospective study of the emergence of early behavioral signs of
autism. Journal of the American Academy of Child & Adolescent Psychiatry. ; :–
. 
Page J, Boucher J. Motor impairments in children with autistic disorder. Child Language Teaching and
Therapy. ; :.
Papoušek, M. Vom ersten Schrei zum ersten Wort: Anfänge der Sprachentwickelung in der
vorsprachlichen Kommunikation. Bern: Verlag Hans Huber; .
Paul R, Augustyn A, Klin A, Volkmar FR. Perception and production of prosody by speakers with
ASD spectrum disorders. Journal of Autsim and Developmental Disorders. ; :–.
Paul R, Fuerst Y, Ramsay G, Chawarska K, Klin A. Out of the mouths of babes: Vocal production in
infant siblings of children with ASD. Journal of Child Psychology and Psychiatry. ; :
–. 
Patten et al.
Page 
J Autism Dev Disord. Author manuscript; available in PMC  October .
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
Peppe S, McCann J, Gibbon F, O’Hara A, Rutherford M. Receptive and expressive prosodic ability in
children with high-functioning ASD. Journal of Speech, Language, and Hearing Research. ;
:–.
Ramsdell HL, Oller DK, Buder EH, Ethington CA, Chorna L. Identification of prelinguistic
phonological categories. Journal of Speech Language and Hearing Research. ; :–.
Robins DI, Fein D, Barton MI, Green JA. The modified checklist for autism in toddlers: An initial
study investigating the early detection of autism and pervasive developmental disorders. Journal of
Autism and Developmental Disorders. ; :–. 
Rogers SJ, Ozonoff S. Annotation: What do we know about sensory dyfunction in autism? A critical
review of the empirical evidence. Journal of Child Psychology and Psychiatry. ; :–
. 
Ross GS. Language functioning and speech development of six children receiving tracheostomy in
infancy. Journal of Communication Disorders. ; :–. 
Rutter, M.; Le Couteur, A.; Lord, C. Autism Diagnostic Interview-Revised. Los Angeles, CA: Western
Psychological Services; .
Schopler, E.; Reichler, RJ.; Rochen Renner, B. The Childhood Autism Rating Scale. Lost Angeles,
CA: Western Psychological Services; .
Sheinkopf SJ, Mundy P, Oller DK, Steffens M. Vocal atypicalities of preverbal autistic children.
Journal of Autism and Developmental Disorders. ; :–. 
Simon BM, Fowler SM, Handler SD. Communication development in young children with long-term
tracheostomies: Preliminary report. International Journal of Otorhinolaryngology. ; :–.
Snow, CE. Issues in the study of input: fine-tuning universality, individual and devlopmental
differences and necessary causes. In: MacWhinney, B.; Fletcher, P., editors. NETwerken:
Bijdragen van het vijfde NET symposium: Antwerp Papers in Linguistics. Vol. . Antwerp:
University of Antwerp; . p. -.
Sparrow, SS.; Balla, DA.; Cicchetti, DV. Vineland Adaptive Behavior Scales. Circle Pines, MN:
American Guidance Service; .
Stark, RE. Stages of speech development in the first year of life. In: Yeni-Komshian, G.; Kavanagh, J.;
Stark, RE.; Ansel, BM.; Bond, J. Are prelinguistic abilities predictive of learning disability? A follow-
up study. In: Masland, RL.; Masland, M., editors. Preschool Prevention of Reading Failure.
Parkton, MD: York Press; .
Stoel-Gammon C. Prespeech and early speech development of two late talkers. First Language. ;
:–.
Stoel-Gammon C, Otomo K. Babbling development of hearing impaired and normally hearing
subjects. Journal of Speech and Hearing Disorders. ; :–. 
Stoel-Gammon C. Relationships between lexical and phonological development in young children.
Journal of Child Language. ; :–. 
Teitelbaum P, Teitelbaum O, Nye J, Fryman J, Maurer RG. Movement analysis in infancy may be
useful for early diagnosis of autism. Proceedings of the National Academy of Sciences of the
United States of America. ; :–. 
Tronick, EZ. Social interchange in infancy. Baltimore: University Park Press; .
Van den Dikkenberg-Pot I, Koopmans-van Beinum F, Clement C. Influence of lack of auditory speech
perception of sound productions of deaf infants. Proceedings of the Institute of Phonetic Sciences,
University of Amsterdam. ; :–.
Vihman, MM. Phonological Development: The Origins of Language in the Child. Cambridge, MA:
Blackwell Publishers; .
Volkmar FR, Chawarska K. Autism in infants: An update. World Psychiatry: Official Journal of the
World Psychiatric Association. ; :–.
Watson LR, Patten E, Baranek GT, Boyd BA, Freuler A, Lorenzi J. Differential associations between
sensory response patterns and language, social, and communication measures in children with
autism or other developmental disabilities. Journal of Speech, Language, and Hearing Research.
; :–.
Patten et al.
Page 
J Autism Dev Disord. Author manuscript; available in PMC  October .
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
Watson LR, Crais ER, Baranek GT, Dykstra JR, Wilson KP. Communicative Gesture Use in Infants
with and without Autism: A Retrospective Home Video Study. American Journal of Speech-
Language Pathology. ; :–. 
Warlaumont, AS.; Oller, DK.; Dale, R.; Richards, JA.; Gilkerson, J.; Xu, D. Vocal Interaction
Dynamics of Children With and Without Autism. Paper presented at the Proceedings of the nd
Annual Conference of the Cognitive Science Society; Austin, TX. .
Warren SF, Gilkerson J, Richards JA, Oller DK. What Automated Vocal Analysis Reveals About the
Language Learning Environment of Young Children with Autism. Journal of Autism and
Developmental Disorders. ; :–. 
Weismer SE, Lord C, Esler A. Early language patterns of toddlers on the autism spectrum compared to
toddlers with developmental delay. Journal of Autism and Developmental Disorders. ;
:–. 
Werner E, Dawson G, Osterling J, Dinno N. Brief report: Recognition of autism spectrum disorder
before one year of age: A retrospective study based on home videotapes. Journal of Autism and
Developmental Disorders. ; :–. 
Wetherby AM, Woods J, Allen L, Cleary J, Dickinson H, Lord C. Early indicators of ASD spectrum
disorders in the second year of life. Journal of ASD and Developmental Disorders. ; :–
.
Yale ME, Messinger DS, Cobo-Lewis AB, Oller DK, Eilers RE. An event-based analysis of the
coordination of early infant vocalizations and facial actions. Developmental Psychology. ;
:–. 
Zwaigenbaum L, Bryson S, Rogers T, Roberts W, Brian J, Szatmari P. Behavioral manifestations of
autism in the first year of life. International Journal of Developmental Neuroscience. ; (–
):–. 
Patten et al.
Page 
J Autism Dev Disord. Author manuscript; available in PMC  October .
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
Figure .
Canonical babbling ratios by participant at – months
Patten et al.
Page 
J Autism Dev Disord. Author manuscript; available in PMC  October .
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
Figure .
Canonical babbling ratios by participant at – months
Patten et al.
Page 
J Autism Dev Disord. Author manuscript; available in PMC  October .
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
Figure .
Syllable volubility by participant at – months.
Patten et al.
Page 
J Autism Dev Disord. Author manuscript; available in PMC  October .
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
Figure .
Syllable volubility by participant at – months.
Patten et al.
Page 
J Autism Dev Disord. Author manuscript; available in PMC  October .
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
Figure .
Canonical babbling ratios by age and diagnosis
Patten et al.
Page 
J Autism Dev Disord. Author manuscript; available in PMC  October .
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
Figure .
Volubility by age and diagnosis
Patten et al.
Page 
J Autism Dev Disord. Author manuscript; available in PMC  October .
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
Patten et al.
Page 
Table 
Participant Demographics
ASD; n=
TD; n=
Age at –months; mean (SD)
. 
Age at – months; mean (SD)
. 
Sex
 males,  females
 males,  females
Race
 White,  Black
 White,  Asian
Maternal education 
.
Childhood Autism Rating Scale; mean (SD)
. 
Maternal education: =th grade or lower; =th to th grade; =partial high school; =high school graduate/GED; =associate of arts/associate
of science or technical training or partial college training; =bachelor of arts/science; =master of arts/science or doctorate or other professional
degree completed
missing information for two participants
missing information for four participants
J Autism Dev Disord. Author manuscript; available in PMC  October .
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
Patten et al.
Page 
Table 
Content Variables for Videos, – months
ASD; mean (SD)
TD; mean (SD)
Number of people present
. 
Amount of physical restrictiona
. 
Amount of social intrusiona
. 
Total number of different event types
. 
aRated by coders on a  to  scale
J Autism Dev Disord. Author manuscript; available in PMC  October .
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
Patten et al.
Page 
Table 
Content Variables for Videos, – months
ASD; mean (SD)
TD; mean (SD)
Number of people present
. 
Amount of physical restrictiona
. 
Amount of social intrusion a
. 
Total number of different event types
. 
aRated by coders on a  to  scale
J Autism Dev Disord. Author manuscript; available in PMC  October .
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
Patten et al.
Page 
Table 
Percentage of each activity type, – month videos
ASD; n=
TD; n=
Mealtime
%
%
Active
.%
Bathtime
.%
Other
.%
special activity
.%
passive activity
.%
J Autism Dev Disord. Author manuscript; available in PMC  October .
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
Patten et al.
Page 
Table 
Percentage of each activity type, – month videos
ASD; n=
TD; n=
Mealtime
.%
Active
%
.%
Bathtime
.%
Other
.%
Special activity
.%
Passive activity
.%
J Autism Dev Disord. Author manuscript; available in PMC  October .
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
Patten et al.
Page 
Table 
Intercorrelations Between Canonical Babbling Ratios and Volubility




. Canonical Babbling – mos
__
.*
.**
.*
. Volubility – mos
__
. Canonical Babbling – mos
__
.**
. Volubility – mos
__
*p < .,
**p < .
J Autism Dev Disord. Author manuscript; available in PMC  October .
ORIGINAL RESEARCH
published:  June 
doi: ./fped..
Frontiers in Pediatrics | 

June  | Volume  | Article 
Edited by:
Sara Calderoni,
Fondazione Stella Maris (IRCCS), Italy
Reviewed by:
Whitney I. Mattson,
Nationwide Children’s Hospital,
United States
Lori-Ann Rosalind Sacrey,
University of Alberta, Canada
*Correspondence:
Xiaoyan Ke

Specialty section:
This article was submitted to
Child and Adolescent Psychiatry,
a section of the journal
Frontiers in Pediatrics
Received:  January 
Accepted:  May 
Published:  June 
Citation:
Qiu N, Tang C, Zhai M, Huang W,
Weng J, Li C, Xiao X, Fu J, Zhang L,
Xiao T, Fang H and Ke X 
Application of the Still-Face Paradigm
in Early Screening for High-Risk
Autism Spectrum Disorder in Infants
and Toddlers. Front. Pediatr. :.
doi: ./fped..
Application of the Still-Face
Paradigm in Early Screening for
High-Risk Autism Spectrum Disorder
in Infants and Toddlers
Nana Qiu , Chuangao Tang , Mengyao Zhai , Wanqing Huang , Jiao Weng , Chunyan Li ,
Xiang Xiao , Junli Fu , Lili Zhang , Ting Xiao , Hui Fang  and Xiaoyan Ke *
 Nanjing Brain Hospital Afﬁliated to Nanjing Medical University, Nanjing, China,  School of Biological Science & Medical
Engineering, Southeast University, Nanjing, China,  College of Telecommunications & Information Engineering, Nanjing
University of Posts and Telecommunications, Nanjing, China,  Wuxi Children’s Hospital, Wuxi, China
Background: Although autism spectrum disorder (ASD) can currently be diagnosed at
the age of  years, age at ASD diagnosis is still  months or even later. In order to early
screening for ASD with more objective method, behavioral videos were used in a number
of studies in recent years.
Method:
The still-face paradigm (SFP) was adopted to measure the frequency and
duration of non-social smiling, protest behavior, eye contact, social smiling, and active
social engagement in high-risk ASD group (HR) and typical development group (TD)
. The HR group was follow-up until they were  years old
to conﬁrm ﬁnal diagnosis. Machine learning methods were used to establish models for
early screening of ASD.
Results:
During the face-to-face interaction (FF) episode of the SFP
, there were
statistically signiﬁcant differences in the duration and frequency of eye contact, social
smiling, and active social engagement between the two groups. During the still-face (SF)
episode, there were statistically signiﬁcant differences in the duration and frequency of
eye contact and active social engagement between the two groups. The  children in
the HR group were reclassiﬁed into two groups after follow-up: ﬁve children in the N-ASD
group who were not meet the criterion of ASD and  children in the ASD group. The
results showed that the accuracy of Support Vector Machine (SVM) classiﬁcation was
.% for the SF episode.
Conclusion: The use of the social behavior indicator of the SFP for a child with HR
before  years old can effectively predict the clinical diagnosis of the child at the age of 
years. The screening model constructed using SVM based on the SF episode of the SFP
was the best. This also proves that the SFP has certain value in high-risk autism spectrum
disorder screening. In addition, because of its convenient, it can provide a self-screening
mode for use at home.
Trial registration: Chinese Clinical Trial Registry, ChiCTR-OPC-.
Keywords: high-risk autism spectrum disorder, the Still-Face Paradigm, social behavior, machine learning, model
for early screening
Qiu et al.
Early Screening for High-Risk ASD
INTRODUCTION
Autism
spectrum
disorder
(ASD)
is
a
serious
neurodevelopmental disorder that starts in early childhood
and is characterized by social communication barriers, restricted
interests, repetitive stereotyped behaviors, and abnormalities in
perception . In recent years, epidemiological survey data on
the incidence of ASD showed that the prevalence rate increased
from . to .% in China . A large number of studies on
ASD have shown that early intervention helps improve patient
prognosis . However, age at ASD diagnosis is still  months
or even later . Therefore, early detection, early diagnosis and
eﬀective intervention are essential to achieve a better prognosis.
Understanding the early childhood behaviors of ASD will help
facilitate early intervention in infants and toddlers who are
suspected of having ASD and improve their prognosis, which has
very important social and economic implications.
The early social interaction between adults and children is
the basis of more complex social cognition. For children with
ASD, the lack of social abilities is especially prominent in their
early life. According to the  version of the Diagnostic and
Statistical Manual of Mental Disorders, ﬁfth edition ,
ASD symptoms are usually present in young children at the
age of – years. Occasionally, initial symptoms often involve
delayed language development, accompanied by a lack of social
interest or unusual social interactions, quirky play modes and
unusual communication patterns . A study by Barbaro and
Dissanayake on family videos and parental reports revealed early
warning signs in social interactions of children with ASD aged
 to  months old , which included lack of joint attention,
lack of eye contact, lack of social smiling, lack of social interest
and sharing, no response to calling name, lack of gestures, and
communication impairments . This study would focus on
the diﬀerences of early social behavior between HR group and
TD group. Previous studies on early behavioral abnormalities in
ASD were mostly in the form of retrospective interviews with
parents or scale-based evaluations, which are highly subjective
and unfavorable for widespread promotion. In recent years, an
increasing number of studies have adopted objective methods
involving video for using behavioral coding .
Tronick et al. proposed the still-face paradigm (SFP) to test
infants’ emotion regulation ability and social expectations in
social interaction . Early maternal-infant interaction is the
core basis of infants’ social emotion, emotion regulation, and
social and communication development . The maternal-
infant relationship is the ﬁrst relationship developed in early
childhood. Flexible and frequent interaction is the basis for
early childhood emotional organization, attention switching and
the emergence of social skills . In the early maternal-
infant interaction, non-verbal communication is dominant. In
addition, in this process, young children learn the rules of
social participation and expressing forms of social expectations,
which provide a social framework for future social interactions
and relationships . There were studies applied SFP in
emotional regulatory of ASD and they found most of children
with ASD employed more simple regulatory behavior and
less complex strategies . Additional, Cassel et al. also
found that there were diﬃculties for children with ASD to
develop socioemotional ability . In this study, the
SFP was used to measure social behavior of HR group and
TD group.
There were studies applied machine learning methods to build
model for early screening of ASD based on the characteristic
values of biological indicators, such as electro-encephalogram
 and brain images , and they found the accuracy was
more than %. And in order to improve the stability and
reliability of model for early screening of ASD, it is essential to
combining social behavioral indicators with biological indicators
in the subsequent research. In this study, we would try to
build a model for early screening of ASD based on social
behavioral indicators.
METHODS
Participants
Forty-ﬁve infants and toddlers with high-risk autism spectrum
disorder (HR) who sought treatment at the outpatient clinic of
Child Mental Health Research Center, Nanjing Brain Hospital
Aﬃliated to Nanjing Medical University, from December 
to December  were enrolled in the HR group, and  infants
and toddlers with typical development (TD) in the Nanjing area
were recruited during the same period and enrolled into the
control group (the TD group).
The inclusion criteria for the HR group were as follows:
 children with positive results based on the Modiﬁed
Checklist for Autism in Toddlers ;  pediatric
psychiatrist recognized that the children met the core criteria
of ASD in DSM- but the months age was under  months;
 children aged  to  months old;  children whose
primary caregiver was the mother; and  children whose
guardian(s) agreed to participate in this study. The exclusion
criteria for the HR group were as follows:  children with
genetic or metabolic diseases, such as Rett syndrome and
fragile X syndrome;  children with neurodevelopmental
disorders other than ASD, such as language development
disorders alone and intellectual disability;  children with
a clear history of craniocerebral trauma; and  children
with
a
history
of
nervous
system
diseases
and
serious
physical illnesses.
The inclusion criteria for the TD group were as follows:
 children with TD whose sex matched that of the children
in the HR group;  children aged  to  months old; 
children whose primary caregiver was the mother; and 
children whose guardian(s) agreed to participate in this study.
The exclusion criteria for the TD group were as follows: 
children who suﬀered from various types of neurodevelopmental
disorders and mental disorders;  children with a clear history
of craniocerebral trauma; and  children with a history of
nervous system diseases and serious physical illness.
This study was approved by the Medical Ethics Committee of
Nanjing Brain Hospital Aﬃliated to Nanjing Medical University
. All subjects’ guardians agreed to participate in
this study and signed informed consent forms.
Frontiers in Pediatrics | 

June  | Volume  | Article 
Qiu et al.
Early Screening for High-Risk ASD
Measure and Procedure
General Psychological Evaluation of the HR and TD
Groups
A self-guided general information questionnaire was used to
collect the general demographic data, past history, medication
history and family history of the study participants.
The Gesell Developmental Scale was used to assess the
developmental levels of all subjects after they were enrolled.
The Gesell Developmental Scale was used to evaluate the
developmental quotient (DQ) of children from  skill domains:
adaptive, gross motor, ﬁne motor, language and personal-social.
The severity of ASD symptoms in the HR group was
assessed using the Communication and Symbolic Behavior Scales
Developmental Proﬁle (CSBS-DP), the Childhood Autism Rating
Scale (CARS) and the Autism Behavior Checklist (ABC). The
CSBS-DP has  factor scores (social communication, language,
and symbolic behavior) and a total score. The lower the CSBS-
DP factor scores are, the more serious the ASD symptoms. The
CARS and the ABC only have a total score. The higher the total
scores are, the more severe the ASD symptoms.
Video of the Behaviors of the HR and TD Groups in
the SFP
The classic paradigm consists of three episodes:  the baseline
episode (face-to-face interaction, FF episode), during which the
mother and the child are required to have normal interactions;
 the still-face (SF) episode, during which the mother’s face is
required to present a neutral expression without any response to
the child’s action; and  the reunion episode, during which the
mother resumes normal interactions with the child. Currently,
the SFP can arouse children’s behavioral changes, e.g., a reduction
in eye gaze and positive facial emotion and an increase in
negative emotion when transitioning from the baseline episode
to the SF episode ; this eﬀect has been recognized and
termed the SF eﬀect. In relevant studies, researchers also found
that the reason for the generation of the SF eﬀect in infants
and toddlers is the disappearance of social responses, such
as eye contact, when their mothers show a still face; the
disappearance of social signals causes the appearance of negative
emotions in infants and toddlers . The ﬁrst two episodes
are often used in research in a randomly presented order. In
the past few decades, the basic settings of the SFP have been
used as a method to explore early childhood social behaviors
, which the reason why this study chosen the ﬁrst
two episodes.
In the present study, all participants and their mothers were
video recorded during the SFP in a designated observation room
at the time of enrollment. The mother was sitting opposite to the
child, interacted with the child for  min under ﬁxed instructions,
and then stopped the interaction and maintained a neutral
face for  min. Figure  shows the setup of the experimental
environment and procedure.
Infant and Caregiver Engagement Phases  and
Nichols et al.  deﬁne the coding indicators as follows: 
protest behavior - the infant shows facial expressions of anger
and frowns; the infant is upset, crying, arching the body, trying
to escape and expresses anger using gestures;  non-social
smiling—the infant smiles not at the mother but toward other
directions or at other objects;  eye contact—the infant looks
directly at the mother’s eyes or face instead of looking at the
camera or toward other directions;  social smiling—the infant
looks at the mother and takes the initiative to smile (initiating
a smile); after the mother smiles, the infant immediately
responds with a smile  active
social engagement—the infant displays happy facial expressions,
including a clear smile, occasional cooing or active vocalization,
laughing, or babbling, and looks at the mother to initiate
interactions (proactively initiate active social engagement), or the
infant has a positive response to the interaction initiated by the
mother (responding to active social engagement).
The Observer XT  behavioral observation and recording
analysis system was used for coding. Video coding was completed
by  trained graduate students, and the duration and frequency
of the  indicators during the FF and SF episodes were calculated.
The duration was measured using seconds (s) as the unit, and
frequency was measured using the number of times as the unit. A
total of  videos (∼%) were randomly selected to determine
intercoder consistency using the intraclass correlation coeﬃcient
(ICC). It was found that the  coders had high consistency: the
ICCs for protest behavior, non-social smiling, eye contact, social
smiling, and active social engagement were ., ., ., .,
and ., respectively.
FIGURE  | SFP for infants and toddlers with HR.
Frontiers in Pediatrics | 

June  | Volume  | Article 
Qiu et al.
Early Screening for High-Risk ASD
TABLE  | Comparison of the general conditions between the HR group and the
TD group (mean ± SD).
HR group
TD group
t/χ
P-value


Sex
−.
.
Male


Female


Age
(months)
DQ
.
<.
Adaptive
. ± .
−.
<.
Gross motor
. ± .
−.
.
Fine motor
. ± .
−.
.
Language
. ± .
−.
<.
Personal-social
. ± .
−.
<.
HR, high-risk autism spectrum disorder; TD, typical development; DQ, developmental
quotient of the Gesell Developmental Scale.
Diagnostic Evaluation of Children in the HR Group at
 Years of Age
At  years of age, the children in the HR group were evaluated
using the Autism Diagnosis Interview-R (ADI-R) and the
Autism Diagnostic Observation Schedule (ADOS). Two pediatric
psychiatrists then clinically diagnosed the children based on the
ASD diagnostic criteria in the DSM- and the aforementioned
evaluation results. All participants with conﬁrmed ASD (the ASD
group) reached the cut-oﬀscores for ASD diagnosis for both
evaluation scales.
Analytic Approach
The sex diﬀerence between the HR and TD groups was compared
using the χ test. The diﬀerences between the HR group and
the TD group in social behaviors were determined using the
independent samples t-test. The correlations of social behaviors
in the HR group with age, DQ, and symptom severity were
analyzed using Pearson’s rho. Finally, models for early ASD
screening were constructed using machine learning methods
based on ASD group and TD group, and the HR group
(contained  children with conﬁrmed ASD and  children
with not met the criterions of ASD) would be used to veriﬁed
the eﬀectiveness of models for early ASD screening. P < .
indicated that the diﬀerence was statistically signiﬁcant.
RESULTS
Comparison of the General Conditions
Between the HR Group and the TD Group
Age (months), adaptive DQ, language DQ, ﬁne motor DQ,
and personal-social DQ were signiﬁcantly diﬀerent 
between the HR group and the TD group, while sex DQ and gross
motor DQ were not signiﬁcantly diﬀerent  between the
two groups. See Table  and Figure S.
Comparison of the Social Behaviors
Between the HR and TD Groups During
Different SFP Episodes
During the FF episode of the SFP, there were statistically
signiﬁcant diﬀerences in the duration and frequency of eye
contact, social smiling, and active social engagement between the
HR group and the TD group (t =-., −., −., −.,
−., −.; all P < .), while the diﬀerences in length and
frequency of non-social smiling and protest behaviors between
the HR group and the TD group were not statistically signiﬁcant
. See Figures A,B.
During the SF episode of the SFP, there were statistically
signiﬁcant diﬀerences in the duration and frequency of eye
contact and active social engagement between the HR group
and the TD group (t = −., −., −., −.; all P <
.), while the diﬀerences in length and frequency of non-
social smiling, protest behaviors and social smiling between the
HR group and the TD group were not statistically signiﬁcant
. See
Figures C,D.
Analysis of the Correlation of Social
Behaviors With Each Factor for the HR and
TD Groups During the Different SFP
Episodes
In the TD group, the frequency of eye contact was signiﬁcantly
positively correlated with gross motor DQ for the SF episode
; the other indicators had no statistically signiﬁcant
correlations with age and DQs .
During the FF episode, the duration of eye contact,
social smiling, and active social engagement for the HR
group
were
signiﬁcantly
positively
correlated
with
the
adaptive DQ, and the duration of eye contact and the gross
motor
DQ
were
also
signiﬁcantly
positively
correlated.
During the SF episode, the duration of eye contact was
signiﬁcantly
positively
correlated
with
the
language
DQ
and the ﬁne motor DQ, and the duration of active social
engagement and the language DQ were also signiﬁcantly
positively correlated . The other indicators
had no signiﬁcant correlation with age and DQs (P > .;
Table ).
The analysis of the correlation between clinical ASD symptom
severity and social behavior indicators suggested that there was
no statistically signiﬁcant diﬀerence between social behavior
indicators and clinical symptom severity during the FF episode
; during the SF episode, the duration of eye contact
was positively correlated with symbolic behavior factor score and
the total CSBS-DP score, the duration of active social engagement
was positively correlated with the total CSBS-DP score, and the
frequency of eye contact was positively correlated with the social
communication factor score, the language factor score, and the
total CSBS-DP score . The other indicators had
no signiﬁcant correlation with the symptom severity (P > .;
Table ).
Frontiers in Pediatrics | 

June  | Volume  | Article 
Qiu et al.
Early Screening for High-Risk ASD
FIGURE  | Comparison of the differences in social behaviors between the HR group and the TD group during the different SFP episodes. (A) Comparison of the
differences in the duration of social behaviors during the FF episode. (B) Comparison of the differences in the frequency of social behaviors during the FF episode. (C)
Comparison of the differences in the duration of social behavior during the SF episode. (D) Comparison of the differences in the frequency of social behaviors during
the SF episode. HR, high-risk autism spectrum disorder; TD, typical development; SFP
, still-face paradigm; **P < ..
Using Machine Learning to Construct
Models for Early ASD Screening
Through the follow-up of the HR group and the re-diagnosis of
the HR group at  years of age, we found that  ( female and
 males) out of the  infants and toddlers with HR no longer
met the diagnostic criteria for ASD [the non-ASD (N-ASD)
group]; the other  children still met the diagnostic standard
(the ASD group). And then the models for early ASD screening
were constructed using machine learning methods based on ASD
group and TD group.
We used the duration and frequency of eye contact, active
social engagement, and social smiling during the FF episode as
well as the duration and frequency of eye contact and active social
engagement during SF episode as the behavioral characteristics
of the samples. The ASD group and the TD group were used
as the samples. In the classiﬁcation of ASD ( samples) and
its comparison group of TD ( samples), each subject within
these  samples was used for testing the model that was trained
on the rest  samples. Then, the prediction labels of each test
sample corresponding to  classiﬁcation models were collected
to calculate the overall accuracy on this dataset. There were
 training samples, and  sample was selected for testing.
The test set data were classiﬁed using the following machine
learning methods: support vector machine (SVM), naïve Bayes
and random forest. And the Python platform was used for
analyses. The Random Forest Classiﬁer, Gaussian NB and SVM
functions in Scikit-learn toolbox developed by Python were
employed for building classiﬁcation models, respectively.
The results showed that the accuracy of Bayesian classiﬁcation
was .% for the FF episode and .% for the SF episode, and
Frontiers in Pediatrics | 

June  | Volume  | Article 
Qiu et al.
Early Screening for High-Risk ASD
TABLE  | Analysis of the correlation of social behavior with age and DQ in the TD group during different SFP episodes (r-value).
Episodes and indicators
Age
(months)
Adaptive DQ
Gross motor DQ
Fine motor DQ
Language DQ
Personal-social DQ
Duration during the FF episode (s)
Eye contact
−.
.
Social smiling
−.
−.
−.
−.
−.
−.
Active social engagement
−.
−.
−.
−.
.
−.
Frequency during the FF episode (number of times)
Eye contact
−.
.
−.
.
Social smiling
.
−.
−.
−.
.
−.
Active social engagement
.
−.
−.
.
−.
Duration during the SF episode (s)
Eye contact
.
−.
−.
−.
.
−.
Social smiling
.
−.
.
Active social engagement
−.
−.
.
−.
.
−.
Frequency during the SF episode (number of times)
Eye contact
.
−.
.
−.
Active social engagement
−.
−.
.
−.
.
−.
TD, typical development; SFP
, still-face paradigm; DQ, developmental quotient of the Gesell Developmental Scale; *P < ..
TABLE  | Analysis of the correlation between social behaviors and age (months), DQ, and clinical symptoms of the HR group during different SFP episodes (r-value).
Episodes and indicators
Age
(months)
DQ
CSBS-DP
CARS
ABC
Adaptive Gross
motor
Fine
motor
Language
Personal-
social
Social
communication
factor
Language
factor
Symbolic
behavior factor
Total
score
Duration during the FF episode (s)
Eye contact
−.
.
−.
−.
.
−.
Social smiling
−.
.
−.
−.
.
−.
Active social engagement
−.
.
−.
.
−.
−.
Frequency during the FF episode (number of times)
Eye contact
−.
.
−.
.
−.
−.
Social smiling
−.
.
−.
−.
−.
−.
−.
Active social engagement
−.
.
−.
.
−.
.
−.
.
Duration during the SF episode (s)
Eye contact
−.
.*
−.
−.
Active social engagement
.*
−.
−.
Frequency during the SF episode (number of times)
Eye contact
−.
.*
−.
−.
Active social engagement
−.
.
−.
.
−.
.
−.
HR, high-risk autism spectrum disorder; SFP
, still-face paradigm; DQ, developmental quotient of the Gesell Developmental Scale; CSBS-DP
, Communication and Symbolic Behavior
Scales Developmental Proﬁle; CARS, Childhood Autism Rating Scale; ABC, Autism Behavior Checklist; *P < ., **P < ..
that the accuracy of random forest classiﬁcation was .% for
the FF episode and .% for the SF episode. And the accuracy
of SVM classiﬁcation was .% for FF episode and .%
for the SF episode, which has higher accuracy. The confusion
matrix was showed in Table . And in order to ﬁnd the age
diﬀerences between the kids who were not picked up by the
machine learning, ASD group and TD group (total  kids) was
divided into  groups with month age, respectively, –, –,
–, and – .
Subsequently, the eﬀectiveness of the SVM classiﬁcation
model was veriﬁed in  children with conﬁrmed ASD and
 N-ASD children in the HR group. Unfortunately, even
though the average classiﬁcation accuracy of SVM was more
than %, the  N-ASD was not classiﬁed correctly. The lack
of enough samples of N-ASD resulted in a large imbalance
between two groups. Such limited samples with only 
indicators within each sample could be a possible reason for
low recall rate of N-ASD samples. Therefore, it is essential to
expand the sample of N-ASD in future study to verify the
eﬀectiveness of the SVM classiﬁcation model. What’s more, it
is necessary to build more eﬀective machine learning model in
following study.
Frontiers in Pediatrics | 

June  | Volume  | Article 
Qiu et al.
Early Screening for High-Risk ASD
TABLE  | The confusion matrix of SVM.
ASD
TD
FF episode
ASD


TD


SF episode
ASD


TD


ASD, autism spectrum disorder; TD, typical development; FF episode, face to face
interaction; SF episode, still-face episode.
DISCUSSION
Face-to-face interaction constitutes the beginning of early
childhood learning and deﬁning social interaction, and face-
to-face interactions between infants and toddlers and primary
caregivers allows the former to learn  the meaning of self-
expression behaviors;  the characteristics of people with whom
they have a close relationship; and  emotional information
and the perception of local culture, primary caretaker identity
and self-identity . Emotion regulation is an important link
of early childhood development milestones and is closely related
to primary caregivers . Studies have shown that strong
emotion regulation abilities in children is associated with good
development and can predict social emotional outcomes at later
stages  and that weak emotion regulation abilities during
early childhood are associated with behavioral problems and
development problems at later stages . Especially
from  to  months old, infants quickly learn how to regulate
emotions through face-to-face interactions; therefore, the quality
of infant-mother interactions is crucial at this stage .
Through comparative analysis of the diﬀerences in social
behaviors between infants and toddlers with HR and infants and
toddlers with TD before the age of , compared with infants and
toddlers with TD, infants and toddlers with HR exhibited shorter
durations and lower frequencies of eye contact, social smiling,
and active social engagement during the FF episode of the SFP.
This ﬁnding is consistent with those of most studies (, –
). In the SF episode, compared with infants and toddlers with
TD, infants and toddlers with HR showed shorter durations and
lower frequencies of eye contact and active social engagement,
which means that although children with HR exhibited behaviors
to attract the attention of the non-responsive mothers, their
ability to initiate active social engagement was lower than that
of children with TD. For infants with HR, avoiding eye contact
results in a low-quality infant-mother interaction; therefore, the
development of emotion regulation abilities in these infants and
toddlers may be delayed, which explains to some extent the causes
of the delayed development of social smiling and active social
engagement in children with ASD.
From the results of the correlation analysis, there was a
diﬀerence between age and the developmental level and social
behaviors of some infants and toddlers in the HR group and
the TD group, but the diﬀerence was not representative, i.e., the
age and developmental level of the infants and toddlers did not
inﬂuence their social behaviors under general conditions. For
infants and toddlers with HR, the analysis of the correlation
between clinical symptoms and social behaviors showed that
there was no correlation between social behaviors and symptom
severity during the FF episode of the SFP. During the SF
episode, the duration of eye contact by infants and toddlers
with HR was positively correlated with the symbolic behavior
factor score and the total CSBS-DP score; the duration of social
smiling was positively correlated with the social communication
factor score and the total CSBS-DP score; and the duration of
active social engagement was positively correlated with the social
communication factor score, the symbolic behavior factor score
and the total CSBS-DP score; and the frequency of eye contact
was positively correlated with the social communication factor
score, the language factor score and the total CSBS-DP score. The
results indicated that the more ﬂexible and appropriate the eye
contact and active social engagement of the infants and toddlers
with HR, the less severe were the ASD symptoms, which is also
consistent with the results of most studies . Although the
social behaviors of infants and toddlers with ASD develop over
time, their development level is limited, the gap between infants
and toddlers with ASD and infants and toddlers with TD also
increases over time, and infants and toddlers with ASD develop
more clinical symptoms of ASD.
The SFP presents changes in children’s expressions, emotions
and behaviors in a more microscopic coding mode. On the
basis of setting a normal interaction, the SFP provides a social
challenge scenario by setting the SF episode, during which the
social signals of the mother are completely missing during the
period. For typically developing children, their inability to adapt
to the loss of social signals stimulated their ability to initiate
social interactions, express emotions, regulate emotions, and bear
stress. The analysis of the above results showed that the social
behaviors in infants and toddlers with HR, especially their social
behaviors during the SF episode of the SFP, were associated with
the core ASD symptoms. According to the extreme male brain
theory of autism  the toddlers with ASD are more prone
to over systematization and thus have lower empathic ability than
do TD toddlers, making them more prone to deﬁciencies in social
and verbal communication. By examining the diﬀerences in
social behaviors between the infants and toddlers in the HR group
and the TD group, we found that although there were many
diﬀerences in the abnormal social behaviors between infants and
toddlers with ASD and infants and toddlers with TD during the
FF and SF episodes of the SFP, the social behaviors of infants
and toddlers with HR, such as eye contact and active social
engagement, during the SF episode (a frustration scenario), were
signiﬁcantly correlated with core communication impairments,
such as the social communication factor score, the symbolic
behavior factor score and the total CSBS-DP score. That is, the SF
episode of the SFP can better induce the social communication
impairments in infants and toddlers with ASD. Markram et al.
 proposed the intense world theory, suggesting that an
excessively active brain would excessively amplify ordinary
sensory experiences, causing the toddlers with ASD to be in a
state of fragmented sensory information and to be overloaded,
and because of such a strong reaction, the intense emotions
perceived by them from the surrounding environment causes
social withdrawal, resulting in a series of autism symptoms such
Frontiers in Pediatrics | 

June  | Volume  | Article 
Qiu et al.
Early Screening for High-Risk ASD
as social communication impairments and stereotyped behaviors.
Therefore, facing social communication challenges such as the SF
episode, infants and toddlers with TD attempted to arouse their
mothers’ responses by pointing and social smiling. In contrast,
for infants and toddlers with HR, even for those who had higher
function, they may have had good interactions with their mothers
during the FF episode, but when their mothers did not respond,
social pressure was reduced. They made fewer attempts or shorter
attempts to initiate social interactions.
In addition, some studies have shown that when responding
to emotional reactions, children with ASD have worse emotion
regulation abilities and more unreasonable expression and are
more likely to show negative emotions . However, in this
study, the negative emotions (protest behavior and non-social
smiling) in infants and toddlers with HR and TD were not
diﬀerent, and there were no diﬀerences during the frustration
scenario, i.e., when the mothers used still faces. Further validation
and discussion are needed in future studies.
Based on the re-diagnosis and regrouping of the children
at  years of age, machine learning methods, including SVM,
naïve Bayes and random forest, were used to construct models
for early ASD screening. And we found the classiﬁcation model
established using the SVM had the best performance, especially it
was found to have better screening ability and reliability for the SF
episode. Unfortunately, when the model was selected and applied
to the diﬀerential diagnosis of the children in the HR group, the
 N-ASD was not classiﬁed correctly. And also, it is the goal to
identify N-ASD from ASD group in our future eﬀorts. Since the
age diﬀerences between the ASD group and TD group, we added
the Figure S, in which we divided ASD group and TD group
(total  kids) into  groups with month age, respectively, –,
–, –, and –, and we found there were not regularity
between the classiﬁcation accuracy vs. month age.
Similarly, there are limitations in this study. First, because
infants
and
toddlers
with
ASD
generally
have
delayed
development, the  groups were not matched by age to
make the development level of the HR group the same as that
of the TD group. Second, the sample size was small. In view of
these limitations, we will continue to expand the sample size
in future studies to further verify the ﬁndings under controlled
physiological and psychological ages. We hope that the SFP will
be widely applied for the early ASD screening and that a more
objective, standardized and convenient way for self-screening at
home will be achieved.
DATA AVAILABILITY STATEMENT
All datasets generated for this study are included in the
article/Supplementary Material.
ETHICS STATEMENT
The studies involving human participants were reviewed and
approved by the Medical Ethics Committee of Nanjing Brain
Hospital Aﬃliated to Nanjing Medical University. Written
informed consent to participate in this study was provided
by
the
participants’
legal
guardian/next
of
kin.
Written
informed consent was obtained from the individual(s), and
minor(s)’ legal guardian/next of kin, for the publication
of any potentially identiﬁable images or data included in
this article.
AUTHOR CONTRIBUTIONS
NQ and XK designed experiments. NQ, CT, MZ, JW, CL, XX,
JF, LZ, TX, and HF carried out experiments. NQ, CT, and WH
analyzed experimental results. NQ wrote the manuscript.
FUNDING
Key Research (Social Development) Foundation of Jiangsu
Province ; the Fifth ‘‘ High Level of Cultivating
Talents Projects’’ in Jiangsu Province.
SUPPLEMENTARY MATERIAL
The Supplementary Material for this article can be found
online
at:

./full#supplementary-material
REFERENCES
. Marty MA, Segal DL. Diagnosis and Statistical Manual of Mental Disorders
DSM-. American Psychiatric Association, W.D.A.P. Association .
. Kim YS, Leventhal BL, Koh Y-J, Fombonne E, Laska E, Lim E-C, et al.
Prevalence of autism spectrum disorders in a total population sample. Am J
Psychiatry.  :–. doi: ./appi.ajp..
. Clark MLE, Vinen Z, Barbaro J, Dissanayake C. School age outcomes of
children diagnosed early and later with autism spectrum disorder. J Autism
Dev Disord.  :–. doi: ./s---x
. Daniels AM, Mandell DS. Explaining diﬀerences in age at autism spectrum
disorder diagnosis: a critical review. Autism Int J Res Pract.  :.
doi: ./
. Barbaro J, Dissanayake C. ASD in infancy and toddlerhood: a review of
the evidence on early signs early identiﬁcation tools and early diagnosis.
J Dev Behav Pediatr.  :–. doi: ./DBP.be
baff
. Adrien JL, Lenoir P, Martineau J, Perrot A, Hameury L, Larmande C,
et al. Blind ratings of early symptoms of autism based upon family
home movies. J Am Acad ChildAdolesc Psychiatry.  :–.
doi: ./--
. Wetherby AM, Woods J, Allen L, Cleary J, Dickinson H, Lord C. Early
indicators of autism spectrum disorders in the second year of life. J Autism
Dev Disord.  :–. doi: ./s---y
. Barbaro
J,
Dissanayake
C.
Early
markers
of
autism
spectrum
disorders
in
infants
and
toddlers
prospectively
identiﬁed
in
the
Social Attention and Communication Study. Autism.  :–.
doi: ./
. Zwaigenbaum L, Bauman ML, Stone WL, Yirmiya N, Estes A, Hansen RL,
et al. Early identiﬁcation of autism spectrum disorder: recommendations
for practice and research. Pediatrics.  :–. :S–.
doi: ./peds.-C
. Cassel TD, Messinger DS, Ibanez LV, Haltigan JD, Acosta SI, Buchman AC.
Early social and emotional communication in the infant siblings of children
Frontiers in Pediatrics | 

June  | Volume  | Article 
Qiu et al.
Early Screening for High-Risk ASD
with autism spectrum disorders: an examination of the broad phenotype. J
Autism Dev Disord.  :–. doi: ./s---
. Heimann M, Laberg KE, Nordøen B. Imitative interaction increases social
interest and elicited imitation in non-verbal children with autism. Infant Child
Dev.  :–. doi: ./icd.
. Harker CM, Ibanez LV, Nguyen TP, Messinger DS, Stone WL. The eﬀect of
parenting style on social smiling in infants at high and low risk for ASD. J
Autism Dev Disord.  :–. doi: ./s---y
. Tronick E, Als H, Adamson L. The infant’s response to entrapment
between contradictory messages in face-to-face interaction. J Am Acad Child
Psychiatry.  :–. doi: ./S--
. Mantis I, Stack DM, Ng L, Serbin LA, Schwartzman AE. Mutual touch during
mother–infant face-to-face still-face interactions: inﬂuences of interaction
period and infant birth status. Infant Behav Dev.  :–.
doi: ./j.infbeh...
. Carey
WB.
From
neurons
to
neighborhoods:
the
science
of
early
childhood development. Aust N Zeal J Psychiatry.  :–.
doi: ./--
. Mercer J. Understanding Attachment: Parenting, Child Care, and Emotional
Development Praeger.  :–. doi: ./X
. Yirmiya N, Gamliel I, Pilowsky T, Feldman R, Baron-Cohen S, Sigman M. The
development of siblings of children with autism at  and  months: social
engagement, communication, and cognition. J Child Psychol Psychiatry. 
:. doi: ./j.-...x
. Ostfeld-Etzion
S,
Golan
O,
Hirschler-Guttenberg
Y,
Zagoory-Sharon
O,
Feldman
R.
Neuroendocrine
and
behavioral
response
to
social
rupture and repair in preschoolers with autism spectrum disorders
interacting
with
mother
and
father.
Mol
Autism.

:–.
doi: ./s---
. Merin N, Young GS, OzonoﬀS, Rogers SJ. Visual ﬁxation patterns during
reciprocal social interaction distinguish a subgroup of -month-old infants
at-risk for autism from comparison infants. J Autism Dev Disord. 
:–. doi: ./s---
. Jamal W, Das S, Oprescu IA, Maharatna K, Apicella F, and Sicca F.
Classiﬁcation of autism spectrum disorder using supervised learning of brain
connectivity measures extracted from synchrostates. J. Neural Eng. 
:. doi: ./-///
. Abraham A, Milham M, Martino AD, Craddock RC, Samaras D, Thirion
B, and Varoquaux G. . Deriving reproducible biomarkers from multi-
site resting-state data: An Autism-based example. Neuroimage. :.
doi: ./j.neuroimage...
. Robins DL, Fein D, Barton ML, Green JA. The modiﬁed checklist for autism
in toddlers: an initial study investigating the early detection of autism and
pervasive developmental disorders. J Autism Dev Disord.  :–.
doi: ./A:
. Legerstee M, Markova G. Intentions make a diﬀerence: infant responses to
still-face and modiﬁed still-face conditions. Infant Behav Dev.  :–
. Yato Y, Kawai M, Negayama K, Sogon S, Tomiwa K, Yamamoto H. Infant
responses to maternal still-face at  and  months. Infant Behav Dev. 
:–. doi: ./j.infbeh...
. Montirosso R, Provenzi L, Tavian D, Morandi F, Bonanomi A, Missaglia S,
et al. Social stress regulation in -month-old infants: contribution of maternal
social engagement and infants’ -HTTLPR genotype. Early Hum Dev. 
:–. doi: ./j.earlhumdev...
. Provenzi L, Fumagalli M, Bernasconi F, Sirgiovanni I, Morandi F, Borgatti
R, et al. Very preterm and full-term infants’ response to socio-emotional
stress: the role of postnatal maternal bonding. Infancy.  :–.
doi: ./infa.
. Tronick EZ, Messinger DS, Weinberg MK, Lester BM, Lagasse L, Seifer R,
et al. Cocaine exposure is associated with subtle compromises of infants’ and
mothers’ social-emotional behavior and dyadic features of their interaction
in the face-to-face still-face paradigm. Dev Psychol.  :–.
doi: ./-...
. Nichols CM, Ibanez LV, Foss-Feig JH, Stone WL. Social smiling and its
components in high-risk infant siblings without later ASD symptomatology. J
Autism Dev Disord.  :–. doi: ./s---
. Calkins SD, Dedmon SE. Physiological and behavioral regulation in two-year-
old children with aggressive/destructive behavior problems. J Abnorm Child
Psychol.  :–. doi: ./A:
. Schultz
D,
Izard
CE,
Ackerman
BP,
Youngstrom
EA.
Emotion
knowledge
in
economically
disadvantaged
children:
self-regulatory
antecedents
and
relations
to
social
diﬃculties
and
withdrawal.
Dev
Psychopathol.

:–.
doi:
./S

. Denham SA, Blair KA, Demulder E, Levitas J, Sawyer K, Auerbach-Major S,
et al. Preschool emotional competence: pathway to social competence? Child
Dev.  :. doi: ./-.
. Eisenberg N, Zhou Q, Losoya SH, Fabes RA, Shepard SA, Murphy BC,
et al. The relations of parenting, eﬀortful control, and ego control
to
children’s
emotional
expressivity.
Child
Dev.

:–.
doi: ./-.
. Calkins
SD,
Fox
NA.
Self-regulatory
processes
in
early
personality
development: a multilevel approach to the study of childhood social
withdrawal
and
aggression.
Dev
Psychopathol.

:–.
doi: ./SX
. Supplee
LH,
Skuban
EM,
Shaw
DS,
Prout
J.
Emotion
regulation
strategies and later externalizing behavior among European American
and
African
American
children.
Dev
Psychopathol.

:.
doi: ./S
. Haley DW, Stansbury K. Infant stress and parent responsiveness: regulation
of physiology and behavior during still-face and reunion. Child Dev. 
:–. doi: ./-.
. OzonoﬀS, Iosif AM, Baguio F, Cook IC, Hill MM, Hutman T, et al.
A prospective study of the emergence of early behavioral signs of
autism. J Am Acad Child Adolesc Psychiatry.  :–e.
doi: ./j.jaac...
. Lambert-Brown BL, Mcdonald NM, Mattson WI, Martin KB, Ibanez LV, Stone
WL, et al. Positive emotional engagement and autism risk. Dev Psychol. 
:–. doi: ./a
. Chen X, Zou X, Chen K, Cen C, Cheng S. Analysis of early symptoms of
autism spectrum disorder children based on three-minute videos. Chin J Appl
Clin Pediatr.  :–. doi: ./cma.j.issn.-X...
. Cliﬀord SM, Dissanayake C. The early development of joint attention
in
infants
with
autistic
disorder
using
home
video
observations
and
parental
interview.
J
Autism
Dev
Disord.

:–.
doi: ./s---
. Parlade MV, Iverson JM. The development of coordinated communication
in
infants
at
heightened
risk
for
autism
spectrum
disorder.
J
Autism
Dev
Disord.

:–.
doi:
./s--
-z
. Campbell SB, Leezenbaum NB, Mahoney AS, Moore EL, Brownell CA.
Pretend play and social engagement in toddlers at high and low genetic
risk for autism spectrum disorder. J Autism Dev Disord.  :–.
doi: ./s---y
. Baron-Cohen
S.
Autism:
the
empathizing-systemizing
(E-S)
theory.
Ann N Y Acad Sci.  :–. doi: ./j.-..
. Baron-Cohen S, Lombardo MV, Auyeung B, Ashwin E, Knickmeyer
R.
Why
are
autism
spectrum
conditions
more
prevalent
in
males?
PLoS
Biol.

:e.
doi:
./journal.pbio.

. Markram H, Tania R, Kamila M. The intense world syndrome–an
alternative
hypothesis
for
autism.
Front
Neurosci.

:–.
doi: ./neuro.....
. Samson AC, Huber O, Gross JJ. Emotion regulation in Asperger’s
syndrome
and
high-functioning
autism.
Emotion.

:–.
doi: ./a
Conﬂict of Interest: The authors declare that the research was conducted in the
absence of any commercial or ﬁnancial relationships that could be construed as a
potential conﬂict of interest.
Copyright ©  Qiu, Tang, Zhai, Huang, Weng, Li, Xiao, Fu, Zhang, Xiao, Fang
and Ke. This is an open-access article distributed under the terms of the Creative
Commons Attribution License (CC BY). The use, distribution or reproduction in
other forums is permitted, provided the original author(s) and the copyright owner(s)
are credited and that the original publication in this journal is cited, in accordance
with accepted academic practice. No use, distribution or reproduction is permitted
which does not comply with these terms.
Frontiers in Pediatrics | 

June  | Volume  | Article 
RESEARCH ARTICLE
Mobile detection of autism through machine
learning on home video: A development and
prospective validation study
Qandeel TariqID, Jena DanielsID, Jessey Nicole SchwartzID, Peter WashingtonID,
Haik Kalantarian, Dennis Paul WallID*
 Department of Pediatrics, Division of Systems Medicine, Stanford University, California, United States of
America,  Department of Biomedical Data Science, Stanford University, California, United States of America
* 
Abstract
Background
The standard approaches to diagnosing autism spectrum disorder (ASD) evaluate between
 and  behaviors and take several hours to complete. This has in part contributed to
long wait times for a diagnosis and subsequent delays in access to therapy. We hypothesize
that the use of machine learning analysis on home video can speed the diagnosis without
compromising accuracy. We have analyzed item-level records from  standard diagnostic
instruments to construct machine learning classifiers optimized for sparsity, interpretability,
and accuracy. In the present study, we prospectively test whether the features from these
optimized models can be extracted by blinded nonexpert raters from -minute home videos
of children with and without ASD to arrive at a rapid and accurate machine learning autism
classification.
Methods and findings
We created a mobile web portal for video raters to assess  behavioral features (e.g., eye
contact, social smile) that are used by  independent machine learning models for identify-
ing ASD, each with >% accuracy in cross-validation testing and subsequent independent
validation from previous work. We then collected  short home videos of children with
autism (mean age =  years  months, SD =  years  months) and  videos of typically
developing children (mean age =  years  months, SD =  year  months). Three raters
blind to the diagnosis independently measured each of the  features from the  models,
with a median time to completion of  minutes. Although several models (consisting of alter-
nating decision trees, support vector machine [SVM], logistic regression (LR), radial kernel,
and linear SVM) performed well, a sparse -feature LR classifier  yielded the highest
accuracy (area under the curve ) across all ages tested. We
used a prospectively collected independent validation set of  videos ( ASD and  non-
ASD) and  independent rater measurements to validate the outcome, achieving lower but
comparable accuracy (AUC: % ). Finally, we applied LR to the -
PLOS Medicine | 
November , 
 / 
a
a
a
a
a
OPEN ACCESS
Citation: Tariq Q, Daniels J, Schwartz JN,
Washington P, Kalantarian H, Wall DP 
Mobile detection of autism through machine
learning on home video: A development and
prospective validation study. PLoS Med :
e. 
pmed.
Academic Editor: Suchi Saria, Johns Hopkins
University, UNITED STATES
Received: June , 
Accepted: October , 
Published: November , 
Copyright: ©  Tariq et al. This is an open
access article distributed under the terms of the
Creative Commons Attribution License, which
permits unrestricted use, distribution, and
reproduction in any medium, provided the original
author and source are credited.
Data Availability Statement: The de-identified data
have been made available at the following github
repository and include the primary dataset and the
validation dataset: 
video_phenotyping_autism_plos/tree/master/
datasets. The code has been made available at the
following github repository and instructions on
how to run each classifier have been provided:

autism_plos
video-feature matrix to construct an -feature model, which achieved . AUC (% CI
.–.) on the held-out test set and . on the validation set of  videos. Validation on
children with an existing diagnosis limited the ability to generalize the performance to undi-
agnosed populations.
Conclusions
These results support the hypothesis that feature tagging of home videos for machine learn-
ing classification of autism can yield accurate outcomes in short time frames, using mobile
devices. Further work will be needed to confirm that this approach can accelerate autism
diagnosis at scale.
Author summary
Why was this study done?
• Autism has risen in incidence by approximately % since  and now impacts at
least  in  children in the United States.
• The current standard for diagnosis requires a direct clinician-to-child observation and
takes hours to administer.
• The sharp rise in incidence of autism, coupled with the un-scalable nature of the stan-
dard of care (SOC), has created strain on the healthcare system, and the average age of
diagnosis remains around . years,  years past the time when it could be reliably
diagnosed.
• Mobile measures that scale could help to alleviate this strain on the healthcare system,
reduce waiting times for access to therapy and treatment, and reach underserved
populations.
What did the researchers do and find?
• We applied  machine learning models to  two-minute home videos of children with
and without autism diagnosis to test the ability to reliably detect autism on mobile
platforms.
• Three nonexpert raters measured  behavioral features needed for machine learning
classification by the  models in approximately  minutes.
• Leveraging video ratings, a machine learning model with only  features achieved %
unweighted average recall (UAR) on  videos and UAR = % on a different and
independently evaluated set of  videos, with UAR = % on children at or under .
• The above machine learning process of rendering a mobile video diagnosis quickly cre-
ated a novel collection of labeled video features and a new video feature–based model
with >% accuracy.
Mobile detection of autism through machine learning on home video
PLOS Medicine | 
November , 
 / 
Funding: The work was supported in part by funds
to DPW from NIH (REB- &
RHD-), The Hartwell Foundation, Bill
and Melinda Gates Foundation, Coulter Foundation,
Lucile Packard Foundation, and program grants
from Stanford University’s Human Centered
Artificial Intelligence Program, Precision Health and
Integrated Diagnostics Center (PHIND), Beckman
Center, Bio-X Center, Predictives and Diagnostics
Accelerator, and the Child Health Research
Institute. We also received philanthropic support
from Bobby Dekesyer and Peter Sullivan. No
funding bodies had any role in study design, data
collection and analysis, decision to publish, or
preparation of the manuscript.
Competing interests: I have read the journal’s
policy and the authors of this manuscript have the
following competing interests: DPW is the scientific
founder of Cognoa, a company focused on digital
pediatric healthcare; the approach and findings
presented in this paper are independent from/not
related to Cognoa. All other authors have declared
no competing interests exist.
Abbreviations: ADI-R, Autism Diagnostic
Interview-Revised; ADOS, Autism Diagnostic
Observation Schedule; ADTree, alternating decision
tree; ADTree, -feature alternating decision tree;
ADTree, -feature alternating decision tree; ASD,
autism spectrum disorder; AUC, area under the
curve; AUC-ROC, area under the receiver operating
characteristic curve; BID, Balanced Independent
Dataset; IRA, interrater agreement; LR, logistic
regression; LR, -feature logistic regression
classifier; LR, -feature logistic regression
classifier; LR, -feature logistic regression
classifier; LR-EN-VF, logistic regression with an
elastic net penalty; ROC, receiver operating
characteristic; SOC, standard of care; SVM,
support vector machine; SVM, -feature
support vector machine; SVM, -feature
support vector machine; SVM, -feature support
vector machine; UAR, unweighted average recall.
What do these findings mean?
• Short home videos can provide sufficient information to run machine learning classifi-
ers trained to detect children with autism from those with either typical or atypical
development. Features needed by machine learning models designed to detect autism
can be identified and measured in home videos on mobile devices by nonexperts in
timeframes close to the total video length and under  minutes.
• The machine learning models provide a quantitative indication of autism risk that pro-
vides more granularity than a binary outcome to flag inconclusive cases, potentially add-
ing value for use in clinical settings, e.g., for triage.
• The process of mobile video analysis for autism detection generates a growing matrix of
video features that can be used to construct new machine learning models that may
have higher accuracy for autism detection in home video.
• Clinical prospective testing in general pediatric settings on populations not yet diag-
nosed will be needed. However, these results support the possibility that mobile video
analysis with machine learning may enable rapid autism detection outside of clinics to
reduce waiting periods for access to care and reach underserved populations in regions
with limited healthcare infrastructure.
Introduction
Neuropsychiatric disorders are the single greatest cause of disability due to noncommunicable
disease worldwide, accounting for % of the global burden of disease . A significant con-
tributor to this metric is autism spectrum disorder (ASD, or autism), which has risen in inci-
dence by approximately % since   and now impacts  in  children in the United
States . ASD is arguably one of the largest pediatric health challenges, as supporting an
individual with the condition costs up to $. million during his/her lifespan in the US  and
over $ billion annually in US healthcare costs .
Like most mental health conditions, autism has a complex array of symptoms  that are
diagnosed through behavioral exams. The standard of care (SOC) for an autism diagnosis uses
behavioral instruments such as the Autism Diagnostic Observation Schedule (ADOS)  and
the Autism Diagnostic Interview-Revised (ADI-R) . These standard exams are similar to
others in developmental pediatrics  in that they require a direct clinician-to-child observa-
tion and take hours to administer . The sharp rise in incidence of autism, coupled with
the unscalable nature of the SOC, has created strain on the healthcare system. Wait times for a
diagnostic evaluation can reach or exceed  months in the US , and the average age of
diagnosis in the US remains near  years of age , with underserved populations’ average
age at ASD diagnosis as high as  years . The high variability in availability of diagnos-
tic and therapeutic services is common to most psychiatry and mental health conditions across
the US, with severe shortages of mental health services in % of US counties . Behavioral
interventions for ASD are most impactful when administered by or before  years of age
; however, the diagnostic bottleneck that families face severely limits the impact of
therapeutic interventions. Scalable measures are necessary to alleviate these bottlenecks,
reduce waiting times for access to therapy, and reach underserved populations in need.
Mobile detection of autism through machine learning on home video
PLOS Medicine | 
November , 
 / 
As a step toward enabling fast and accurate access to care for ASD, we have used supervised
machine learning approaches to identify minimal sets of behaviors that align with clinical diag-
noses of ASD . We assembled and analyzed item-level outcomes from the administra-
tion of the ADOS and ADI-R to train and test the accuracy of a range of classifiers. For the
ADOS, we focused our analysis on ordinal outcome data from modules , , and , which
assess children with limited or no vocabulary, with phrased speech, and with fluent speech,
respectively. Each of the  ADOS modules uses approximately  activities for a clinical obser-
vation of the child at risk and – additional behavioral measurements used to score the
child following the observation. Our machine learning analyses focused on archived records of
the categorical and ordinal data generated from the scoring component of these ADOS exami-
nations. Similarly, the ADI-R involves  multiple-choice questions asked by a clinician of the
child’s primary care provider during an in-clinic interview; as with the ADOS, we focused our
classification task on the ordinal outcome data that resulted from the test’s administration.
These preliminary studies focused on building models optimized for accuracy, sparsity, and
interpretability that differentiate autism from non-autism while managing class imbalance.
We chose models with small numbers of features, with performance at or no more than  stan-
dard error away from best test performance, and with interpretable outcomes—for example,
scores generated by a boosted decision tree or logistic regression (LR) approach. In all, these
studies have used score data from  individuals with autism (mixed with low-, medium-,
and high-severity autism) and  controls (including some children for whom autism may
have been suspected but was ruled out) and have identified the following  classifiers: a -fea-
ture alternating decision tree  , an -feature alternating decision tree 
, a -feature support vector machine  , a -feature LR classifier  , a
-feature support vector machine  , a -feature LR classifier  , a -fea-
ture LR classifier  , and a -feature support vector machine  .
Two of these  classifiers have been independently tested in  separate analyses. In a pro-
spective head-to-head comparison between the clinical outcome and ADTree (measured
prior to the clinical evaluation and official diagnosis) on  children (NASD = ; Ncontrols =
; median age = . years), the performance, measured as the unweighted average recall
(UAR ; the mean of the sensitivity and specificity), was .% . Separately, Bone and
colleagues  tested the ADTree on a “Balanced Independent Dataset” (BID) consisting of
ADI-R outcome data from  participants ( ASD, mean age = . years, SD = . years)
and  non-ASD (mean age = . years, SD = . years) and found the performance to be
similarly high at %. Duda and colleagues  tested the ADTree with  individuals
with autism (mean age = . years) and  “non-autism” control individuals (mean age = .
years) and found the performance to be .%. Bone and colleagues  also tested this
ADTree model in  participants from the BID— autism (mean age = . years,
SD = . years),  autism spectrum (mean age = . years, SD = . years), and  non-spec-
trum (mean age = . years, SD = . years)—and found the performance to be slightly higher
at %. These independent validation studies report classifier performance in the range of the
published test accuracy and lend additional support to the hypothesis that models using mini-
mal feature sets are reliable and accurate for autism detection.
Others have run similar training and testing experiments to identify top-ranked features
from standard instrument data, including Bone . These approaches have
arrived at similar conclusions, namely that machine learning is an effective way to build objec-
tive, quantitative models with few features to distinguish mild-, medium-, and high-severity
autism from children outside of the autism spectrum, including those with other developmen-
tal disorders. However, the translation of such models into clinical practice requires additional
steps that have not yet been adequately addressed. Although some of our earlier work has
Mobile detection of autism through machine learning on home video
PLOS Medicine | 
November , 
 / 
shown that untrained video annotators can measure autism behaviors on home videos with
high interrater reliability and accuracy , the question of what steps must be taken to move
from minimal behavioral models into clinical practice remains.
The present study builds on this prior work to address this question and the hypothesis that
features represented in our minimal viable classifiers can be labeled quickly, accurately, and
reliably from short home videos by video raters with no official training in autism diagnosis or
child development. We deployed crowdsourcing and real-time video analysis for feature label-
ing to run and evaluate the accuracy of the  machine learning models trained to detect autism
in  independent home video repositories. This procedure enabled us to test the ability to
reduce to practice the process of rapid mobile video analysis as a viable method for identifying
autism symptoms and screening. In addition, as the mobile feature tagging of videos automati-
cally generates a rich feature matrix, it presents the opportunity to train a new artificial intelli-
gence model that has potentially higher generalizability to the task of automatic detection of
autism in short video clips. We test this related hypothesis by constructing a novel video fea-
ture classifier and comparing its results to alternative models in a held-out subset of the origi-
nal video feature matrix and in an independent external validation set. The results from this
work support the hypothesis that autism detection can be done from mobile devices outside of
clinical settings with high efficiency and accuracy.
Methods
Source classifiers for reduce-to-practice testing
We assembled  published machine learning classifiers to test viability for use in the rapid
mobile detection of autism in short home videos. For all of the  models, the source of training
and validation data was medical records generated through the administration of one of two
gold-standard instruments in the diagnosis of autism, the ADOS or the ADI-R. The ADOS has
several modules containing approximately  features that correspond to developmental level
of the individual under assessment. Module  is used on individuals with limited or no vocabu-
lary. Module  is used on individuals who use phrase speech but who are not fluent. Module 
is used on individuals who are fluent speakers. The ADI-R is a parent-directed interview that
includes > elements each asked of the parent, with multiple choices for answers. Each
model was trained on item-level outcomes from the administration of either the ADOS and
ADI-R and optimized for accuracy, sparsity of features, and interpretability.
For the purpose of brevity without omission of detail, we opted to create an abbreviation
for each model using a basic naming convention. This abbreviation took the form of “model_-
type”-“number of features.” For example, we used ADTree to refer to the use of an alternating
decision tree (ADTree) with  features developed from medical data from the administration
of the diagnostic instrument ADOS Module , and LR to refer to the LR with  behavioral
features developed from analysis of ADOS Module  medical record data, and so on.
ADTree.
We (Wall and colleagues ) applied machine learning to electronic medical
record data recorded through the administration of the ADI-R in the diagnostic assessment of
children at risk for autism. We used an %:% training and testing split and performed
-fold cross-validation for a sample of  children with autism and  non-autism control
participants with an ADTree model containing  features. The ADTree uses boosting to man-
age class imbalance . We also performed up-sampling through  bootstrap permu-
tations to manage class imbalance. The model was validated in a clinical trial on 
participants . The lowest
sensitivity and specificity exhibited were . and ., respectively (UAR = .%).
Mobile detection of autism through machine learning on home video
PLOS Medicine | 
November , 
 / 
ADTree.
We  used a dataset of score sheets from ADOS Module  for  children
with ASD and  non-autism control participants with a %:% training and testing split
and -fold cross-validation to train and test an ADTree model with  of the  Module  fea-
tures. The ADTree uses boosting and has inherent robustness to class imbalance . We
also performed up-sampling through  bootstrap permutations to test the sensitivity of
model performance to class imbalance. This -feature ADTree model was independently
tested on  individuals with autism by Wall and colleagues , on  individuals with
autism and  without autism by Duda and colleagues , and on  individuals (
autism,  autism spectrum,  non-spectrum) by Bone and colleagues . The lowest sensi-
tivity and specificity reported were .% and .%, respectively (UAR = .%).
LR. We  performed training with ADOS Module  records on  individuals with
autism and  individuals without autism with backward feature selection and iterative
removal of the single lowest-ranked feature across  folds each with a %:% class split.
Classes were weighted inversely proportional to class size to manage imbalance. The model
with the highest sensitivity and specificity and lowest number of features, LR with L regulari-
zation and  features, was selected for testing. We tested the model on independent data from
 individuals with autism and  individuals with no autism diagnosis. The lowest sensitiv-
ity and specificity identified were .% and .%, respectively (UAR = .%).
SVM.
We  used score sheets from ADOS Module  generated by the evaluation of
 children with ASD and  non-ASD control participants. These data were split into a %
training and % testing set. Training and parameter tuning were performed with stepwise
backward feature selection and iterative removal of the single lowest-ranked feature across 
folds. Classes were weighted inversely proportional to class size to manage imbalance. Several
models were fit to each of the feature cross-validation folds. The model with the highest sensi-
tivity and specificity and lowest number of features, a Support Vector Machine (SVM) with a
radial basis function, was then applied to the test set to measure generalization error. We tested
the model on  individuals with autism and  individuals who did not qualify for an
autism diagnosis. The lowest sensitivity and specificity identified on the test set were .%
and .%, respectively (UAR = .%).
LR and SVM.
In this experiment, we  used medical records generated through the
administration of ADOS Module  for  children with autism and  non-autism control
participants. The dataset was split %:% into train and test sets, with the same proportion
for participants with and without ASD in each set. Class imbalance was managed by setting
class weights inversely proportional to the class sizes. A -fold cross-validation was used to
select features, and a separate -fold cross-validation was run for hyperparameter tuning
prior to testing the performance. An SVM and an LR model with L regularization showed the
highest test performance with  features. The lowest sensitivity and specificity exhibited on the
test set for SVM were % and %, respectively, (UAR = %) and % and %, respec-
tively, (UAR = %) for LR.
LR and SVM. In this experiment, we  used medical records generated through
the administration of ADOS Module  for  children with autism and  non-autism
control participants. The dataset was split %:% into train and test sets, with the same pro-
portion for participants with and without ASD in each set. Class imbalance was managed by
setting class weights inversely proportional to the class sizes. A -fold cross-validation was
used to select features, and a separate -fold cross validation was run for hyperparameter tun-
ing prior to testing the performance. An SVM and an LR model with L regularization showed
the highest test performance with  features. The lowest sensitivity and specificity exhibited
on the independent test set for SVM were % and %, respectively, (UAR = %) and
% and %, respectively, (UAR = .%) for LR.
Mobile detection of autism through machine learning on home video
PLOS Medicine | 
November , 
 / 
Accounting for overlap in the features selected, these  models measure  unique features
in total. The test accuracy for each model was >%. All models contain approximately %
fewer questions than the ADI-R and %–% fewer questions than the total features mea-
sured within the ADOS. An additional  features were chosen for their potential diagnostic
value and scored by video raters to assess their suitability for scoring home videos, creating a
total of  features for the mobile video rating process described below .
Recruitment and video collection
Under an approved Stanford University IRB protocol, we developed a mobile portal to facili-
tate the collection of videos of children with ASD, from which participants electronically con-
sented to participate and upload their videos. Participants were recruited via crowdsourcing
methods  targeted at social media platforms and listservs for families of children with
autism. Interested participants were directed to a secure and encrypted video portal website to
consent to participate. We required participants to be at least  years of age and the primary
care provider(s) for a child with autism between the ages of  months and  years. Partici-
pants provided videos either through direct upload to the portal or via reference to a video
already uploaded to YouTube together with age, diagnosis, and other salient characteristics.
We considered videos eligible if they  were between  and  minutes in length,  showed
the face and hands of the child,  showed clear opportunities for or direct social engagement,
and  involved opportunities for the use of an object such as a utensil, crayon, or toy.
Fig . Feature-to-classifier mapping. Video analysts scored each video with  features. This matrix shows which feature
corresponds to which classifier. Darker colored features indicate higher overlap, and lighter colors indicate lower overlap
across the models. The features are rank ordered according to their frequency of use across the  classifiers. Further details
about the classifiers are provided in Table . The bottom  features were not part of the machine learning process but were
chosen because of their potential relationship with the autism phenotype and for use in further evaluation of the models’
feature sets when constructing a video feature–specific classifier. ADTree, -feature alternating decision tree; ADTree,
-feature alternating decision tree; LR, -feature logistic regression classifier; LR, -feature logistic regression classifier;
SVM, -feature support vector machine; SVM, -feature support vector machine; SVM, -feature support vector
machine.

Mobile detection of autism through machine learning on home video
PLOS Medicine | 
November , 
 / 
We relied on self-reported information provided by the parents concerning the child’s offi-
cial diagnosis of autism or non-autism, the age of the child when the video was submitted, and
additional demographic information for videos that were submitted directly to the web portal.
For videos that were provided via YouTube URLs, we used YouTube metatags to confirm the
age and diagnosis of the child in the video. If a video did not include a metatag for the age of
the child in the video, the age was assigned following full agreement among the estimates
made by  clinical practitioners in pediatrics. To evaluate the accuracy of the parents’ self-
report and to safeguard against reporting biases, we commissioned a practicing pediatric spe-
cialist certified to administer the ADOS to review a random selection of  videos. We also
commissioned a developmental pediatrician to review a nonoverlapping random selection of
 additional videos. These clinical experts classified each video as “ASD” or “non-ASD.”
Feature tagging of videos to run machine learning models
We employed a total of  video raters who were either students (high school, undergraduate,
or graduate-level) or working professionals. None had training or certification for detection or
diagnosis of autism. All were given instructions on how to tag the  questions and were asked
to score  example videos before performing independent feature tagging of new videos.
Table . Eight machine learning classifiers used for video analysis and autism detection. The models were constructed from an analysis of archived medical records
from the use of standard instruments, including the ADOS and the ADI-R. All  models identified a small, stable subset of features in cross-validation experiments. The
total numbers of affected and unaffected control participants for training and testing are provided together with measures of accuracy on the test set. Four models were
tested on independent datasets and have been mentioned in a separate “Test” category. The remaining , indicated with “Train/test,” used the given dataset with an
%:% train:test split to calculate test accuracy on the % held-out test set. The naming convention of the classifiers is “model type”-“number of features”.
Abbreviations: ADI-R, Autism Diagnostic Interview-Revised; ADOS, Autism Diagnostic Observation Schedule; ADTree, -feature alternating decision tree; ADTree,
-feature alternating decision tree; LR, logistic regression; LR, -feature LR classifier; LR, -feature LR classifier; SVM, support vector machine; SVM, -feature
SVM; SVM, -feature SVM; SVM, -feature SVM.

Mobile detection of autism through machine learning on home video
PLOS Medicine | 
November , 
 / 
After training, we provided the raters with unique usernames and passwords to access the
secure online portal to watch videos and answer  questions for each video needed by the fea-
ture vectors to run the  machine learning classifiers . Features were presented to the
video raters as multiple-choice questions written at an approximately seventh-grade reading
level. The raters, who remained blind to diagnosis throughout the study, were tasked to choose
one of the tags for each feature that best described the child’s behavior in the video. Each
response to a feature was then mapped to a score between  and , with higher scores indicat-
ing more severe autism features in the measured behavior, or  to indicate that the feature
could not be scored. The behavioral features and the overlap across the models are provided in
Fig .
To test the viability of feature tagging videos for rapid machine learning detection and diag-
nosis of autism, we empirically identified a minimum number of video raters needed to score
parent-provided home videos. We selected a random subset of videos from the full set of vid-
eos collected through our crowdsourced portal and ran the ADTree  model on feature
vectors tagged by all  raters. We chose to run only ADTree for efficiency reasons and
because this model has been previously validated in  independent studies . We used a
sample-with-replacement permutation procedure to measure accuracy as a function of major-
ity rater agreement with the true diagnostic classification. We incrementally increased the
number of video raters per trial by  rater, starting with  and ending with , drawing with
replacement  times per trial. When considering only  raters, we required perfect class
agreement between the raters. With an odd number of raters, we required a strict majority
consensus. When an even number of raters disagreed on classification, we used an indepen-
dent and randomly chosen rater’s score to break the tie.
After determining the minimally viable number of video raters, we used that minimum to
generate the full set of -feature vectors on all videos. Seven of the models were written in
Python  using the package scikit-learn, and one was written in R. We ran these  models on
our feature matrices after feature tagging on videos. We measured the model accuracy through
comparison of the raters’ majority classification result with the true diagnosis. We evaluated
model performance further by age categories:  years, > to  years, > years to  years,
and > years. For each category, we calculated accuracy, sensitivity, and specificity.
We collected timed data from each rater for each video, which began when a video rater
pressed “play” on the video and concluded when a video rater finished scoring by clicking
“submit” on the video portal. We used these time stamps to calculate the time spent annotating
each video. We approximated the time taken to answer the questions by excluding the length
of the video from the total time spent to score a video.
Building a video feature classifier
The process of video feature tagging provides an opportunity to generate a crowdsourced col-
lection of independent feature measurements that are specific to the video of the child as well
as independent rater impressions of that child’s behaviors. This in turn has the ability to gener-
ate a valuable feature matrix to develop models that include video-specific features rather than
features identified through analysis on archived data generated through administration of the
SOC . To this end, and following the com-
pletion of the annotation on all videos by the minimum number of raters, we performed
machine learning on our video feature set. We used LR with an elastic net penalty  (LR-
EN-VF) to predict the autism class from the non-autism class. We randomly split the dataset
into training and testing, reserving % for the latter while using cross-validation on the train-
ing set to tune for hyperparameters. We used cross-validation for model hyperparameter
Mobile detection of autism through machine learning on home video
PLOS Medicine | 
November , 
 / 
tuning by performing a grid search with different values of alpha (varying penalty weights)
and L ratio (the mixing parameter determining how much weight to apply to L versus L
penalties). Based on the resulting area under the curve (AUC) and accuracy from each combi-
nation, we selected the top-performing pair of hyperparameters. Using this pair, we trained
the model using LR and balanced class weights to adjust weights inversely proportional to
class frequencies in the input data. After determining the top-ranked features based on the
trained model and the resulting coefficients, we validated the model on the reserved test set.
Independent test set for validation of video phenotyping processes
We used our video portal and crowdsourcing approaches to generate an independent collec-
tion of videos for evaluation and feature tagging by  different raters than those used in the pri-
mary analysis. These raters had similar characteristics to the original group (age, education, no
clinical certifications in developmental pediatrics) and were trained for video tagging through
the same procedures.
Ethics statement
This study was conducted under approval by Stanford University’s IRB under protocol IRB-
. Informed and written consent was obtained from all study participants who submitted
videos to the study.
Results
All classifiers used for testing the time and accuracy of mobile video rating had accuracies
above % . The union of features across these  classifiers  was  .
These features plus an additional  chosen for clinical validity testing were loaded into a
mobile video rating portal to enable remote feature tagging by nonclinical video raters.
We collected a total of  videos  with average video length of  minutes  sec-
onds (SD =  minute  seconds). Of the  ASD videos,  were direct submissions made by
Table . Demographic information on children in the collected home videos. We collected N =  ( ASD,  non-ASD) home videos for analysis. We excluded 
videos because of inadequate labeling or video quality. We used a randomly chosen  autism and  non-autism videos to empirically define an optimal number of raters.
Video feature tagging for machine learning was then done on  home videos.
Abbreviation: ASD, autism spectrum disorder.

Mobile detection of autism through machine learning on home video
PLOS Medicine | 
November , 
 / 
the primary caregiver of the child, and  were links to an existing video on YouTube. Of the
 non-ASD videos,  non-ASD videos were links to existing YouTube videos, and  were
direct submissions from the primary caregiver. We excluded  videos because of insufficient
evidence for the diagnosis  or inadequate video quality , leaving  videos (
with ASD and  non-ASD) which were loaded into our mobile video rating portal for the pri-
mary analysis. To validate self-reporting of the presence or absence of an ASD diagnosis, 
clinical staff trained and certified in autism diagnosis evaluated a random selection of  videos
( with ASD and  non-ASD) from the  videos. Their classifications had perfect corre-
spondence with the diagnoses provided through self-report by the primary caregiver.
We randomly selected  videos ( ASD and  non-ASD) from the total  collected
videos and had  raters feature tag all in an effort to evaluate the potential for an optimal num-
ber of raters, with optimal being defined through a balance of scalability and information con-
tent. The average video length of this random subset was  minute  seconds (SD = 
seconds) for the ASD class and  minutes  seconds (SD =  minute  seconds) for the non-
ASD class. We then ran the ADTree  model on the feature vectors generated by the
 raters. We found the difference in accuracy to be statistically insignificant between  raters—
the minimum number to have a majority consensus on the classification with no ties—and 
Fig . Accuracy across different permutations of  raters for  videos. We performed the analysis to determine the optimal number (the minimum number to reach
a consensus on classification) of video raters needed to maintain accuracy without loss of power. Nine raters analyzed and generated feature tags for a subset of n = 
videos . The increase in accuracy conferred by the use of  versus  raters was not
significant. We therefore set the optimal rater number to  for subsequent analyses. ADTree, -feature alternating decision tree; ASD, autism spectrum disorder.

Mobile detection of autism through machine learning on home video
PLOS Medicine | 
November , 
 / 
raters . We therefore elected to use a random selection of  raters from the  to feature
tag all  crowdsourced home videos.
Model performance
Three raters performed video screening and feature tagging to generate vectors for each of the
 machine learning models for comparative evaluation of performance . All classifiers
had sensitivity >.%. However, only  of the  models exhibited specificity above %. The
top-performing classifier was LR, which showed an accuracy of .%, sensitivity of .%,
and specificity of .%. The next-best-performing models were SVM with .% accuracy
(.% specificity) and LR with .% accuracy (% specificity).
LR exhibited high accuracy on all age ranges with the exception of children over  years
old (although note that we had limited examples of non-ASD  class in this range). This
model performed best on children between the ages of  and  years, with sensitivity and speci-
ficity both above % . SVM and LR showed an increase in performance on
children ages – years, both with % sensitivity and the former with .% and the latter
with .% specificity. The  raters agreed unanimously on  out of  videos (%) when
using the top-performing classifier, LR. The interrater agreement (IRA) for this model was
above % in all age ranges with the exception of the youngest age group of children, those
Fig . Overall procedure for rapid and mobile classification of ASD versus non-ASD and performance of models from Table . Participants were recruited to
participate via crowdsourcing methods and provided video by direct upload or via a preexisting YouTube link. The minimum for majority rules of  video raters
tagged all features, generating feature vectors to run each of the  classifiers automatically. The sensitivity and specificity based on majority outcome generated by the
 raters on  ( with autism) videos are provided. Highlighted in yellow is the best performing model, LR. ADTree, -feature alternating decision tree;
ADTree, -feature alternating decision tree; ASD, autism spectrum disorder; LR, -feature logistic regression classifier; LR, -feature logistic regression classifier;
LR, -feature logistic regression classifier; SVM, -feature support vector machine; SVM, -feature support vector machine; SVM, -feature support
vector machine.

Mobile detection of autism through machine learning on home video
PLOS Medicine | 
November , 
 / 
under  years, for which there was a greater frequency of disagreement. The numbers of non-
ASD representatives were small for the older age ranges evaluated .
The median time for the  raters to watch and score a video was  minutes .
Excluding the time spent watching the video, raters required a median of  minutes  seconds
to tag all  features in the analyst portal. We found a significant difference 
between the average time spent to score the videos of children with ASD and the average time
spent to score the non-ASD videos ( minutes  seconds compared with  minutes 
seconds).
Independent validation
To validate the feasibility and accuracy of rapid feature tagging and machine learning on short
home videos, we launched a second effort for crowdsourcing videos of children with and with-
out autism to generate an independent replication dataset. We collected  videos,  of chil-
dren with autism and  non-ASD. This set of videos was comparable to the initial set of 
videos in terms of gender, age, and video length. The average age for children with ASD was 
years  months (SD =  year  months), and the average age for non-ASD children was  years
 months  of the children with ASD were
male and %  of the non-ASD children were male. The average video length was 
minutes  seconds, with an SD of  seconds. For this independent replication, we used  dif-
ferent raters, each with no official training or experience with developmental pediatrics. The
raters required a median time of  minutes  seconds for complete feature tagging. LR again
yielded the highest accuracy, with a sensitivity of .% and a specificity of .%. A total of 
of the  videos were misclassified, with  false negatives.
Fig . Performance for LR by age. LR exhibited the highest classifier performance . This model
performed best on children between the ages of  and  years. (A) shows the performance of LR across  age ranges, and (B) provides the ROC curve for LR’s
performance for children ages  to  years. Table  provides additional details, including the number of affected and unaffected control participants within each
age range. AUC, area under the curve; LR, -feature logistic regression classifier; ROC, receiver operating characteristic.

Mobile detection of autism through machine learning on home video
PLOS Medicine | 
November , 
 / 
Given the higher average time for video evaluation, we hypothesized that the videos con-
tained challenging displays of autism symptoms. Therefore, we examined the probabilities
generated by the LR model for the  misclassified videos. Two of the  false negatives and 
of the  false positives had borderline probabilities scores between . and .. We elected to
define a probability threshold between . and . to flag videos as inconclusive cases. Twenty-
six of the  videos fell within this inconclusive group when applying this threshold. When we
excluded these  from our accuracy analysis, the sensitivity and specificity increased to .%
and .%, respectively.
Table . Model performance by age. This table details the accuracy, sensitivity, specificity, precision, and recall for  classifiers  and for  age ranges found in
evaluation of  home videos with an average length of  minutes. We also provide the IRA, which indicates the frequency with which the model results from all  raters’
feature tags agreed on class. The top-performing classifier was LR, which yielded an accuracy of .%, sensitivity of .%, and specificity of .%. Other notable classifi-
ers were SVM and LR, which yielded .% and .% accuracy, respectively. These  best-performing classifiers showed improved classification power within certain
age ranges.
Abbreviations: ADTree, -feature alternating decision tree; ADTree, -feature alternating decision tree; ASD, autism spectrum disorder; IRA, interrater agreement;
LR, -feature logistic regression classifier; LR, -feature logistic regression classifier; LR, -feature logistic regression classifier; SVM, -feature support vector
machine; SVM, -feature support vector machine; SVM, -feature support vector machine; UAR, unweighted average recall.

Mobile detection of autism through machine learning on home video
PLOS Medicine | 
November , 
 / 
Training a video feature–specific classifier
To build a video feature–specific classifier, we trained an LR-EN-VF model on  ( rat-
ers ×  videos) novel measures of the  video features used to distinguish the autism class
from the neurotypical cohort. Out of these  videos , 
 were from the analysis set, and  videos (ASD = , non-
ASD = ) were from the set of  validation videos. Model hyperparameters (alpha and L
ratio) identified through -fold cross-validation were . and ., respectively. We used a
high L ratio to enforce sparsity and to decrease model complexity and the number of features.
We had similar proportions  for non-ASD and ASD measures in the training set and
held-out test set, which allowed us to create a model that generalizes well without a significant
change in sensitivity or specificity on novel data. The model had an area under the receiver
operating characteristic curve (AUC-ROC) of .% and accuracy of .% on the held-out
test set. A comparison of LR-EN-VF with LR L penalty (no feature reduction) revealed similar
results . The top- features selected by the
model consisted of the following, in order of highest to lowest rank: speech patterns, commu-
nicative engagement, understands language, emotion expression, sensory seeking, responsive
social smile, stereotyped speech. One of these  features—sensory seeking—was not part of the
full sets of items on the standard instrument data used in the development and testing of the 
models depicted in Table . We then validated this classifier on the remaining  videos
 from the validation set, and the results showed an accuracy of
.% and an AUC-ROC of .%.
Discussion
Previous work  has shown that machine learning models built on records from stan-
dard autism diagnoses can achieve high classification accuracy with a small number of fea-
tures. Although promising in terms of their minimal feature requirements and ability to
generate an accurate risk score, their potential for improving autism diagnosis in practice has
Table . Time required for mobile tagging of video features needed to run the machine learning models. We highlight the average length of videos (all participants,
only participants with ASD, and only participants without ASD) as well as the average time required to watch and score the videos and the average time required from start
to end of the scoring component alone.
Abbreviation: ASD, autism spectrum disorder.

Mobile detection of autism through machine learning on home video
PLOS Medicine | 
November , 
 / 
remained an open question. The present study tested the ability to reduce these models to the
practice of home video evaluation by nonexperts using mobile platforms (e.g., tablets, smart-
phones). Independent tagging of  features by  raters blind to diagnosis enabled majority
rules machine learning classification of  two-minute (average) home videos in a median of
 minutes at % AUC on children ages  months to  years. This performance was main-
tained at % AUC (% CI %–%) in a prospectively collected and independent external
set of  videos each with  independent rater measurement vectors. Taking advantage of the
probability scores generated by the best-performing model (L-regularized LR model with 
features) to flag low-confidence cases, we were able to achieve a % AUC, suggesting that the
approach could benefit from the use of the scores on a more quantitative scale rather than just
as a binary classification outcome.
By using a mobile format that can be accessed online, we showed that it is possible to get
multiple independent feature vectors for classification. This has the potential to elevate confi-
dence in classification outcome at the time of diagnosis (i.e., when  or more agree on class)
while fostering the growth of a novel matrix of features from short home videos. In the second
part of our study, we tested the ability for this video feature matrix to enable development of a
new model that can generalize to the task of video-based classification of autism. We found
that an -feature LR model could achieve an AUC of . on the held-out subset and . on
the prospective independent validation set. One of the features used by this model, sensory
seeking, was not used by the instruments on which the original models were trained, suggest-
ing the possibility that alternative features may provide added power for video classification.
These results support the hypothesis that the detection of autism can be done effectively at
scale through mobile video analysis and machine learning classification to produce a quanti-
fied indicator of autism risk quickly. Such a process could streamline autism diagnosis to
enable earlier detection and earlier access to therapy that has the highest impact during earlier
windows of social development. Further, this approach could help to reduce the geographic
and financial burdens associated with access to diagnostic resources and provide more equal
Fig . ROC curve for LR-EN-VF showing performance on test data along with an ROC for L loss with no feature
reduction. The former chose  out of  video features. AUC, area under the curve; LR-EN-VF, logistic regression
with an elastic net penalty; ROC, receiver operating characteristic.

Mobile detection of autism through machine learning on home video
PLOS Medicine | 
November , 
 / 
opportunity to underserved populations, including those in developing countries. Further test-
ing and refinement should be conducted to identify the most viable method(s) of crowdsourc-
ing video acquisition and feature tagging. In addition, prospective trials in undiagnosed and in
larger, more-balanced cohorts including examples of children with non-autism developmental
delays will be needed to better understand the approach’s potential for use in autism diagnosis.
Supporting information
S Table. Results of  classifiers on independent validation set. LR, LR, and ADTree
are the top- best-performing classifiers on the validation set, which falls in line with the
results observed on the test dataset of  videos used earlier. LR still performs with the high-
est specificity out of the  models. ADTree, -feature alternating decision tree; LR, -feature
logistic regression classifier; LR, -feature logistic regression classifier.
(DOCX)
S Text. Instructions for video raters.
(DOCX)
S Checklist. The tripod checklist.
(DOCX)
Acknowledgments
We would like to thank Kaitlyn Dunlap, the participating families, and each of our video raters
for their important contributions to this study.
Author Contributions
Conceptualization: Dennis Paul Wall.
Data curation: Qandeel Tariq, Jena Daniels, Jessey Nicole Schwartz, Peter Washington, Haik
Kalantarian, Dennis Paul Wall.
Formal analysis: Qandeel Tariq, Peter Washington, Haik Kalantarian, Dennis Paul Wall.
Funding acquisition: Dennis Paul Wall.
Investigation: Qandeel Tariq, Jena Daniels, Jessey Nicole Schwartz, Dennis Paul Wall.
Methodology: Qandeel Tariq, Jena Daniels, Dennis Paul Wall.
Project administration: Jena Daniels, Jessey Nicole Schwartz, Dennis Paul Wall.
Resources: Jena Daniels, Jessey Nicole Schwartz, Peter Washington, Haik Kalantarian, Dennis
Paul Wall.
Software: Qandeel Tariq, Dennis Paul Wall.
Supervision: Dennis Paul Wall.
Validation: Qandeel Tariq, Dennis Paul Wall.
Visualization: Qandeel Tariq, Jessey Nicole Schwartz, Dennis Paul Wall.
Writing – original draft: Qandeel Tariq, Jessey Nicole Schwartz, Dennis Paul Wall.
Writing – review & editing: Qandeel Tariq, Jena Daniels, Jessey Nicole Schwartz, Peter Wash-
ington, Haik Kalantarian, Dennis Paul Wall.
Mobile detection of autism through machine learning on home video
PLOS Medicine | 
November , 
 / 
References
.
Prince M, Patel V, Saxena S, Maj M, Maselko J, Phillips MR, et al. Global mental health  - No health
without mental health. Lancet. ; :–. 
- PMID: 
.
Baio J, Wiggins L, Christensen DL, Maenner MJ, Daniels J, Warren Z, et al. Prevalence of Autism Spec-
trum Disorder Among Children Aged  Years—Autism and Developmental Disabilities Monitoring Net-
work,  Sites, United States, . MMWR Surveillance Summaries. ; :. 
/mmwr.ssa PMID: . PMCID: PMC.
.
Hertz-Picciotto I, Delwiche L. The Rise in Autism and the Role of Age at Diagnosis. Epidemiology.
; :–.  PMID: . PMCID:
PMC.
.
Christensen DL, Baio J, Van Naarden Braun K, Bilder D, Charles J, Constantino JN, et al. Prevalence
and Characteristics of Autism Spectrum Disorder Among Children Aged  Years–Autism and Develop-
mental Disabilities Monitoring Network,  Sites, United States, . MMWR Surveill Summ. ; 
:–.  PMID: .
.
Christensen DL, Bilder DA, Zahorodny W, Pettygrove S, Durkin MS, Fitzgerald RT, et al. Prevalence
and characteristics of autism spectrum disorder among -year-old children in the autism and develop-
mental disabilities monitoring network. Journal of Developmental & Behavioral Pediatrics. ; 
:–.  PMID: .
.
Buescher AV, Cidav Z, Knapp M, Mandell DS. Costs of autism spectrum disorders in the United King-
dom and the United States. JAMA Pediatr. ; :–. 
.
McPartland JC, Reichow B, Volkmar FR. Sensitivity and specificity of proposed DSM- diagnostic crite-
ria for autism spectrum disorder. J Am Acad Child Adolesc Psychiatry. ; :–. 
org/./j.jaac... PMID: . PMCID: PMC.
.
Lord C, Rutter M, Goode S, Heemsbergen J, Jordan H, Mawhood L, et al. Austism diagnostic observa-
tion schedule: A standardized observation of communicative and social behavior. Journal of autism and
developmental disorders. ; :–. PMID: .
.
Lord C, Rutter M, Le Couteur A. Autism Diagnostic Interview-Revised: a revised version of a diagnostic
interview for caregivers of individuals with possible pervasive developmental disorders. Journal of
autism and developmental disorders. ; :–. PMID: .
.
Association AP. Diagnostic and statistical manual of mental disorders (DSM-®). Arlington, VA: Ameri-
can Psychiatric Pub; .
.
Bernier R, Mao A, Yen J. Psychopathology, families, and culture: autism. Child Adolesc Psychiatr Clin
N Am. ; :–.  PMID: .
.
Dawson G. Early behavioral intervention, brain plasticity, and the prevention of autism spectrum disor-
der. Dev Psychopathol. ; :–.  PMID:
.
.
Mazurek MO, Handen BL, Wodka EL, Nowinski L, Butter E, Engelhardt CR. Age at first autism spec-
trum disorder diagnosis: the role of birth cohort, demographic factors, and clinical features. J Dev Behav
Pediatr. ; :–.  PMID: .
.
Wiggins LD, Baio J, Rice C. Examination of the time between first evaluation and first autism spectrum
diagnosis in a population-based sample. Journal of Developmental and Behavioral Pediatrics. ; 
:S–S. PMID: .
.
Gordon-Lipkin E, Foster J, Peacock G. Whittling Down the Wait Time: Exploring Models to Minimize the
Delay from Initial Concern to Diagnosis and Treatment of Autism Spectrum Disorder. Pediatr Clin North
Am. ; :–.  PMID: . PMCID:PMC.
.
Howlin P, Moore A. Diagnosis in autism: A survey of over  patients in the UK. autism. ; 
:–.
.
Kogan MD, Strickland BB, Blumberg SJ, Singh GK, Perrin JM, van Dyck PC. A National Profile of the
Health Care Experiences and Family Impact of Autism Spectrum Disorder Among Children in the United
States, -. Pediatrics. ; :E–E. 
PMID: .
.
Siklos S, Kerns KA. Assessing the diagnostic experiences of a small sample of parents of children with
autism spectrum disorders. Res Dev Disabil. ; :–. 
 PMID: .
.
Thomas KC, Ellis AR, Konrad TR, Holzer CE, Morrissey JP. County-level estimates of mental health
professional shortage in the United States. Psychiatr Serv. ; :–. 
/ps.... PMID: .
Mobile detection of autism through machine learning on home video
PLOS Medicine | 
November , 
 / 
.
Dawson G, Jones EJH, Merkle K, Venema K, Lowy R, Faja S, et al. Early Behavioral Intervention Is
Associated With Normalized Brain Activity in Young Children With Autism. Journal of the American
Academy of Child and Adolescent Psychiatry. ; :–. 
.
Dawson G, Rogers S, Munson J, Smith M, Winter J, Greenson J, et al. Randomized, controlled trial of
an intervention for toddlers with autism: the Early Start Denver Model. Pediatrics. ; :e–.
 PMID: . PMCID: PMC.
.
Landa RJ. Efficacy of early interventions for infants and young children with, and at risk for, autism spec-
trum disorders. International Review of Psychiatry. ; :–. 
.
Phillips DA, Shonkoff JP. From neurons to neighborhoods: The science of early childhood development.
Washington, D.C.: National Academies Press; .  PMID: .
.
Duda M, Daniels J, Wall DP. Clinical Evaluation of a Novel and Mobile Autism Risk Assessment. J
Autism Dev Disord. ; :–.  PMID:
.
Duda M, Kosmicki JA, Wall DP. Testing the accuracy of an observation-based classifier for rapid detec-
tion of autism risk. Transl Psychiatry. ; :e.  PMID:
.
.
Kosmicki JA, Sochat V, Duda M, Wall DP. Searching for a minimal set of behaviors for autism detection
through feature selection-based machine learning. Translational Psychiatry. ; :e. https://
doi.org/./tp.. PMID: . PMCID: PMC.
.
Levy S, Duda M, Haber N, Wall DP. Sparsifying machine learning models identify stable subsets of pre-
dictive features for behavioral detection of autism. Mol Autism. ; :. 
s--- PMID: . PMCID: PMC.
.
Wall DP, Kosmicki J, DeLuca TF, Harstad E, Fusaro VA. Use of machine learning to shorten observa-
tion-based screening and diagnosis of autism. Translational Psychiatry. ; :e. 
org/./tp.. PMID: . PMCID: PMC.
.
Wall DP, Dally R, Luyster R, Jung JY, Deluca TF. Use of artificial intelligence to shorten the behavioral
diagnosis of autism. PLoS One. ; :e. 
PMID: .
.
Wall DP, Kosmiscki J, Deluca TF, Harstad L, Fusaro VA. Use of machine learning to shorten observa-
tion-based screening and diagnosis of autism. Translational Psychiatry. ; . 
.
Schuller B, Vlasenko B, Eyben F, Wollmer M, Stuhlsatz A, Wendemuth A, et al. Cross-Corpus Acoustic
Emotion Recognition: Variances and Strategies. Ieee Transactions on Affective Computing. ; 
:–. 
.
Bone D, Goodwin MS, Black MP, Lee CC, Audhkhasi K, Narayanan S. Applying machine learning to
facilitate autism diagnostics: pitfalls and promises. J Autism Dev Disord. ; :–. https://
doi.org/./s--- PMID: . PMCID: PMC.
.
Bone D, Bishop SL, Black MP, Goodwin MS, Lord C, Narayanan SS. Use of machine learning to
improve autism screening and diagnostic instruments: effectiveness, efficiency, and multi-instrument
fusion. Journal of Child Psychology and Psychiatry. ; :–. 
 PMID: . PMCID: PMC.
.
Bussu G, Jones EJH, Charman T, Johnson MH, Buitelaar JK, Team B. Prediction of Autism at  Years
from Behavioural and Developmental Measures in High-Risk Infants: A Longitudinal Cross-Domain
Classifier Analysis. Journal of Autism and Developmental Disorders. ; :–. 
org/./s---x PMID: . PMCID: PMC.
.
Fusaro VA, Daniels J, Duda M, DeLuca TF, D’Angelo O, Tamburello J, et al. The Potential of Accelerat-
ing Early Detection of Autism through Content Analysis of YouTube Videos. Plos One. ; :
e.  PMID: . PMCID: PMC.
.
Freund Y, Schapire RE, editors. Experiments with a new boosting algorithm. Icml;  July , ;
Bari, Italy. San Francisco, CA, USA: Morgan Kaufman Publishers Inc.; .
.
Freund Y, Mason L, editors. The alternating decision tree learning algorithm. icml;  June , ;
Bled, Slovenia. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.
.
Behrend TS, Sharek DJ, Meade AW, Wiebe EN. The viability of crowdsourcing for survey research.
Behav Res Methods. ; :–.  PMID:
.
.
David MM, Babineau BA, Wall DP. Can we accelerate autism discoveries through crowdsourcing?
Research in Autism Spectrum Disorders. ; :–.
Mobile detection of autism through machine learning on home video
PLOS Medicine | 
November , 
 / 
.
Ogunseye S, Parsons J, editors. What Makes a Good Crowd? Rethinking the Relationship between
Recruitment Strategies and Data Quality in Crowdsourcing. Proceedings of the th AIS SIGSAND
Symposium;  May -, ; Cincinnati, OH.
.
Swan M. Crowdsourced health research studies: an important emerging complement to clinical trials in
the public health research ecosystem. J Med Internet Res. ; :e. 
jmir. PMID: . PMCID: PMC.
.
Zou H, Hastie T. Regularization and variable selection via the elastic net. Journal of the Royal Statistical
Society: Series B :–. 
..x
Mobile detection of autism through machine learning on home video
PLOS Medicine | 
November , 
 / 
A Video-Based Measure to Identify Autism Risk in Infancy
Gregory S. Young, PhDa, John N. Constantino, MDb, Simon Dvorak, BSc, Ashleigh Belding, 
MPHa, Devon Gangi, PhDa, Alesha Hill, BAa, Monique Hill, MAa, Meghan Miller, PhDa, 
Chandni Parikh, PhDa, AJ Schwichtenberg, PhDd, Erika Solis, BSa, Sally Ozonoff, PhDa
aDepartment of Psychiatry & Behavioral Sciences, MIND Institute, University of California-Davis
bDepartment of Psychiatry, Washington University-St. Louis School of Medicine
cInformation and Educational Technology, University of California-Davis
dDepartment of Human Development & Family Studies, Purdue University
Abstract
Background: Signs of autism are present in the first two years of life, but the average age of 
diagnosis lags far behind. Instruments that improve detection of autism risk in infancy are needed. 
This study developed and tested the psychometric properties of a novel video-based approach to 
detecting ASD in infancy.
Methods: A prospective longitudinal study of children at elevated or lower risk for autism 
spectrum disorder was conducted. Participants were  infants with an older sibling with ASD and 
 infants with no known family history of autism. The Video-referenced Infant Rating System for 
Autism (VIRSA) is a web-based application that presents pairs of videos of parents and infants 
playing together and requires forced-choice judgments of which video is most similar to the child 
being rated. Parents rated participants on the VIRSA at , , , and  months of age. We 
examined split-half and test-retest reliability; convergent and discriminant validity; and sensitivity, 
specificity, and negative and positive predictive value for concurrent and -month ASD 
diagnoses.
Results: The VIRSA demonstrated satisfactory reliability and convergent and discriminant 
validity. VIRSA ratings were significantly lower for children ultimately diagnosed with ASD than 
children with typical development by  months of age. VIRSA scores at  months identified all 
children diagnosed with ASD at that age, as well as % of children diagnosed at  months.
Conclusions: This study represents an initial step in the development of a novel video-based 
approach to detection of ASD in infancy. The VIRSA’s psychometric properties were promising 
when used by parents with an older affected child, but still must be tested in community samples 
with no family history of ASD. If results are replicated, then the VIRSA’s low-burden, web-based 
format has the potential to reduce disparities in communities with limited access to screening.
Keywords
Autism; Screening; Infancy; Social Development
Correspondence: Sally Ozonoff, MIND Institute, UC Davis Health,  th Street, Sacramento CA ; --; 
. 
Published in final edited form as:
J Child Psychol Psychiatry.  January ; : –. doi:./jcpp..
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
Introduction
The developmental course of autism spectrum disorder (ASD) involves the onset of 
symptoms in the first three years of life. Differences between children who will later receive 
an ASD diagnosis and those with typical development emerge before the second birthday 
(Gammer et al., ; Landa & Garrett-Mayer, ; Ozonoff et al., ; Zwaigenbaum et 
al., ), with some studies documenting signs in the first year of life (Maestro et al., ; 
Miller et al., ; Werner, Dawson, Osterling & Dinno, ), and parents first expressing 
concerns at an average age of  months . Despite advances in 
knowledge about the earliest presentations of ASD, the mean age of diagnosis has 
stubbornly remained over  years  and has not declined over the last two 
decades, squandering years of potential intervention when the brain is most plastic. It is 
critical that further attempts are made to decrease the age of ASD diagnosis so that it better 
aligns with the age of first symptom emergence.
One of the identified barriers to more prompt recognition of ASD is measurement (Al 
Qabandi, Gorter & Rosenbaum, ). Over the last decade, much effort has gone into the 
development of instruments for earlier detection of ASD . The 
most feasible method for large-scale screening is parent report and most existing measures 
use this methodology. However, recent studies have demonstrated low agreement between 
parent report and more objective measures of ASD symptoms , as well 
as lower reliability for screening instruments when used in rural, low income, less educated, 
and racially diverse samples . A 
population screening study of  twelve-month-olds  using a parent-
report measure  identified  infants 
with ASD. This represents significant under-identification, even after accounting for cases 
with later onset , since current prevalence studies 
estimate that  of  children have ASD .
The lower sensitivity of early screening measures may be due to the subtlety of initial ASD 
symptoms and the difficulty of accurately conveying them to parents through written 
descriptions. Major sources of error in parent questionnaires include comprehension and 
interpretation problems , 
such as limited understanding of the queried constructs, inadequate knowledge of 
developmental milestones, and bias due to post-event information (e.g., eventual diagnosis). 
The current study moves beyond verbal descriptions by employing video examples to reduce 
subjective interpretations. The use of videos has been shown to dramatically increase clarity 
in other fields, from music instruction to motor vehicle repair . 
Recently, video was incorporated in ASD screening by Marrus and colleagues , who 
had parents complete ratings after watching a video of a socially competent toddler, in order 
to “reduce discrepant interpretations of items by providing informants with a common 
naturalistic standard for comparison” .
Here we describe the development of a new instrument, the Video-referenced Infant Rating 
System for Autism  by 
Young et al.
Page 
J Child Psychol Psychiatry. Author manuscript; available in PMC  January .
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
creating a large library of video clips depicting a wide range of social-communication ability 
and relying solely on video in the ratings, with no written descriptions of behavior. We 
hypothesized that the semantic clarity afforded by video would improve early discrimination 
of infants at highest risk for ASD.
Methods
Instrument Development
The VIRSA was developed using video from participants in a longitudinal infant sibling 
study and then validated on an independent sample of infants. Videos used in the VIRSA 
were drawn from an archive of over  minutes of digitized video recorded in a clinical 
laboratory setting. Video depicted infants and parents playing together with age-appropriate 
toys. Segments were selected from a task that used a standardized toy set and instructed 
parents to play with their child as they would at home (Schwichtenberg, Kellerman, Miller, 
Young & Ozonoff, ). Video recordings utilized a consistent camera angle facing the 
child, with the parent in profile. All families gave both informed consent and legal 
authorization to include their videos in the VIRSA.
Social behaviors, including smiles, vocalizations, and eye contact, were coded by research 
assistants unaware of participant risk group or outcome, using a previously validated coding 
scheme that is sensitive to the changes that occur during the onset of ASD symptoms as 
early as  months . In order to include a broad 
range of behaviors in the VIRSA, candidate videos were ranked by frequencies of the coded 
behaviors. Twenty-second segments were then excised from the original video files, 
resulting in a collection of over  video segments from  past participants between  
and  months of age. Next, video segments were rated by  clinical research staff on a scale 
from  (least socially competent) to  (most socially competent). Each clinician rated a 
randomly selected set of  videos twice to establish test-retest reliability (mean=., range 
.–.). Inter-rater reliability was examined on a larger randomly selected set of  
video clips rated by all raters using a two-way random ICC model. The average measures 
ICC for absolute agreement was ., with a lower bound of ., suggesting strong inter-
rater reliability of the -point scale. Video segments were excluded for poor lighting or 
audio quality, obscured video angles, or use of the child’s name. This resulted in a pool of 
video comprising  individual -second clips, which was then constrained to insure 
adequate representation across the -point rating scale within each age (, , , and  
months). To limit the software overhead for the VIRSA app,  videos were then randomly 
selected to create the final VIRSA video library. The final pool of video included segments 
from  children with ASD,  children with non-ASD developmental concerns (e.g. 
speech-language delays), and  children with typical development, based on -month 
outcome ( children total). Thirty-eight (.%) of the children depicted in VIRSA videos 
were male and  (.%) were Non-Hispanic Caucasian. Analysis of ratings of VIRSA 
videos, using a generalized linear model with random effects for subjects and age, revealed a 
significant group effect , with the ASD videos rated an average of 
. , the videos of children with non-ASD developmental concerns 
rated an average of . , and the videos of typically developing 
Young et al.
Page 
J Child Psychol Psychiatry. Author manuscript; available in PMC  January .
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
participants rated an average of . . Simple comparisons indicated 
that the ratings of the ASD videos differed significantly from the videos of both the non-
ASD developmental concerns  and typically developing cases (t=., p=.
), who did not differ from one another , as expected. Since the VIRSA 
was designed specifically to detect the social-communication behaviors relevant to ASD, but 
not broader developmental delays, this pattern of results provided further validation of the 
final video pool.
The library of video segments was incorporated into a web-based application that presented 
pairs of videos, depicting differing degrees of social competence, side by side, accompanied 
by the prompt, “Which video is more like your child’s interaction with you on a typical 
day?” On each trial, the video on the left played automatically, followed by the video on the 
right, at which point the viewer selected the one most like the child. Presentation of video 
followed an algorithm that always began with a pair of videos rated as  (less social) and  
(more social) on the -point scale. After each choice, the algorithm selected and displayed 
a second pair of videos with new scale values contingent upon the previously chosen video’s 
ranking. In each subsequent trial, the viewer’s video choice dictated the range of sociability 
represented in the videos on the next trial, analogous to how optometrists help patients select 
eyeglass prescriptions. In this way, the algorithm presented videos of increasing similarity 
over subsequent trials until the distance between videos reduced to  rating scale point on  
subsequent trials, at which point the average rating of the last  trials was recorded as the 
final score (see Figure S in online Appendix for examples).
The VIRSA web application was designed with a brief introductory video that oriented 
parents to the concepts and range of social behaviors depicted in the videos and provided 
rating instructions. The VIRSA app also asked for confirmation of the child’s age in order to 
present videos from a matching age group. Since multiple video exemplars of each scale 
point  were available, videos were sampled from the pool without replacement.
The UC Davis Institutional Review Board approved the study procedures. Parents signed an 
informed consent form prior to participation. They completed VIRSA ratings when their 
child was -, -, -, and -months-old and again two weeks later to examine test-retest 
reliability. An automated email invited parents to the online VIRSA app, which could be 
accessed by computer or mobile device (e.g., smartphones, tablets). VIRSA ratings were 
always done prior to in-person assessments, which were conducted at , , , , and  
months by examiners unaware of risk group or previous test results.
Participants
The VIRSA validation sample consisted of  infants ( with an older sibling with ASD, 
 with no known family history of autism), none of whom supplied videos used in 
instrument development. Twenty-one children in the familial high-risk (HR) group received 
a diagnosis of ASD, whereas none of the low-risk (LR) children did. ASD diagnoses were 
made at any age that a child met DSM- criteria, based on all information available. One 
child was diagnosed with ASD at  months,  were diagnosed at  months,  at  
months, and  at  months. All children diagnosed before  months retained the ASD 
diagnosis at the final visit. The rest of the sample was classified as Non-ASD and then 
Young et al.
Page 
J Child Psychol Psychiatry. Author manuscript; available in PMC  January .
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
stratified by familial risk to yield the HR Non-ASD and LR Non-ASD comparison groups. 
Descriptive statistics are shown in Table .
Measures
Mullen Scales of Early Learning  is an assessment of cognitive, 
motor, and language development for children aged  to  months. MSEL scores were used 
to describe the sample. Scores on the Fine Motor and Visual Reception subtests were used to 
evaluate discriminant validity since these developmental domains are theoretically unrelated 
to the social-communicative focus of the VIRSA.
Autism Diagnostic Observation Schedule, nd edition  is an 
observational measure that assesses ASD symptoms through semi-standardized interactions 
between the clinician and child. It is comprised of five modules appropriate for various ages 
and language levels; the Toddler module was administered at  and  months and either 
module  or  was used at  months, depending upon the child’s verbal level. Analyses 
utilized the overall total algorithm (Social Affect + Restricted Repetitive Behavior or 
SARRB) score. Scores on the ADOS at  months were also used to examine convergent 
validity with VIRSA ratings at  months.
Analysis Plan
Psychometric properties of the VIRSA were examined in several ways. Split-half reliability 
was analyzed by comparing the first half of the ratings within a given session to the second 
half. Test-retest reliability was analyzed by comparing initial VIRSA ratings to those 
obtained two weeks later. Parents were shown the same series of paired videos they had seen 
two weeks earlier, instead of video pairs dictated by the VIRSA algorithm, permitting 
examination of the reliability of individual trial choices. Convergent validity was examined 
through correlations at  months with concurrent SARRB algorithm scores on the ADOS- 
Toddler module. Discriminant validity was examined by correlating VIRSA scores with 
concurrent MSEL fine motor and visual reception age equivalents. Predictive validity was 
assessed by examining diagnostic outcome group differences on the VIRSA scores at each 
age, using a mixed model with group, age, and their interaction included as fixed effects and 
VIRSA score as the time-varying dependent variable. We also used ROC analysis to assess 
the VIRSA’s sensitivity, specificity, positive predictive value (PPV), and negative predictive 
value (NPV) in predicting ASD diagnosis. Area under the curve (AUC) was computed as a 
measure of the ability to distinguish between groups. Sample sizes initially projected were 
 high-risk and  low-risk infants. With an anticipated ASD outcome rate of 
approximately % in the high-risk group, power was estimated to be . to detect an AUC 
value of . on the ROC analyses. All analyses were conducted in R, version ...
Results
VIRSA trials (selections between paired videos) took an average of . seconds 
. The average number of trials before a final score was reached 
was . , with completion of the VIRSA taking an average of 
. minutes . Split-half reliability was moderate, at r=., 
Young et al.
Page 
J Child Psychol Psychiatry. Author manuscript; available in PMC  January .
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
and test-retest reliability relatively strong, with % of the same video choices made two 
weeks later, which was significantly greater than chance . 
Convergent validity correlation with concurrent ADOS- SARRB algorithm scores at  
months was r=−., which was significantly stronger than discriminant validity correlations 
with concurrent MSEL fine motor  and visual reception (r=.
, Fisher’s z=., p<.) age equivalent scores.
Figure  shows the modeled parent VIRSA scores for each group between  and  months. 
Examination of the model revealed a main effect for group . The 
main effect for age and the interaction between age and group were not significant. Planned 
contrasts revealed no significant differences in VIRSA scores between the ASD and 
comparison groups at  months (HR Non-ASD: t=., p=.; LR Non-ASD: t=., 
p=.) and  months . At  
months, VIRSA scores were significantly lower in the ASD group than both the HR Non-
ASD  and the LR Non-ASD groups . At  months, VIRSA 
scores were significantly lower in the ASD group than in the HR Non-ASD group (t=., 
p=.) and marginally lower than the LR Non-ASD group .
ROC analyses were conducted on VIRSA scores at each age, predicting a binary -month 
outcome . The threshold/cutoff at which ROC analyses best separated ASD and 
Non-ASD cases was defined as the VIRSA score closest to the theoretical limit of maximum 
specificity and sensitivity. The VIRSA performed best at  months. Table  presents ROC 
models that examined how well VIRSA scores at  months predicted concurrent -month 
diagnoses (n= diagnosed with ASD at that age). Sensitivity was % (no false negatives), 
but specificity and positive predictive value were low.
Discussion
We hypothesized that employing video examples within a screening tool would enable 
detection of ASD in infancy. Starting at  months of age, VIRSA ratings were significantly 
lower for the group eventually diagnosed with ASD than for the comparison groups. 
Sensitivity of -month VIRSA scores in predicting -month diagnosis was approximately 
.. This compares quite favorably to a study reporting sensitivity of -month clinical 
diagnostic assessment in predicting -month diagnosis of only . . 
In fact, VIRSA scores had better sensitivity even at – months of age than that reported 
for clinical diagnosis at  months in Ozonoff et al. . We hypothesize that the use of 
video allowed parents to “see” differences in their child that preceded the full onset of 
symptoms. The VIRSA’s sensitivity is especially impressive given the extended time course 
of development of ASD symptoms. Multiple previous studies have demonstrated that 
symptoms slowly unfold over the first two years of life and many children who are 
ultimately diagnosed with ASD do not show overt signs before the first birthday (Gammer et 
al., ; Landa & Garrett-Mayer, ; Ozonoff et al., ; Zwaigenbaum et al., ). 
Children who are not identified by the VIRSA may not yet be showing signs of ASD for 
parents to rate.
Young et al.
Page 
J Child Psychol Psychiatry. Author manuscript; available in PMC  January .
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
We also examined the VIRSA’s ability to index concurrent symptoms. ROC analyses of 
VIRSA ratings at  months identified all eight children who had been diagnosed with ASD 
by that age, with no false negatives. A recent meta-analysis of the accuracy of ASD 
screeners between  and  months of age  reported a pooled 
sensitivity of . and specificity of .. The sensitivity of the VIRSA at  months is thus 
comparable to or better than existing measures and suggests that it may be a useful adjunct 
in identifying toddlers in need of referral for an ASD evaluation. Its specificity and positive 
predictive value, however, were lower than recommended standards , 
resulting in over-identification of risk. For this reason, the present results do not support use 
of the VIRSA as a stand-alone ASD screener in infancy yet. Future studies could examine 
whether using the VIRSA as an initial step in a two-stage screening process improves 
accuracy.
In addition to the predictive validity of the VIRSA, we examined a number of other 
psychometric properties. Test-retest reliability was strong, with parents selecting over % 
of the same videos when they retook the VIRSA two weeks later. VIRSA scores at  
months were significantly correlated with ADOS- scores and correlations were 
significantly higher than with measures of divergent abilities (e.g., fine motor and visual 
reception skills).
The majority of participants, and all the children who developed ASD, came from the high-
risk group (e.g., had older siblings with ASD). It is imperative, prior to recommending the 
VIRSA for clinical use, to examine its psychometric properties when used by parents who 
are naïve to ASD. The positive predictive value of an instrument is dependent upon the base 
rate of the condition in the population  
and thus the VIRSA’s predictive ability may be reduced in a community-based sample that 
has a lower prevalence of ASD than in high-risk families. Studies are currently underway in 
our laboratory to determine whether the present results generalize to low-risk samples.
Despite these limitations, the VIRSA makes several contributions to the literature. First, it 
demonstrates that it is possible to develop a parent report instrument capable of identifying 
ASD risk in the first year of life. Second, it demonstrates that video can be used to clarify 
developmental phenomena and improve parent reporting of early development. Finally, an 
innovation of the VIRSA is its web-based, mobile-optimized application. Over % of 
American adults of childbearing age own a smartphone, with rates over % even in lower 
income, rural, and minority communities . It is vital that 
screening procedures keep pace with such advances in technology and society’s increasingly 
internet-based preferences for information acquisition and communication. Thus, this study 
provides an initial step in the proof of principle of video- and web-based screening for ASD. 
With further development, the VIRSA, with its low-burden, quick, online ratings, has 
potential to reduce disparities in communities with limited access to screening and provide 
the possibility of initiating intervention before the full symptom set of ASD has emerged.
Supplementary Material
Refer to Web version on PubMed Central for supplementary material.
Young et al.
Page 
J Child Psychol Psychiatry. Author manuscript; available in PMC  January .
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
Acknowledgments
This work was supported by NIH grants R MH (Ozonoff) and U HD (Abbeduto; MIND 
Institute Intellectual and Developmental Disabilities Research Center) and Autism Speaks grant  (Ozonoff). 
We are deeply grateful to the parents who authorized the use of their child’s video in the development of the VIRSA 
and the children and families who participated in the validation study.
Disclosures: Dr. Constantino receives royalties from Western Psychological Services for the commercial 
distribution of the Social Responsiveness Scale. Dr. Miller has received research grant funding from the National 
Institutes of Health and travel reimbursement and/or honoraria from the Society for Clinical Child and Adolescent 
Psychology and the Help Group. Dr. Ozonoff has received research grant funding from the National Institutes of 
Health and Autism Speaks, travel reimbursement and honoraria for editorial activities from Autism Speaks, Autism 
Science Foundation, and Wiley, and book royalties from Guilford Press and American Psychiatric Press, Inc. Dr. 
Schwichtenberg has received grant funding from the National Institutes of Health and travel support from Autism 
Speaks and the Autism Science Foundation. All other authors report no financial disclosures or potential conflicts 
of interest.
Abbreviations:
ASD
autism spectrum disorder
VIRSA
Video-referenced Infant Rating System for Autism
References
Al-Qabandi M, Gorter JW, Rosenbaum P. . Early autism detection: Are we ready for routine 
screening? Pediatrics, , e–e.  
Arguel A, Jamet E. . Using video and static pictures to improve learning of procedural contents. 
Computers in Human Behavior, , –.
Baio J, Wiggins L, Christensen DL, Maenner MJ, Daniels J, Warren Z, Kurzius-Spencer M, Zahorodny 
W, Robinson-Rosenberg C, White T, Durkin MS, Imm P, Nikolaou L, Yeargin-Allsopp M, Lee LC, 
Harrington R, Lopez M, Fitzgerald RT, Hewitt A, Pettygrove S, Constantino JN, Vehorn A, 
Shenouda, Hall-Lande J, Van Naarden-Braun K, Dowling NF. . Prevalence of autism 
spectrum disorder among children aged  years: autism and developmental disabilities monitoring 
network,  sites, United States . MMWR Surveillance Summary, , –.
Barger BD, Campbell JM, McDonough JD. . Prevalence and onset of regression within autism 
spectrum disorders: A meta-analytic review. Journal of Autism and Developmental Disorders, , 
–.  
Chawarska K, Paul R, Klin A, Hannigen S, Dichtel LE, Volkmar F. . Parental recognition of 
developmental problems in toddlers with autism spectrum disorders. Journal of Autism and 
Developmental Disorders, , –.  
Cicchetti DV, Volkmar F, Klin A, Showalter D. . Diagnosing autism using ICD- criteria: A 
comparison of neural networks and standard multivariate procedures. Child Neuropsychology, , 
–.
Clark A, & Harrington R . On diagnosing rare disorders rarely: Appropriate use of screening 
instruments. Journal of Child Psychology and Psychiatry, , –.  
Gammer I, Bedford R, Elsabbagh M, Garwood H, Pasco G, Tucker L, Volein A, Johnson MH, 
Charman T, BASIS Team. . Behavioral markers for autism in infancy: Scores on the Autism 
Observational Scale for Infants in a prospective study of at-risk siblings. Infant Behavior and 
Development, , –.  
Gangi DN, Boterberg S, Schwichtenberg AJ, Solis E, Young GS, Iosif A, & Ozonoff S . Use of 
prospective longitudinal gaze measurements in defining regression Paper presented at the annual 
meeting of the International Society for Autism Research, Montreal, .
Grimes DA, & Schulz KF . Uses and abuses of screening tests. Lancet, , –. [PubMed: 
] 
Young et al.
Page 
J Child Psychol Psychiatry. Author manuscript; available in PMC  January .
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
Khowaja MK, Hazzard AP, Robins DL. . Sociodemographic barriers to early detection of 
autism: Screening and evaluation using the M-CHAT, M-CHAT-R, and follow-up. Journal of 
Autism and Developmental Disorders, , –.  
Koriat A, Goldsmith M, Pansky A. . Toward a psychology of memory accuracy. Annual Review 
of Psychology, , –.
Krosnick JA, Presser S. . Question and questionnaire design In Marsden PV, Wright JD (Eds.), 
Handbook of Survey Research, nd edition . London: Emerald.
Landa R, Garrett-Mayer E. . Development in infants with autism spectrum disorders: A 
prospective study. Journal of Child Psychology and Psychiatry, , –. [PubMed: 
] 
Lord C, Rutter M, DiLavore PC, Risi S, Gotham K, Bishop SL. . Autism Diagnostic 
Observation Schedule Manual, nd edition Torrance, CA: Western Psychological Services.
Maestro S, Muratori F, Cavallaro MC, Pei F, Stern D, Golse B, Palacio-Espasa F. . Attentional 
skills during the first six months of age in autism spectrum disorder. Journal of the American 
Academy of Child and Adolescent Psychiatry, , –.  
Marrus N, Glowinski AL, Jacob T, Klin A, Jones W, Drain CE, Holzhauer KE, Hariprasad V, 
Fitzgerald RT, Mortensen EL, Sant SM, Cole L, Siegel SA, Zhang Y, Agrawal A, Heath AC, 
Constantino JN. . Rapid video-referenced ratings of reciprocal social behavior in toddlers: A 
twin study. Journal of Child Psychology and Psychiatry, , –.  
Miller M, Iosif A, Hill M, Young GS, Schwichtenberg AJ, Ozonoff S. . Response to name in 
infants developing autism spectrum disorder: A prospective study. Journal of Pediatrics, , –
.  
Mullen EM. . Mullen Scales of Early Learning. Circle Pines, MN: AGS Publishing.
Ozonoff S, Iosif AM, Baguio F, Cook IC, Hill MM, Hutman T, Rogers SJ, Rozga A, Sangha S, Sigman 
M, Steinfeld MB, Young GS. . A prospective study of the emergence of early behavioral 
signs of autism. Journal of the American Academy of Child and Adolescent Psychiatry, , 
–.  
Ozonoff S, Iosif A, Young GS, Hepburn S, Thompson M, Colombi C, Cook IC, Werner E, Goldring S, 
Baguio F, Rogers S. . Onset patterns in autism: Correspondence between home video and 
parent report. Journal of the American Academy of Child and Adolescent Psychiatry, , –
.  
Ozonoff S, Young GS, Landa RJ, Brian J, Bryson S, Charman T. Chawarska K, Macari SL, Messinger 
D, Stone WL, Zwaigenbaum L, Iosif AM. . Diagnostic stability in young children at risk for 
autism spectrum disorder: A baby siblings research consortium study. Journal of Child Psychology 
and Psychiatry, , –.  
Pew Research Center. . Demographics of mobile device ownership and adoption in the United 
States.  Accessed March , .
Pierce K, Carter C, Weinfeld M, Desmond J, Hazin R, Bjork R, Gallagher N. . Detecting, 
studying, and treating autism early: The one-year well-baby check-up approach. Journal of 
Pediatrics, , –.  
Sanchez-Garcia AB, Galindo-Villardon P, Nieto-Librero AB, Martin-Rodero H, & Robins DL . 
Toddler screening accuracy for autism spectrum disorder: A meta-analysis of diagnostic accuracy. 
Journal of Autism and Developmental Disorders, , –.  
Scarpa A, Reyes NM, Patriquin MA, Lorenzi J, Hassenfeldt TA, Desai VJ Kerkering KW. . The 
modified checklist for autism in toddlers: Reliability in a diverse rural American sample. Journal 
of Autism and Developmental Disorders, , –.  
Schwichtenberg AJ, Kellerman A, Miller M, Young GS, Ozonoff S. . Mothers of children with 
autism spectrum disorder: Play behaviors with infant siblings and social responsiveness. Autism 
[epub ahead of print]. doi: ./.
Werner E, Dawson G, Osterling J, Dinno N. . Recognition of autism spectrum disorder before 
one year of age: A retrospective study based on home videotapes. Journal of Autism and 
Developmental Disorders, –.  
Young et al.
Page 
J Child Psychol Psychiatry. Author manuscript; available in PMC  January .
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
Wetherby AM, Brosnan-Maddox S, Peace V, Newton L. . Validation of the infant-toddler 
checklist as a broadbrand screener for autism spectrum disorders from  to  months of age. 
Autism, , –.  
Zwaigenbaum L, Bryson S, Rogers T, Roberts W, Brian J, Szatmari P. . Behavioral 
manifestations of autism in the first year of life. International Journal of Developmental 
Neuroscience, , –.  
Zwaigenbaum L, Bauman ML, Fein D, Pierce K, Buie T, Davis PA, Newschaffer C, Robins DL, 
Wetherby A, Choueiri R, Kasari C, Stone WL, Yirmiya N, Estes A, Hansen RL, McPartland JC, 
Natowicz MR, Carter A, Granpeesheh D, Mailloux Z, Smith-Roley S, Wagner S. . Early 
screening of autism spectrum disorder: Recommendations for practice and research. Pediatrics, 
, S–S.  
Young et al.
Page 
J Child Psychol Psychiatry. Author manuscript; available in PMC  January .
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
Key Points
.
Signs of ASD are present in the first two years of life, but the average age of 
diagnosis lags far behind. Instruments that improve detection of autism risk in 
infancy are needed.
.
We hypothesized that employing video examples within a screening tool 
would improve detection of ASD in infancy.
.
A newly developed video-based screening tool had high sensitivity at  
months in concurrently identifying the toddlers diagnosed with ASD at that 
age, as well as predicting ASD at  months.
.
Employing video examples within a screening tool may be helpful in 
identifying ASD in infancy. A brief, low-burden, web-based screening tool 
could help reduce disparities in communities with limited access to care.
Young et al.
Page 
J Child Psychol Psychiatry. Author manuscript; available in PMC  January .
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
Figure : 
VIRSA ratings by group from  to  months.
Young et al.
Page 
J Child Psychol Psychiatry. Author manuscript; available in PMC  January .
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
Young et al.
Page 
Table :
Sample Descriptives.
ASD
HR Non-ASD
LR Non-ASD
Sample size



Sex (% male)
.%a
Race/Ethnicity
 % Non-Hispanic Caucasian
.%a
 % Non-White Race or Multiracial
.%a
 % Hispanic
.%b
Maternal education
 % Graduate degree
.%ab
 % College degree
.%a
 % High school or Vocational training
.%a
Household income
 % $k or less
.%a
 % $k to $k
.%a
 % $k or higher
.%a
Age at outcome (months)
. a
ADOS- SARRB* algorithm score at  months
. b
MSEL outcome fine motor age eq
. c
MSEL outcome visual reception age eq
. b
MSEL outcome expressive language age
. b
MSEL outcome receptive language age eq
. b
Note:
Values with different subscripts are significantly different at p<.
*Social Affect + Restrictive Repetitive Behavior overall total
MSEL outcome = Mullen Scales of Early Learning at  months of age
J Child Psychol Psychiatry. Author manuscript; available in PMC  January .
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
Young et al.
Page 
Table :
ROC analyses with -month ASD diagnostic classification.
 months
 months
 months
 months
True positives




False positives




True negatives




False negatives




AUC
.
.
.
.
Specificity (% CI)
. 
. 
. 
. 
Sensitivity (% CI)
. 
. 
. 
. 
Negative Predictive Value (% CI)
. 
. 
. 
. 
Positive Predictive Value (% CI)
. 
. 
. 
. 
Threshold
.
J Child Psychol Psychiatry. Author manuscript; available in PMC  January .
Author Manuscript
Author Manuscript
Author Manuscript
Author Manuscript
Young et al.
Page 
Table :
ROC analyses for -month VIRSA with concurrent -month diagnosis.
VIRSA ( months)
True positives

False positives

True negatives

False negatives

AUC
.
Specificity
.
Sensitivity
.
Negative Predictive Value
.
Positive Predictive Value
.
Threshold
.
J Child Psychol Psychiatry. Author manuscript; available in PMC  January .
Research and development of
autism diagnosis information
system based on deep convolution
neural network and facial
expression data
Wang Zhao and Long Lu
School of Information Management, Wuhan University, Wuhan, China
Abstract
Purpose – Facial expression provides abundant information for social interaction, and the analysis and
utilization of facial expression data are playing a huge driving role in all areas of society. Facial expression data
can reflect people’s mental state. In health care, the analysis and processing of facial expression data can
promote the improvement of people’s health. This paper introduces several important public facial expression
databases and describes the process of facial expression recognition. The standard facial expression database
FER and CKþ were used as the main training samples. At the same time, the facial expression image data
of  Chinese children were collected as supplementary samples. With the help of VGG and Resnet
algorithm models of deep convolution neural network, this paper studies and develops an information system
for the diagnosis of autism by facial expression data.
Design/methodology/approach – The facial expression data of the training samples are based on the
standard expression database FER and CKþ. FER and CKþ databases are a common facial
expression data set, which is suitable for the research of facial expression recognition. On the basis of FER
and CKþ facial expression database, this paper uses the machine learning model support vector machine
(SVM) and deep convolution neural network model CNN, VGG and Resnet to complete the facial
expression recognition.
Findings – In this study, ten normal children and ten autistic patients were recruited to test the accuracy of the
information system and the diagnostic effect of autism. After testing, the accuracy rate of facial expression
recognition is . percent. This information system can easily identify autistic children. The feasibility of
recognizing autism through facial expression is verified.
Research limitations/implications – The CKþ facial expression database contains some adult facial
expression images. In order to improve the accuracy of facial expression recognition for children, more facial
expression data of children will be collected as training samples. Therefore, the recognition rate of the
information system will be further improved.
Originality/value – This research uses facial expression data and the latest artificial intelligence technology,
which is advanced in technology. The diagnostic accuracy of autism is higher than that of traditional systems,
so this study is innovative. Research topics come from the actual needs of doctors, and the contents and
methods of research have been discussed with doctors many times. The system can diagnose autism as early as
possible, promote the early treatment and rehabilitation of patients, and then reduce the economic and mental
burden of patients. Therefore, this information system has good social benefits and application value.
Keywords Facial expression data, FER, CKþ, Deep convolution neural network, VGG, Resnet,
Autism, Diagnostic information system
Paper type Research paper
. Introduction
Facial expression recognition is an important social cognitive skill. Emotions are expressed
by facial expressions. Therefore, recognition and understanding of facial expressions is the
Facial
expressions for
autism
diagnosis
This research has been possible thanks to the support of projects: National Natural Science Foundation
of China  and Independent Research Project of School of Information Management
Wuhan University .
The current issue and full text archive of this journal is available on Emerald Insight at:

Received  August 
Revised  December 
Accepted  January 
Library Hi Tech
© Emerald Publishing Limited
-
DOI ./LHT---
basis of communication and interpersonal relationships with others. Abnormal expression is
a prominent manifestation of autism, and it is also one of the criteria for the diagnosis of
autism. Doctors can diagnose autism by responding to abnormal facial expressions in
children.
Autism, also known as autism or autism disorders, is a representative disease of
generalized developmental disorders. In recent years, the incidence of autism in children has
become higher and higher, experiencing a transition from rare diseases to epidemics. At
present, research on autism is still in its infancy at home and abroad, and research methods
and tools are still developing.
The main symptoms of autism include impaired social and interpersonal communication,
language retardation, repetitive behavior and sensory dysfunction. It is difficult for autistic
patients to correctly recognize faces and explain facial emotions. They have different
emotional expressions from ordinary people, and they cannot correctly perceive and
understand some basic expressions such as anger .
At present, the diagnostic methods for autism spectrum disorders include: traditional
standard DSM-IV-TR  and ICD- , various autism
diagnostic assessment scales such as “Childhood Autism Rating Scale (CARS)”, “the autism
child behavior scale (ABC)” and autism behavior rating scale and questionnaire interviews
. Most of these methods rely on doctors’ direct observation of the
patient’s expression, speech and behavior based on their experience. Diagnostic results are
easily disturbed by external factors such as hospital level, physician’s subjective level,
patient’s education level, age and so on. There are relatively large subjective factors, resulting
in a certain degree of missed diagnosis and misdiagnosis. It takes about – h for each autistic
patient to diagnose, so doctors have a lot of work to do. The best period of treatment for
autistic patients is before the age of six. Early diagnosis is of great significance for the
rehabilitation of autistic patients.
The purpose of our research and design is to train the model and make a facial expression
recognition system based on the normal expression, so as to verify the abnormal expression.
This system can test the facial expression of autistic children and judge the difference
between autistic children and normal children.
In this study, FER and CKþ were used as the main facial expression training
samples. At the same time, we collected the facial expression image data of  Chinese
children as a supplementary sample of facial expression. With the help of VGG and
Resnet algorithm models of deep convolution neural network, according to the hospital
autism diagnosis scale and diagnosis process, this paper studies and designs an information
system for the diagnosis of autism by facial expression data. After the actual test of recruiting
testers, the recognition rate of the system is . percent. It can effectively distinguish
whether the expression of children is normal or not. It provides a practical information system
for the diagnosis of autism. This paper will continue to collect more children’s facial
expression data from different countries and regions as training samples to further improve
the recognition rate of facial expressions.
The autism diagnosis information system designed in this study has the following
important significance:

Autism can be diagnosed as early as possible by using this system. The best time to
treat autism is before the age of six. The earlier the diagnosis of autism is made, the
less the treatment cost and the higher the probability of recovery. Early diagnosis is
of great value in alleviating the burden on families and society of autistic patients.
The system can be published in the form of app or web pages and disseminated
through the Internet. The system can be installed and used on different devices, such
as computers, mobile phones, tablets, etc. It has good applicability. Through this
LHT
system, autism can be diagnosed conveniently, and time can be saved for the early
treatment of autism patients, especially those in underdeveloped areas.

It can make the diagnosis of autism more objective. The whole diagnosis process is
completed by the system. Because artificial intelligence technology is used to
recognize facial expressions without human intervention, the diagnosis results are
objective and accurate.

Reduce the intensity of doctors’ work. Before the system was used, it took an hour for
doctors to diagnose an autistic patient. By using this system, doctors can save a lot of
time and pay attention to the treatment of autism.

The facial expression database used in the training of this system contains different
races in the world. Therefore, this system can not only diagnose children in different
countries and regions but also diagnose suspected autism patients all over the world.

This research designs the system according to the actual business. The early design
of the system adopts the suggestions of several doctors, so it is designed and
manufactured according to the actual needs of doctors. Although there are some
papers on autism diagnosis by facial expressions at home and abroad, there are still
few autism diagnosis systems developed which can be used in practice.

This paper uses the latest in-depth learning technology to improve the accuracy of
facial expression recognition. Previous traditional techniques and methods have low
recognition rate of facial expressions. In recent years, with the development of
artificial intelligence technology and the improvement of computing speed, the
convolutional neural network has greatly improved the accuracy of facial expression
recognition, which is the innovation of this research in technology.
. Facial expression database
Facial expression is an important way for people to express their emotions. In the social
process, facial expression is an important way to judge the attitude and inner feelings of
the other party . Mehrabian  found that in a conversation, the change
of facial expression played the most important role. Of these,  percent are facial
expressions,  percent are voice and only  percent are words .
Compared with voice, expression can convey more abundant information. Recognition
and understanding of facial expressions is very important for communicating with
others . In , Ekman demonstrated through empirical research that
human beings have six basic facial expressions: happiness, sadness, anger, fear, disgust
and surprise . In subsequent studies, neutral expression has also been
added to the basic expression, and it is generally believed that there are seven basic
expressions in facial expression.
With the continuous development of computer software and hardware technology, people
have a deeper understanding of facial expression recognition technology. In order to better
study facial expression recognition technology, many international research institutions
have established standard facial expression databases, the main facial expression databases
are as follows:
 JAFFE
The database stores facial expression data of Japanese women. It contains  facial images
of ten Japanese women. There are seven types of facial expressions, namely neutral, happy,
Facial
expressions for
autism
diagnosis
sad, surprise, anger, disgust and fear. The resolution of each image is    pixels.
Everyone has seven kinds of pictures of facial expressions.
 CKþ
The expression database was collected under laboratory conditions. It includes African
Americans, Asians and South Americans. The resolution of each image is * pixels. It
contains  expression sequences of  people,  percent of whom are female and 
percent are male. Each sequence begins and ends with neutral expression, which includes the
process from calm to strong expression. CKþ is a facial expression data set with many
applications. The reliability of various facial expression evaluation experiments using this
database is very high. It includes seven types of facial expressions: anger, contempt, disgust,
fear, happy, sadness and surprise.
 FER
There are  facial images in the library, and there are seven facial expression types:
angry, disgust, fear, happy, sad, surprise and neutral. The resolution of each image is *
pixels. All the images are gray images. There are three sample sets:  images in the
training set;  images in the validation set and  images in the test set.
 MMI
The expression database can be divided into two parts: one is a dynamic data set composed of
more than  video sequences. The other part is a static data set consisting of a large
number of high resolution images. There are seven types of expression in the library.
 AFEW
All the facial images in the database are edited from the movies and contain seven basic facial
expressions.
 SFEW
The expression library is a static frame image extracted from the AFEW data set, which
contains seven basic expressions.
. Facial expression recognition process
The process of facial expression recognition includes two stages as shown in Figure : One is
the training stage and the other is the recognition stage. The training and recognition stages
can be divided into three parts: the pretreatment of facial expression images, the extraction of
facial expression features and the classification of facial expressions. The training stage is to
train the model in order to achieve the purpose that the model can be used. The recognition
stage is to recognize and classify the expression of the test image .
The two stages of expression recognition process include the following processes: First,
face detection is carried out on the image in the expression database, including the location,
Figure .
Facial expression
recognition process
LHT
alignment and clipping of the face area. This is the basis of the follow-up process. Only when
the expression area is accurately obtained, the following series of work will be more accurate.
After the face area is detected, the image needs to be preprocessed in order to eliminate the
noise caused by the influence of acquisition equipment and environment and avoid the
interference of feature extraction. Then it is the feature extraction step, which aims to extract
the features that can represent the essence of expression from the preprocessed facial images.
In this process, in order to avoid the high dimension of feature extraction and affect the
efficiency of the algorithm, we need to reduce the dimension of extracted features in order to
extract the most representative expression features. Finally, the extracted facial features are
classified to determine which type of facial expression is.
. Facial expression recognition technology
Facial expression recognition technology mainly includes traditional machine learning
technology and deep learning technology. The two technologies have similarities and
different characteristics.
 Traditional machine learning technology
Facial expression recognition algorithm based on traditional machine learning includes three
steps: image preprocessing, facial expression feature extraction and feature classification.
First, for the convenience of feature extraction, it is necessary to preprocess the image,
which can effectively avoid the interference of various noises and leave the key information
needed by the face. The pretreatment process includes image gray processing, face
alignment, face size tailoring, data enhancement, brightness, pose normalization, etc. (Li and
Deng, ).
Second, the traditional feature extraction methods include directional gradient histogram
feature, Gabor filter feature, local directional pattern feature and enhanced local binary
algorithm. Because these methods are artificial design, time-consuming and laborious, and
have certain limitations and often have better effect in feature extraction in small sample
image set, most of the current studies are based on deep learning feature extraction method.
There are many basic machine learning methods for expression classification, such as
support vector machine (SVM), hidden Markov model (HMM) and k-nearest classification
algorithm.
 Deep learning technology
Facial expression recognition algorithm based on deep learning also needs image
preprocessing. The difference is that it often combines feature extraction and feature
classification into an end-to-end model, which greatly simplifies the process of facial
expression recognition. In addition to end-to-end learning, deep learning algorithm can be
used to extract facial expression features, and then other independent classifiers can be used.
For example, SVM or random forest algorithm is used to process the extracted features and
classify them.
In this paper, we construct a facial expression recognition model based on deep learning
technology, extract facial expression feature data of children and classify them into groups,
so as to diagnose autism.
. Driving role of facial expression data
Research on facial expression recognition has been applied in a series of life scenarios. In
children’s education, advanced human-computer interaction, medical diagnosis and other
aspects have played an important role .
In distance education or classroom teaching, teachers can better improve students’
learning quality by observing students’ emotional changes in the classroom and adjusting
Facial
expressions for
autism
diagnosis
teaching plans in time. Advanced human-computer interaction can make human-computer
interaction more harmonious. For example, intelligent robots can automatically respond to
the facial expressions of their interlocutors. In medical diagnosis, facial expressions also play
an important role in the prevention and diagnosis of diseases. For example, this article is to
diagnose autism by analyzing children’s facial expressions.
. Autism and its development
Autism is a neurodevelopmental disorder, which is collectively referred to as autism
spectrum disorder .
Since Kanner, an American child psychiatrist, first reported autism in , the incidence
of autism has risen rapidly worldwide. In the s, about – out of every  people
suffered from the disease, while in , . out of every  children suffered from the
disease . According to the National Center for Health Statistics,
the probability of autism among children aged – in the United States reached . percent
in  .
There is no statistical survey on autistic children in China. However, according to the data
of the report on the development of China’s autism education and rehabilitation industry II,
the number of people with autism in China is estimated to exceed  million, of which  million
are autistic children. At the same time, it is growing at the rate of nearly  annually
.
Autism brings serious financial burden to both society and family. Families with autistic
children, on the one hand, spend a lot of time caring for their children, while working hours are
reduced so that work income is reduced. On the other hand, the cost of family rehabilitation
treatment for autistic children is huge, which increases the family’s financial burden (Wu and
Chen, ). According to the survey on the occupational and economic burden of preschool
autistic children’s families,  percent of parents of autistic children reported that their
caregiving problems seriously affected their careers, and their annual income was
significantly lower than that of ordinary families, with an average loss of income of 
yuan per year. Meanwhile, the average annual cost of autistic children’s families for children’s
education and training is significantly higher than that of ordinary families (Yang and Wang,
). The society and the government also need to invest a lot of money in the rehabilitation
education of autistic children. At the same time, autism also brings high subjective load and
depression to the families of patients, which has a negative impact on their quality of life
. It can be seen that the incidence of autism in children is
relatively serious, and the harm to society and family is enormous.
.. Facial expression recognition disorder. Autistic children have facial expression
recognition obstacles, which are mainly manifested in their inability to recognize facial
expressions . It is easy to distinguish autistic children from normal children by
observing their facial expressions. Therefore, we combine facial expression recognition
technology to extract facial expression response feature vectors and use artificial intelligence
technology to distinguish normal group and autistic group based on these facial features.
.. The principle of diagnosing autism through facial expressions. A large number of
studies have pointed out that autistic patients have deficiencies in facial expression
recognition and understanding. This is the core source of impaired social function in
autistic patients . Autistic children are more difficult to identify other
people’s emotional behavior, and it is difficult to make appropriate judgment and response
LHT
. Overseas research on facial expression recognition ability of autistic
patients has been carried out not only in children but also in adults. Most studies believe that
the ability of facial expression recognition of autistic patients is low. Baron-Cohen et al. 
used standard facial expression maps to study the recognition of different emotional types in
autistic adults. It was found that autistic adults had better recognition of some basic facial
expressions, such as happiness, but relatively complex facial expressions such as surprise
recognition were difficult to recognize.
At present, the main diagnostic criteria of autism are: IDC-, DSM-IV, the autism child
behavior scale (ABC), the children autism rating scale (CARS) and the Clancy behavior scale
.
After consulting a large number of literatures and investigating the actual situation of the
hospital, now the hospital mainly uses CABS (filled by parents), ABC (filled by parents) and
CARS (filled by doctors) to diagnose autism. After a detailed review of the test items of the
three scales, these scales all contain the test items to judge autism through children’s facial
expressions. There were  items in the CABS scale, of which the seventh item was
inexplicable laughter and the tenth item was not looking at each other’s face. Avoiding eye
contact was related to expression. There were  items in the ABC scale, of which the seventh
item was non-communicative smile, the seventeenth item did not respond to other people’s
facial expressions, and the twenty-fourth item was active avoidance of eye contact with
others. Fifteen items of the CARS scale, the third of which is emotional response, pleasure and
unhappiness and interest, are expressed by changes in facial expression and posture. These
scales basically include the items of autism detection by children’s facial expressions, which
show that the diagnosis of autism can be more accurate by facial expressions. With the
progress of artificial intelligence technology, facial expression recognition technology can
objectively and effectively reflect the mental health of children and can be used in early
diagnosis of autism .
We also communicated with doctors of Hubei Maternal and Child Health Hospital, Wuhan
Children’s Hospital and Guangzhou Women and Children’s Medical Center many times, and
actually checked the process of using the above autism diagnostic scale to diagnose children.
The doctor observes the tester’s reaction to determine whether the tester is autistic after
requesting the tester to make the corresponding expression. Doctors point out that facial
expression is an important part of autism diagnosis. In terms of system design, they put
forward requirements and suggestions for the process of diagnosing autism through facial
expression.
. Facial expression database selection
The expression databases in this study mainly come from two public expression databases
CKþ and FER. In addition,  Chinese children’s expression data were collected as
supplementary samples. The two public expression databases are standard and international
and have been widely used, including facial expression data of adults and children. Each
sample in the database contains seven expressions: angry, disgust, fear, happy, sad, surprise
and neutral. Because children’s facial expressions are different from adults, in order to
improve the recognition rate of children’s facial expressions, we collected facial expression
data of  children aged  to  in China. Seven expressions were collected from each child. We
combine Chinese children’s facial expression data and public expression database as our
system’s facial expression database.
.. FER facial expression database. The reason for choosing FER expression
database is that it has more samples and is more mature than other expression databases. It
has advantages in model training. At the same time, it has been used in many studies (see
Plate ).
Facial
expressions for
autism
diagnosis
.. CKþ Facial expression database. CKþ facial expression database was selected
because it was collected in the laboratory, so its accuracy is relatively high 
.
.. Facial expression data of Chinese children. At present, the mature facial expression
databases at home and abroad are mainly based on adult male or female facial expression
images. Therefore, it is urgent to establish a facial expression database for children.
Facial images of children are quite different from those of adults. Children have
rounder faces, larger eyes and less prominent bones. Because of these differences,
children’s facial features are less obvious and more difficult to recognize than adults.
Because of the particularity of children, it is very difficult to collect children’s facial
images. In order to improve the recognition rate of children’s facial expressions, we
cooperated with Amy Education School in Zhengzhou. Sixteen healthy children as
volunteers were recruited to collect facial expression data. Each of them collected seven
kinds of expressions, totaling  pictures. These children are between  and  years old,
including  boys and  girls. The acquisition environment is quiet and there is no
external interference. High-definition cameras are used to collect facial expression
images, which are processed professionally. Before collecting facial expression data,
parents have been informed of the purpose of collecting facial expression data. After
questioning with parents, all the children who participated in the collection of facial
expression data had no history of autism.
We loaded the expression data into the training sample library. The purpose of collecting
Chinese children’s facial expression data is to increase the number of Chinese children’s facial
expression samples in training samples and improve the recognition rate of the system for
children’s facial expression. The collection process and the collected children’s facial
expression data are shown in Plate .
. Network topology
According to the network environment and equipment of the information service platform,
the network topology can be divided into four levels. The network topology diagram is shown
in Figure .
The first layer is the application layer, which consists of users, computers and various
smart devices. Smart devices include smart tablet computer, smartphones and other
electronic devices. Users access and use the information service platform through computers
and various smart devices.
Plate .
FER facial
expression database
Plate .
CKþ Facial expression
database
LHT
The second layer is the communication layer, mainly based on the internet network
environment, providing access channels for users and systems.
The third layer is the application server layer, which is composed of firewall and
application server and has an ontology display system for autism. The application server
manages various business functions, handles various business requests submitted by users
and can access the database server for various data exchange.
The fourth layer is the database server layer, which stores all kinds of data and knowledge
resources of the information service platform.
. System architecture
The smart diagnosis system of autism adopts client/server architecture. The client includes
different versions of programs suitable for computers and smartphones. The system
architecture diagram is shown in Figure .
The client includes three main modules: user interaction, image acquisition and face
detection. User interaction module is responsible for human-computer interaction. According
to the requirements of the autism diagnostic scale, users who diagnose are required to make
appropriate expressions and feedback by prompting pictures and voice guidance. Through
the camera, the image acquisition module can dynamically capture facial expression images.
At the right time, the system will collect facial expression images and transmit them to the
face detection module. Face detection module recognizes the valid face features and
compresses the image and transfers it to the server through the internet or mobile Internet.
The server includes six main modules: image processing, feature extraction, group
classification, automatic diagnosis, training model and data management. The image
processing module can receive the expression image transmitted by the client and then
process the expression image and transfer it to other modules on the server side. The feature
extraction module receives the facial expression images provided by the image processing
module and extracts the facial expression features. The group classification module is
Figure .
Network topology
diagram
Plate .
Collection of facial
expression data of
Chinese children
Facial
expressions for
autism
diagnosis
responsible for group classification and correctly classifies the expression images into the
most matching expressions among the seven kinds of expressions. The automatic diagnosis
module gives the diagnosis of autism by comparing the facial expressions that the tester is
required to imitate and the facial expressions that the tester actually makes. Model training
module is the core module of the system, which is responsible for recognizing and processing
the newly collected facial expression images. The data management module mainly manages
facial expression data, including storing and reading facial expression images transmitted by
the client.
The system server stores facial expression feature files, which are formed by feature
extraction of facial expression database. The expression feature file is HDF file format. The
expression recognition system running on the server can read the expression feature file at
any time. If new facial expression samples are collected, the model can be retrained and the
facial expression feature files can be updated.
The client collects the tester’s facial expression data by high-definition camera and
transmits thefacial expression datatothe serverbyJSONfileaccordingtoTCP communication
protocol.Thefacialexpressionrecognition systemrunning on the serverprocessesthe collected
facial expression data and then feeds the recognition results back to the tester through the
network and stores the recognition results and facial expression data in the server database.
Facial data and diagnostic system are stored on a server, and the recognition results and facial
data are stored in the SQL Server database. The diagnostic system reads data from the
database through SQL structured query language. The response time of the whole database
operation and communication process should not exceed  s.
.. VGG model. Researchers from the Oxford University and the Google Brain have
jointly developed the convolutional neural network VGG. VGNet consists of , ,  and 
layers of neural networks .VGNet constructs – layers of neural networks by stacking
small convolution cores of    and maximum pooling layers of    repeatedly. VGGNet
Figure .
System architecture
LHT
has strong scalability and greatly reduces the error rate when extending. When migrating to
other image data, it has good generalization ability and simple structure.
.. ResNet model. ResNet was proposed by Kaiming He and others of Microsoft
Research institute. They have successfully trained  layers of neural networks by using
ResNet unit. The structure of ResNet can accelerate the training of the neural network, and
the accuracy of the model has been greatly improved.
.. Graphic of deep learning framework. The deep learning framework used in this paper
for facial expression recognition is shown in Plate .
The whole process includes image input, image preprocessing, model building, model
training, model testing and output of expression recognition results. There are two kinds of
deep learning algorithms used in this paper: VGG and ResNet. ResNet solves the
problem of network performance degradation caused by the high depth of VGG. By
training the two models and synthesizing the two convolutional neural network models, the
facial expression features of autistic children can be extracted accurately.
.. Image preprocessing. The purpose of image preprocessing is to achieve uniform
normalization of the final input image. The process is shown in Figure .
Converting an image to a grayscale image can reduce the computational complexity of the
latter pixel level and also reflect the overall and local distribution and characteristics of the
image. Then, image transformation is used to enhance data by zooming, rotating, cutting and
translating, and the image is located in the center of the window. The contrast and brightness
of the image can be improved by histogram equalization to reduce the influence of
illumination on expression feature learning. In order to make the image uniform, it is planned
to transform the image size into the same size by normalizing the image size. Finally, the
mask is used to remove the occlusion of non-face areas.
.. Model training. Before model training, we need to enhance the image data. We choose
SGD random gradient descent algorithm as the optimization method. The batch size is still
 by default, and the learning rate is set to . initially. In addition, the initialization of
Plate .
Framework of deep
learning
Figure .
Image preprocessing
Facial
expressions for
autism
diagnosis
Plate .
Facial expression
recognition results
LHT
network parameters is also very important. We have adopted a random initialization method
to train the two network algorithms. The core code of Python is as follows:
# Model training
def train(epoch):
if epoch > learning_rate_decay_start and learning_rate_decay_start >= :
frac = (epoch - learning_rate_decay_start) // learning_rate_decay_every
decay_factor = learning_rate_decay_rate ** frac
current_lr = opt.lr * decay_factor
utils.set_lr(optimizer, current_lr) # set the decayed rate
else:
current_lr = opt.lr
for batch_idx, (inputs, targets) in enumerate(trainloader):
if use_cuda:
inputs, targets = inputs.cuda(), targets.cuda()
optimizer.zero_grad()
utils.clip_gradient
optimizer.step()
correct += predicted.eq(targets.data).cpu().sum()
.. Recognition results. Through the trained model, we use some children’s facial
expressions pictures and videos to test, and get the probability of various expressions and the
final prediction results of the model. As shown in Plate , the histogram shows the probability
of each type of facial expression, and the histogram of maximum probability is the final
recognized facial expression. After testing, the recognition rate of children’s facial expression
reaches . percent, which can effectively distinguish whether children’s facial expression is
normal or not.
. Testing environment
In this study, two kinds of mobile phones, personal computers and servers are selected as test
environments. The hardware and software environments are shown in Table I.
. Diagnostic procedure and interface of diagnostic system
The diagnostic process is shown in Figure . First, the system randomly displays one of the
seven kinds of facial expressions for the tester to imitate. The system will prompt the tester
Testing equipment
Hardware environment
Software environment
OPPO R mobile phone
CPU:SDM RAM:GB
Android
IPhone  mobile phone
CPU:A RAM:GB
iOS
Personal computer
CPU:Intel i RAM:GB
Windows 
Server
CPU:Intel W RAM:GB
Windows Server 
Figure .
Automatic diagnostic
procedure
Table I.
Testing environment
Facial
expressions for
autism
diagnosis
to imitate the facial expression by pictures and sounds. For example, the system displays
happy cartoon smiling faces, plays happy children’s songs and induces children to make
happy expressions. The system displays the same expression example three times and
collects the tester’s expression data at the same time. Then the system compares the
expression examples and the actual collected expression data and gives the diagnosis
results.
.. Diagnostic procedure. The diagnostic process is shown in Figure . First, the system
randomly displays one of the seven kinds of facial expressions for the tester to imitate. The
system will prompt the tester to imitate the facial expression by pictures and sounds. For
example, the system displays happy cartoon smiling faces, plays happy children’s songs and
induces children to make happy expressions. The system displays the same expression
example three times and collects the tester’s expression data at the same time. Then the
system compares the expression examples and the actual collected expression data and gives
the diagnosis results.
.. Interface of diagnostic system. The system diagnostic interface is designed according
to the diagnostic process .
.. Test sample. We recruited ten normal children and ten autistic children and divided
them into normal children group and autistic children group for comparative verification.
The accuracy of the system is verified by the actual test of the autism diagnosis information
system.
The normal group of children was provided by Amy Education School in Zhengzhou,
which cooperated with us. Ten healthy children as volunteers were recruited as the normal
Plate .
(a) The main interface
of the autism smart
diagnosis information
system, including
system introduction,
knowledge
introduction of autism
and other functions. (b)
The facial expression
that the system
prompts the tester to
simulate after starting
the diagnostic process.
(c) The expression
analysis after
diagnosis. (d) The
result given by the
system after three
diagnoses
LHT
group for testing. These children were between  and  years old, including  boys and  girls.
Parents were informed of the purpose and content of the experiment before the experiment.
Children who participated in the experiment had no history of autism after being asked by
their parents.
The autistic children were provided by Guangzhou Children’s Care Center, which
cooperated with us. Ten volunteers of autistic children were recruited as the autistic children
group for testing. These children were aged between  and  years old, including  boys and 
girls. Parents were informed of the purpose and content of the experiment before the
experiment. The selected children with autism were diagnosed by a professional physician.
.. Test environment and process. All the tests were conducted in quiet classrooms
without noise and external factors. Through our autism diagnosis information system, each
child was prompted by pictures and sounds to imitate seven kinds of facial expressions and
prompted to make corresponding facial responses according to the facial expressions on the
pictures. At this time, the camera will capture their facial expressions, and after system
analysis, they will be saved in the form of pictures in the computer of the test system (see
Plate ).
.. Test result. We used the system to test the normal combination and autistic children
respectively. Finally, we compared the recognition rate of the two groups.
From Table II, the average recognition rate of each expression is angry  percent, disgust
 percent, fear  percent, happy  percent, sad  percent, surprise  percent and neutral
 percent.
Test child  only had a disgusting expression recognition error, and other facial
expression recognition was correct, then the average recognition rate of the seven facial
Facial expression
Number of test children
Correct identification (Y  Yes, No  N)
Average recognition rate %










Angry
Y
Y
Y
N
Y
Y
Y
Y
Y
N

Disgust
N
Y
Y
Y
N
Y
N
Y
Y
Y

Fear
Y
Y
Y
N
Y
Y
Y
Y
N
Y

Happy
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y

Sad
Y
Y
N
Y
Y
Y
Y
Y
Y
N

Surprise
Y
N
Y
N
Y
Y
N
Y
Y
Y

Neutral
Y
Y
Y
Y
Y
Y
Y
Y
Y
N

Plate .
(a) Test environment
for normal children
group. (b) Test
environment for
autistic children group
Table II.
Test results in normal
children group
Facial
expressions for
autism
diagnosis
expressions of test child  was . percent. According to this method, the average
recognition rates of the seven expressions from test child  to test child  were . percent,
. percent and . percent, respectively. The average recognition rate is . percent.
Judging by the  percent threshold, there are two test children’s facial expression
recognition rates at . percent. This shows that in real environment, the algorithm of the
system is affected by the environment and light, and the accuracy will be affected to a certain
extent. However, according to the accuracy of . percent, it can basically meet the
preliminary diagnostic requirements of whether the expression is abnormal or not. In the
future, more real samples will be added to further improve the accuracy of the system
algorithm.
The experimental results show that the errors mainly concentrate on the expressions of
disgust and surprise. The main reasons are as follows:

Disgust and surprise have only minor local changes in the faces of the two kinds of
expressions, and there is no significant distinguishing feature.

Some of the participants had little change in the two facial expressions, did not have
the obvious features of the corresponding categories, approached neutral expressions
and were easy to confuse.
From Table III, the average recognition rate of each expression is angry  percent, disgust 
percent, fear  percent, happy  percent, sad  percent, surprise  percent and neutral 
percent.
The average recognition rates of the seven expressions from test child  to test child 
were . percent, . percent, . percent, . percent, . percent, . percent, .
percent, . percent, . percent and . percent, and the average recognition rate is .
percent.
Facial expression
Number of test children
Correct identification (Y  Yes, No  N)
Average recognition rate %










Angry
Y
N
N
N
Y
Y
N
Y
Y
N

Disgust
N
N
N
Y
N
N
N
N
N
N

Fear
N
N
Y
N
Y
Y
N
N
N
N

Happy
N
Y
Y
Y
N
Y
N
Y
N
Y

Sad
N
Y
N
Y
Y
Y
N
Y
Y
N

Surprise
Y
N
N
N
N
N
Y
N
N
N

Neutral
N
N
N
N
N
N
Y
N
N
N

Facial expression
Normal children group
Autistic children group
Angry
%
%
Disgust
%
%
Fear
%
%
Happy
%
%
Sad
%
%
Surprise
%
%
Neutral
%
%
Average recognition rate %
.%
Table III.
Test results in autistic
children group
Table IV.
Comparisons of two
groups of children’s
facial expression
recognition rate
LHT
The experimental results show that the recognition rate of happiness and sadness is
higher in the seven expressions. Testing children showed difficulty in identifying complex
facial expressions such as neutrality and aversion (see Table IV).
The experimental results showed that the recognition rate of facial expressions in autistic
children was significantly lower than that in normal children. All the autistic children who
participated in the test had a facial recognition rate of less than  percent. Therefore, if the
accuracy rate of facial expression diagnosis by the system was less than  percent, the tester
would have a tendency to suffer from autism. The lower the recognition rate, the higher the
tendency of autism.
. Conclusion
In the era of rapid development of information technology, the processing of a large number
of health data has brought new opportunities and challenges to medical research. The
incidence of autism is increasing, which has attracted more and more attention from all
aspects of society. The use of information technology, especially artificial intelligence
technology, to build an autism diagnosis system has become an urgent need for doctors and
patients. In this paper, an autism diagnosis system based on deep convolution neural network
and expression data is constructed. After testing, it can meet the design requirements of
autism diagnosis. The public can download and use the system through the network to
diagnose autism conveniently. In addition, we will expand the function of the system,
increase the recognition of children’s physical movement and realize the diagnosis of autism
from multiple perspectives.
Because the average age of children using and collecting facial expression data is between
 and  years old, the system can recognize children aged – years old. Therefore, through
this system, autism can be diagnosed as soon as possible. The earlier the diagnosis and
treatment of autism is, the better the rehabilitation effect. Therefore, it is of great significance
for the treatment of autism.
Because the training samples of the system adopt the international open facial expression
database, which contains the facial expression data of children and adults in different
countries and regions, the system can diagnose autism for children and adults in different
countries and regions.
Of course, the system also needs to be improved through practical use. Next, we will
arrange for the system to be tested in a large number of cooperative hospitals. Next, there are
two main tasks to be done. The first is to collect more data of normal and autistic children’s
facial expressions, improve the recognition effect of the system on children’s facial
expressions and establish a special database of children’s facial expressions. The second is to
improve the system function, according to the results of facial expression diagnosis of autistic
children for a detailed classification, to distinguish between severe, moderate and mild autism
patients, in order to facilitate the treatment of doctors.
This study hopes to be helpful to the diagnosis of autism in remote and underdeveloped
areas, so as to promote the early diagnosis and treatment of autistic children and reduce the
medical costs and burdens of autistic families and society. Therefore, this study has more
important social significance and application value.
References
Baron-Cohen, S., Wheelwright, S., Jolliffe, T. , “Is there a “language of the eyes”? Evidence from
normal adults, and adults with autism or asperger syndrome”, Visual Cognition, Vol.  No. ,
pp. -.
Beijing Wucai Deer Autism Research Institute , Report on the Development of Autism Education
and Rehabilitation Industry in China , Huaxia Publishing House, Beijing.
Facial
expressions for
autism
diagnosis
Cai, Y. , “Facial tracking and facial expression recognition based on in-depth learning”
Southeast University.
Du, J. , “Research on face expression recognition based on Kernel relieff” Zhengzhou University.
Duan, Y., Wu, X. and Jinfeng , “Research progress on etiology and treatment of autism”, Chinese
Science: Life Science, Vol. , pp. -.
Ekman, P. , “An argument for basic emotions”, Cognition and Emotion, Vol.  Nos -,
pp. -.
Lanlan , “Research on facial expression recognition method based on multi-feature fusion”, Jilin
University.
Li, S. and Deng, W. , “Deep facial expression recognition: a survey” arXiv preprint arXiv:
..
Liu, Y., Huo, W. and Hu, X. , “Summary of research on facial expression recognition of autistic
children”, Modern Special Education, Vol. , pp. -.
Lucey, P., Cohn, J.F., Kanade, T., Saragih, J. and Ambadar, Z. , “The extended cohn-kanade
dataset (ckþ): a complete dataset for action unit and emotion-specified expression”,  IEEE
Computer Society Conference on Computer Vision and Pattern Recognition-Workshops,
, IEEE.
Mehrabian, A. , “Communication without words”, Communication Theory, Vol. , pp. -.
Mei, J. and Hu, B. , “Research and implementation of real-time face expression recognition
method”, Information and Technology, Vol.  No. , pp. -.
Organization W H , The ICD- Classification of Mental and Behavioural Disorders: Clinical
Descriptions and Diagnostic Guidelines, World Health Organization, Geneva.
Segal, D.L. , “Diagnostic and statistical manual of mental disorders (DSM-IV-TR)”, The Corsini
Encyclopedia of Psychology, Vol.  No. , pp. -.
Shen, X., He, Z. and Ding, X. , “Computer facial expression recognition training to improve the
facial expression recognition ability of autistic children”, Sci-tech Horizon, Vol. , pp. -.
Singh, P., Ghosh, S. and Nandi, S. , “Subjective burden and depression in mothers of children
with autism spectrum disorder in India: moderating effect of social support”, Journal of Autism
and Developmental Disorders, Vol.  No. , pp. -.
Vismara, L.A. and Rogers, S.J. , “The early start denver model”, Journal of Early Intervention,
Vol.  No. , pp. -.
Wang, H. , “Psychological and behavioral characteristics, diagnosis and evaluation of autistic
children”, Chinese Journal of Rehabilitation Medicine, Vol.  No. , pp. -.
Wang, G. and Lu, M. , “Research on educational games for children with autism spectrum
disorders”, Modern Special Education, Vol. , pp. -.
Wang, Y., Xiao, L. and Chen, R. , “Social impairment of children with autism spectrum disorder
affects parental quality of life in different ways”, Psychiatry Research, Vol.  No. ,
pp. -.
Wu, X. and Chen, S. , “Research progress on quality of life and its influencing factors of primary
caregivers for autistic children”, General Nursing, Vol.  No. , pp. -.
Yan, S. , “Experimental study on facial expression processing of autistic children”, East China
Normal University.
Yanbin, H., Fuxing, W., Heping, X., Jing, A., Yuxin, W. and Huashan, L. , “Facial processing
characteristics of autism spectrum disorders: meta-analysis of eye movement research”,
Progress in Psychological Science, Vol. , pp. -.
Yang, Y. and Wang, M. , “Employment and financial burdens of families with preschool-aged
children with autism”, Chinese Journal of Clinical Psychology, Vol.  No. , pp. -, .
LHT
Yang, J., Xing, H., Shao, Z. and Yuan, J. , “Facial expression sensitivity deficits in patients with
autism spectrum disorder: impact of task nature and implications for intervention”, Chinese
Science: Life Science, Vol.  No. , pp. -.
Zablotsky, B., Black, L.I. and Blumberg, S.J. , “Estimated prevalence of children with diagnosed
developmental disabilities in the United States, -”, NCHS Data Brief, Vol. , pp. -.
Corresponding author
Wang Zhao can be contacted at: 
For instructions on how to order reprints of this article, please visit our website:

Or contact us for further details: 
Facial
expressions for
autism
diagnosis
